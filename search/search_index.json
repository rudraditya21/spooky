{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Technical documentation for the Spooky HTTP/3 to HTTP/2 reverse proxy and load balancer. Quick Navigation Getting Started Overview - Project introduction and capabilities Installation - System requirements and installation procedures Quick Start Tutorial - Step-by-step guide to get running Configuration Configuration Reference - Complete configuration documentation TLS Setup - Certificate generation and management User Guides Basic Usage - Core concepts and usage patterns Load Balancing - Load balancing algorithms and health checks Architecture Architecture Overview - System design and component interaction Component Details - High-level architectural principles Component Breakdown - Detailed crate documentation Deployment Production Deployment - Production deployment guide Troubleshooting - Common issues and solutions Development Contributing Guide - Development setup and guidelines Protocol Reference HTTP/3 Protocol - HTTP/3 overview and implementation QUIC Protocol - QUIC fundamentals and usage API and Observability API Overview - Metrics, logging, and future admin API Planning Roadmap - Feature roadmap and priorities Documentation Structure README.md : Documentation index (this page) architecture.md : Main architecture document roadmap.md : Project roadmap getting-started/ : Overview and installation guides configuration/ : Configuration reference and TLS setup user-guide/ : Basic usage and load balancing guide architecture/ : High-level design and component details deployment/ : Production deployment guidance troubleshooting/ : Common issues and fixes development/ : Development and contribution guidance tutorials/ : Quickstart walkthroughs protocols/ : HTTP/3 and QUIC protocol notes api/ : API and observability overview internal/ : Internal architecture notes Quick References Common Configuration Tasks Basic upstream pool : upstream: backend: route: path_prefix: \"/\" backends: - id: \"backend-1\" address: \"127.0.0.1:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 Note: Load balancing strategy is configured globally, not per upstream. Path-based routing : upstream: api: route: path_prefix: \"/api\" # ... backends web: route: path_prefix: \"/\" # ... backends Load balancing algorithms : - random - Random selection - round-robin - Sequential rotation - consistent-hash - Hash-based affinity Common Commands Start Spooky : spooky --config /etc/spooky/config.yaml Test HTTP/3 connection : curl --http3-only -k \\ --resolve proxy.example.com:9889:127.0.0.1 \\ https://proxy.example.com:9889/health Check configuration : spooky --config config.yaml # Starts serving after validation View logs : # All logs RUST_LOG=info spooky --config config.yaml # Debug QUIC only RUST_LOG=spooky_edge=debug spooky --config config.yaml # Trace everything RUST_LOG=trace spooky --config config.yaml Documentation Guidelines This documentation follows these principles: Technical Accuracy : All examples are based on the actual codebase Honest Status : Capabilities and limitations are documented as-is Direct Communication : Clear, concise technical writing Complete Coverage : All configuration options documented Practical Examples : Working code and configuration samples Contributing to Documentation To improve documentation: Check accuracy against source code Test all examples and commands Use clear, technical language Include practical examples Update this index when adding new docs See Contributing Guide for more details. External Resources QUIC RFC 9000 HTTP/3 RFC 9114 quiche Documentation Rust Documentation Getting Help Review troubleshooting guide: docs/troubleshooting/common-issues.md Check GitHub issues for community discussions Read protocol documentation for HTTP/3 and QUIC specifics License Elastic License 2.0 (ELv2) - see LICENSE for details.","title":"Home"},{"location":"#quick-navigation","text":"","title":"Quick Navigation"},{"location":"#getting-started","text":"Overview - Project introduction and capabilities Installation - System requirements and installation procedures Quick Start Tutorial - Step-by-step guide to get running","title":"Getting Started"},{"location":"#configuration","text":"Configuration Reference - Complete configuration documentation TLS Setup - Certificate generation and management","title":"Configuration"},{"location":"#user-guides","text":"Basic Usage - Core concepts and usage patterns Load Balancing - Load balancing algorithms and health checks","title":"User Guides"},{"location":"#architecture","text":"Architecture Overview - System design and component interaction Component Details - High-level architectural principles Component Breakdown - Detailed crate documentation","title":"Architecture"},{"location":"#deployment","text":"Production Deployment - Production deployment guide Troubleshooting - Common issues and solutions","title":"Deployment"},{"location":"#development","text":"Contributing Guide - Development setup and guidelines","title":"Development"},{"location":"#protocol-reference","text":"HTTP/3 Protocol - HTTP/3 overview and implementation QUIC Protocol - QUIC fundamentals and usage","title":"Protocol Reference"},{"location":"#api-and-observability","text":"API Overview - Metrics, logging, and future admin API","title":"API and Observability"},{"location":"#planning","text":"Roadmap - Feature roadmap and priorities","title":"Planning"},{"location":"#documentation-structure","text":"README.md : Documentation index (this page) architecture.md : Main architecture document roadmap.md : Project roadmap getting-started/ : Overview and installation guides configuration/ : Configuration reference and TLS setup user-guide/ : Basic usage and load balancing guide architecture/ : High-level design and component details deployment/ : Production deployment guidance troubleshooting/ : Common issues and fixes development/ : Development and contribution guidance tutorials/ : Quickstart walkthroughs protocols/ : HTTP/3 and QUIC protocol notes api/ : API and observability overview internal/ : Internal architecture notes","title":"Documentation Structure"},{"location":"#quick-references","text":"","title":"Quick References"},{"location":"#common-configuration-tasks","text":"Basic upstream pool : upstream: backend: route: path_prefix: \"/\" backends: - id: \"backend-1\" address: \"127.0.0.1:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 Note: Load balancing strategy is configured globally, not per upstream. Path-based routing : upstream: api: route: path_prefix: \"/api\" # ... backends web: route: path_prefix: \"/\" # ... backends Load balancing algorithms : - random - Random selection - round-robin - Sequential rotation - consistent-hash - Hash-based affinity","title":"Common Configuration Tasks"},{"location":"#common-commands","text":"Start Spooky : spooky --config /etc/spooky/config.yaml Test HTTP/3 connection : curl --http3-only -k \\ --resolve proxy.example.com:9889:127.0.0.1 \\ https://proxy.example.com:9889/health Check configuration : spooky --config config.yaml # Starts serving after validation View logs : # All logs RUST_LOG=info spooky --config config.yaml # Debug QUIC only RUST_LOG=spooky_edge=debug spooky --config config.yaml # Trace everything RUST_LOG=trace spooky --config config.yaml","title":"Common Commands"},{"location":"#documentation-guidelines","text":"This documentation follows these principles: Technical Accuracy : All examples are based on the actual codebase Honest Status : Capabilities and limitations are documented as-is Direct Communication : Clear, concise technical writing Complete Coverage : All configuration options documented Practical Examples : Working code and configuration samples","title":"Documentation Guidelines"},{"location":"#contributing-to-documentation","text":"To improve documentation: Check accuracy against source code Test all examples and commands Use clear, technical language Include practical examples Update this index when adding new docs See Contributing Guide for more details.","title":"Contributing to Documentation"},{"location":"#external-resources","text":"QUIC RFC 9000 HTTP/3 RFC 9114 quiche Documentation Rust Documentation","title":"External Resources"},{"location":"#getting-help","text":"Review troubleshooting guide: docs/troubleshooting/common-issues.md Check GitHub issues for community discussions Read protocol documentation for HTTP/3 and QUIC specifics","title":"Getting Help"},{"location":"#license","text":"Elastic License 2.0 (ELv2) - see LICENSE for details.","title":"License"},{"location":"architecture/","text":"Overview Spooky is a reverse proxy that terminates HTTP/3/QUIC connections and forwards requests to HTTP/2 backends. The architecture prioritizes correctness, observability, and operational simplicity. Design Principles Protocol Isolation : QUIC termination is separate from HTTP/2 backend communication Fail Fast : Configuration errors are caught at startup, not during runtime Health-Aware Routing : Backend selection considers health state Observability First : All state transitions and errors are logged Component Architecture \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Spooky Process \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Main Event Loop \u2502 \u2502 \u2502 \u2502 (Synchronous UDP polling with timeout) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 QUIC Listener (crates/edge) \u2502 \u2502 \u2502 \u2502 - UDP socket management \u2502 \u2502 \u2502 \u2502 - quiche connection handling \u2502 \u2502 \u2502 \u2502 - Connection ID routing \u2502 \u2502 \u2502 \u2502 - HTTP/3 stream multiplexing \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Router (find_upstream_for_request) \u2502 \u2502 \u2502 \u2502 - Path prefix matching \u2502 \u2502 \u2502 \u2502 - Host header matching \u2502 \u2502 \u2502 \u2502 - Longest match selection \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Load Balancer (crates/lb) \u2502 \u2502 \u2502 \u2502 - Backend selection algorithms \u2502 \u2502 \u2502 \u2502 - Health state filtering \u2502 \u2502 \u2502 \u2502 - Per-upstream strategy \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Protocol Bridge (crates/bridge) \u2502 \u2502 \u2502 \u2502 - HTTP/3 to HTTP/2 header conversion \u2502 \u2502 \u2502 \u2502 - Body buffering \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 HTTP/2 Pool (crates/transport) \u2502 \u2502 \u2502 \u2502 - Backend connection pooling \u2502 \u2502 \u2502 \u2502 - Request forwarding \u2502 \u2502 \u2502 \u2502 - Concurrency limiting \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Health Checker (async tasks) \u2502 \u2502 \u2502 \u2502 - Periodic HTTP probes \u2502 \u2502 \u2502 \u2502 - Backend state tracking \u2502 \u2502 \u2502 \u2502 - Health transition logging \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Request Flow 1. Connection Establishment Client Spooky Backend \u2502 \u2502 \u2502 \u251c\u2500 QUIC Initial \u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 \u2502 \u2502 \u2502 \u2502 \u2502<\u2500\u2500\u2500\u2500\u2500 ServerHello \u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 Handshake \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 \u2502 \u2502 \u2502 \u2502 \u2502<\u2500\u2500\u2500\u2500\u2500 Handshake \u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502 \u2502 [Connection ID routing established] \u2502 Key Points : - Server generates 16-byte SCID for each connection - Connection stored by SCID for subsequent packet routing - Prefix matching handles clients that extend DCID - Peer-based fallback for connection migration 2. HTTP/3 Request Processing \u2502 \u2502 \u2502 \u251c\u2500 HEADERS frame \u2500\u2500\u2500\u2500\u2500\u2500>\u2502 \u2502 \u251c\u2500 DATA frame \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 Route matching \u2502 \u2502 \u251c\u2500 Upstream selection \u2502 \u2502 \u251c\u2500 Backend selection \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 HTTP/2 request \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 \u2502 \u2502 \u2502 \u2502 \u2502<\u2500\u2500\u2500\u2500 HTTP/2 response \u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502<\u2500\u2500 HEADERS frame \u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502<\u2500\u2500 DATA frame \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Processing Steps : Stream Reception : HTTP/3 frames decoded via quiche Request Envelope : Headers, path, authority, and body buffered Route Matching : Find upstream with longest matching path prefix Load Balancing : Select healthy backend from upstream pool Protocol Bridge : Convert HTTP/3 request to HTTP/2 Backend Call : Forward via connection pool with timeout Response Streaming : Convert HTTP/2 response to HTTP/3 3. Route Matching Algorithm fn find_upstream_for_request( upstreams: HashMap<String, Upstream>, path: &str, host: Option<&str> ) -> Option<String> { let mut best_match = None; let mut best_length = 0; for (name, upstream) in upstreams { // Check host match if let Some(required_host) = upstream.route.host { if host != Some(required_host) { continue; } } // Check path prefix match if let Some(prefix) = upstream.route.path_prefix { if path.starts_with(prefix) && prefix.len() > best_length { best_match = Some(name); best_length = prefix.len(); } } } best_match } Example : - Request: /api/users/123 - Routes: / (length 1), /api (length 4) - Selected: /api (longest match) Connection Management Connection ID Routing Spooky uses a connection ID-based routing scheme to multiplex multiple QUIC connections: Initial Packet : Client sends with random DCID Server Response : Generates 16-byte SCID, stores connection Subsequent Packets : Client uses server SCID as DCID Lookup : HashMap lookup by DCID finds connection Special Cases : - Prefix Match : Client extends DCID (e.g., 20 bytes) \u2192 match by prefix - Peer Fallback : DCID not found \u2192 search by peer address - Version Negotiation : Unsupported version \u2192 send version negotiation packet Connection Lifecycle [Initial Packet] \u2192 [Handshake] \u2192 [Established] \u2192 [Active] \u2192 [Draining] \u2192 [Closed] \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc Accept & SCID HTTP/3 Streams Shutdown Cleanup Generation Signal Load Balancing Backend Selection Each upstream pool maintains its own backend list with health state: struct BackendState { address: String, weight: u32, health_state: HealthState, consecutive_failures: u32, } enum HealthState { Healthy, Unhealthy { until: Instant, // Cooldown expiry successes: u32, // Success count during recovery }, } Algorithms Random : candidates = healthy_backends() index = random(0, candidates.len()) return candidates[index] Round Robin : candidates = healthy_backends() index = (next_counter % candidates.len()) next_counter += 1 return candidates[index] Consistent Hash : ring = build_ring(backends, replicas=64) key_hash = hash(request_key) position = ring.find_next(key_hash) return backends[position] Health Checking Each backend has an independent health checker that: Issues periodic HTTP GET to configured path Evaluates response status (2xx = healthy) Updates backend state on success/failure Applies threshold-based state transitions State Transitions : Healthy \u2500[failure_threshold fails]\u2500> Unhealthy Unhealthy \u2500[cooldown expires + success_threshold succeeds]\u2500> Healthy Data Structures QUICListener pub struct QUICListener { socket: UdpSocket, quic_config: quiche::Config, h3_config: Arc<quiche::h3::Config>, connections: HashMap<Vec<u8>, QuicConnection>, // Key: SCID upstream_pools: HashMap<String, Arc<Mutex<UpstreamPool>>>, h2_pool: Arc<H2Pool>, metrics: Metrics, // ... } QuicConnection pub struct QuicConnection { quic: quiche::Connection, h3: Option<quiche::h3::Connection>, streams: HashMap<u64, RequestEnvelope>, peer_address: SocketAddr, last_activity: Instant, } UpstreamPool pub struct UpstreamPool { pool: BackendPool, // Backend list with health state strategy: String, // Load balancing algorithm name } Concurrency Model Main Thread (Synchronous) UDP socket polling with 50ms timeout QUIC packet processing via quiche HTTP/3 stream handling Route matching and backend selection Synchronous backend calls via run_blocking Async Tasks (Tokio Runtime) Health check probes (one task per backend) Shutdown signal handling Blocking Operations Backend forwarding temporarily enters Tokio runtime: fn run_blocking<F, T>(f: F) -> Result<T> where F: FnOnce() -> Future<Output = T>, { if let Ok(handle) = Handle::try_current() { // Within Tokio context tokio::task::block_in_place(|| handle.block_on(f())) } else { // Outside Tokio context let rt = Runtime::new()?; rt.block_on(f()) } } Configuration System Validation Pipeline YAML file \u2192 Parse \u2192 Validate \u2192 Build runtime structures \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc serde::de Validator QUICListener::new UpstreamPool::from_upstream LoadBalancing::from_config Validation Checks TLS certificate and key files exist and are readable Listen port in valid range (1-65535) All backend addresses are parseable Load balancing types are supported Health check intervals are non-zero Route patterns are valid Error Handling Request-Level Errors Error Source HTTP Status Action Invalid request 400 Return error to client No healthy backends 503 Return error to client Backend timeout 503 Mark backend failure, return error Backend connection error 502 Mark backend failure, return error Backend 5xx response Pass through Mark backend failure Connection-Level Errors Error Type Action QUIC crypto failure Log and close connection QUIC protocol violation Log and close connection HTTP/3 stream error Reset stream, keep connection Idle timeout Close connection System-Level Errors Error Type Action Config validation failure Exit on startup TLS load failure Exit on startup Socket bind failure Exit on startup Health check task panic Log error, continue Performance Characteristics Memory Usage Base process: ~50MB Per connection: ~1-2KB Per stream: ~500B Buffer sizes: 64KB (configurable) CPU Usage Packet processing: Minimal (quiche handles crypto) Route matching: O(N) where N = upstream count Load balancing: O(1) for random/round-robin, O(log M) for consistent hash where M = backend count Health checking: Periodic, minimal impact Bottlenecks Current architectural bottlenecks: Synchronous backend calls : Block main thread during HTTP/2 roundtrip Full body buffering : Materializes entire request/response in memory Consistent hash ring : Rebuilds on every request Single-threaded poll loop : All QUIC processing on one thread See roadmap for planned improvements. Security TLS Configuration TLS 1.3 only (via quiche) ALPN: h3 (HTTP/3) Peer verification disabled (development mode) Certificate chain loaded from PEM files Attack Mitigation Current protections: Connection ID randomization Idle timeout enforcement Buffer size limits Health check prevents amplification to backends Missing protections (planned): Rate limiting per client IP Request size limits DDoS protection TLS peer verification Observability Logging Structured logging at multiple levels: Error : Critical failures, backend errors Warn : Backend health transitions, timeouts Info : Request processing, backend selection Debug : QUIC packet handling, connection state Trace : Detailed protocol messages Metrics Current metrics (AtomicU64): requests_total : Total requests received requests_success : Successfully forwarded requests requests_failure : Failed requests backend_timeouts : Backend timeout count backend_errors : Backend error count No metrics exporter currently implemented. Debugging Connection state logging: debug!(\"Packet DCID (len={}): {:02x?}, type: {:?}, active connections: {}\", dcid_bytes.len(), &dcid_bytes, header.ty, self.connections.len()); Backend selection logging: info!(\"Selected backend {} via {}\", backend_addr, lb_name(load_balancer)); Health transition logging: info!(\"Backend {} became unhealthy\", addr); Future Directions See roadmap for planned architectural improvements: Async data plane Streaming request/response bodies Multi-threaded QUIC handling Metrics export Configuration hot reload","title":"Overview"},{"location":"architecture/#overview","text":"Spooky is a reverse proxy that terminates HTTP/3/QUIC connections and forwards requests to HTTP/2 backends. The architecture prioritizes correctness, observability, and operational simplicity.","title":"Overview"},{"location":"architecture/#design-principles","text":"Protocol Isolation : QUIC termination is separate from HTTP/2 backend communication Fail Fast : Configuration errors are caught at startup, not during runtime Health-Aware Routing : Backend selection considers health state Observability First : All state transitions and errors are logged","title":"Design Principles"},{"location":"architecture/#component-architecture","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Spooky Process \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Main Event Loop \u2502 \u2502 \u2502 \u2502 (Synchronous UDP polling with timeout) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 QUIC Listener (crates/edge) \u2502 \u2502 \u2502 \u2502 - UDP socket management \u2502 \u2502 \u2502 \u2502 - quiche connection handling \u2502 \u2502 \u2502 \u2502 - Connection ID routing \u2502 \u2502 \u2502 \u2502 - HTTP/3 stream multiplexing \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Router (find_upstream_for_request) \u2502 \u2502 \u2502 \u2502 - Path prefix matching \u2502 \u2502 \u2502 \u2502 - Host header matching \u2502 \u2502 \u2502 \u2502 - Longest match selection \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Load Balancer (crates/lb) \u2502 \u2502 \u2502 \u2502 - Backend selection algorithms \u2502 \u2502 \u2502 \u2502 - Health state filtering \u2502 \u2502 \u2502 \u2502 - Per-upstream strategy \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Protocol Bridge (crates/bridge) \u2502 \u2502 \u2502 \u2502 - HTTP/3 to HTTP/2 header conversion \u2502 \u2502 \u2502 \u2502 - Body buffering \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 HTTP/2 Pool (crates/transport) \u2502 \u2502 \u2502 \u2502 - Backend connection pooling \u2502 \u2502 \u2502 \u2502 - Request forwarding \u2502 \u2502 \u2502 \u2502 - Concurrency limiting \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Health Checker (async tasks) \u2502 \u2502 \u2502 \u2502 - Periodic HTTP probes \u2502 \u2502 \u2502 \u2502 - Backend state tracking \u2502 \u2502 \u2502 \u2502 - Health transition logging \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Component Architecture"},{"location":"architecture/#request-flow","text":"","title":"Request Flow"},{"location":"architecture/#1-connection-establishment","text":"Client Spooky Backend \u2502 \u2502 \u2502 \u251c\u2500 QUIC Initial \u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 \u2502 \u2502 \u2502 \u2502 \u2502<\u2500\u2500\u2500\u2500\u2500 ServerHello \u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 Handshake \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 \u2502 \u2502 \u2502 \u2502 \u2502<\u2500\u2500\u2500\u2500\u2500 Handshake \u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502 \u2502 [Connection ID routing established] \u2502 Key Points : - Server generates 16-byte SCID for each connection - Connection stored by SCID for subsequent packet routing - Prefix matching handles clients that extend DCID - Peer-based fallback for connection migration","title":"1. Connection Establishment"},{"location":"architecture/#2-http3-request-processing","text":"\u2502 \u2502 \u2502 \u251c\u2500 HEADERS frame \u2500\u2500\u2500\u2500\u2500\u2500>\u2502 \u2502 \u251c\u2500 DATA frame \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 Route matching \u2502 \u2502 \u251c\u2500 Upstream selection \u2502 \u2502 \u251c\u2500 Backend selection \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 HTTP/2 request \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 \u2502 \u2502 \u2502 \u2502 \u2502<\u2500\u2500\u2500\u2500 HTTP/2 response \u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502<\u2500\u2500 HEADERS frame \u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502<\u2500\u2500 DATA frame \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Processing Steps : Stream Reception : HTTP/3 frames decoded via quiche Request Envelope : Headers, path, authority, and body buffered Route Matching : Find upstream with longest matching path prefix Load Balancing : Select healthy backend from upstream pool Protocol Bridge : Convert HTTP/3 request to HTTP/2 Backend Call : Forward via connection pool with timeout Response Streaming : Convert HTTP/2 response to HTTP/3","title":"2. HTTP/3 Request Processing"},{"location":"architecture/#3-route-matching-algorithm","text":"fn find_upstream_for_request( upstreams: HashMap<String, Upstream>, path: &str, host: Option<&str> ) -> Option<String> { let mut best_match = None; let mut best_length = 0; for (name, upstream) in upstreams { // Check host match if let Some(required_host) = upstream.route.host { if host != Some(required_host) { continue; } } // Check path prefix match if let Some(prefix) = upstream.route.path_prefix { if path.starts_with(prefix) && prefix.len() > best_length { best_match = Some(name); best_length = prefix.len(); } } } best_match } Example : - Request: /api/users/123 - Routes: / (length 1), /api (length 4) - Selected: /api (longest match)","title":"3. Route Matching Algorithm"},{"location":"architecture/#connection-management","text":"","title":"Connection Management"},{"location":"architecture/#connection-id-routing","text":"Spooky uses a connection ID-based routing scheme to multiplex multiple QUIC connections: Initial Packet : Client sends with random DCID Server Response : Generates 16-byte SCID, stores connection Subsequent Packets : Client uses server SCID as DCID Lookup : HashMap lookup by DCID finds connection Special Cases : - Prefix Match : Client extends DCID (e.g., 20 bytes) \u2192 match by prefix - Peer Fallback : DCID not found \u2192 search by peer address - Version Negotiation : Unsupported version \u2192 send version negotiation packet","title":"Connection ID Routing"},{"location":"architecture/#connection-lifecycle","text":"[Initial Packet] \u2192 [Handshake] \u2192 [Established] \u2192 [Active] \u2192 [Draining] \u2192 [Closed] \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc Accept & SCID HTTP/3 Streams Shutdown Cleanup Generation Signal","title":"Connection Lifecycle"},{"location":"architecture/#load-balancing","text":"","title":"Load Balancing"},{"location":"architecture/#backend-selection","text":"Each upstream pool maintains its own backend list with health state: struct BackendState { address: String, weight: u32, health_state: HealthState, consecutive_failures: u32, } enum HealthState { Healthy, Unhealthy { until: Instant, // Cooldown expiry successes: u32, // Success count during recovery }, }","title":"Backend Selection"},{"location":"architecture/#algorithms","text":"Random : candidates = healthy_backends() index = random(0, candidates.len()) return candidates[index] Round Robin : candidates = healthy_backends() index = (next_counter % candidates.len()) next_counter += 1 return candidates[index] Consistent Hash : ring = build_ring(backends, replicas=64) key_hash = hash(request_key) position = ring.find_next(key_hash) return backends[position]","title":"Algorithms"},{"location":"architecture/#health-checking","text":"Each backend has an independent health checker that: Issues periodic HTTP GET to configured path Evaluates response status (2xx = healthy) Updates backend state on success/failure Applies threshold-based state transitions State Transitions : Healthy \u2500[failure_threshold fails]\u2500> Unhealthy Unhealthy \u2500[cooldown expires + success_threshold succeeds]\u2500> Healthy","title":"Health Checking"},{"location":"architecture/#data-structures","text":"","title":"Data Structures"},{"location":"architecture/#quiclistener","text":"pub struct QUICListener { socket: UdpSocket, quic_config: quiche::Config, h3_config: Arc<quiche::h3::Config>, connections: HashMap<Vec<u8>, QuicConnection>, // Key: SCID upstream_pools: HashMap<String, Arc<Mutex<UpstreamPool>>>, h2_pool: Arc<H2Pool>, metrics: Metrics, // ... }","title":"QUICListener"},{"location":"architecture/#quicconnection","text":"pub struct QuicConnection { quic: quiche::Connection, h3: Option<quiche::h3::Connection>, streams: HashMap<u64, RequestEnvelope>, peer_address: SocketAddr, last_activity: Instant, }","title":"QuicConnection"},{"location":"architecture/#upstreampool","text":"pub struct UpstreamPool { pool: BackendPool, // Backend list with health state strategy: String, // Load balancing algorithm name }","title":"UpstreamPool"},{"location":"architecture/#concurrency-model","text":"","title":"Concurrency Model"},{"location":"architecture/#main-thread-synchronous","text":"UDP socket polling with 50ms timeout QUIC packet processing via quiche HTTP/3 stream handling Route matching and backend selection Synchronous backend calls via run_blocking","title":"Main Thread (Synchronous)"},{"location":"architecture/#async-tasks-tokio-runtime","text":"Health check probes (one task per backend) Shutdown signal handling","title":"Async Tasks (Tokio Runtime)"},{"location":"architecture/#blocking-operations","text":"Backend forwarding temporarily enters Tokio runtime: fn run_blocking<F, T>(f: F) -> Result<T> where F: FnOnce() -> Future<Output = T>, { if let Ok(handle) = Handle::try_current() { // Within Tokio context tokio::task::block_in_place(|| handle.block_on(f())) } else { // Outside Tokio context let rt = Runtime::new()?; rt.block_on(f()) } }","title":"Blocking Operations"},{"location":"architecture/#configuration-system","text":"","title":"Configuration System"},{"location":"architecture/#validation-pipeline","text":"YAML file \u2192 Parse \u2192 Validate \u2192 Build runtime structures \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc serde::de Validator QUICListener::new UpstreamPool::from_upstream LoadBalancing::from_config","title":"Validation Pipeline"},{"location":"architecture/#validation-checks","text":"TLS certificate and key files exist and are readable Listen port in valid range (1-65535) All backend addresses are parseable Load balancing types are supported Health check intervals are non-zero Route patterns are valid","title":"Validation Checks"},{"location":"architecture/#error-handling","text":"","title":"Error Handling"},{"location":"architecture/#request-level-errors","text":"Error Source HTTP Status Action Invalid request 400 Return error to client No healthy backends 503 Return error to client Backend timeout 503 Mark backend failure, return error Backend connection error 502 Mark backend failure, return error Backend 5xx response Pass through Mark backend failure","title":"Request-Level Errors"},{"location":"architecture/#connection-level-errors","text":"Error Type Action QUIC crypto failure Log and close connection QUIC protocol violation Log and close connection HTTP/3 stream error Reset stream, keep connection Idle timeout Close connection","title":"Connection-Level Errors"},{"location":"architecture/#system-level-errors","text":"Error Type Action Config validation failure Exit on startup TLS load failure Exit on startup Socket bind failure Exit on startup Health check task panic Log error, continue","title":"System-Level Errors"},{"location":"architecture/#performance-characteristics","text":"","title":"Performance Characteristics"},{"location":"architecture/#memory-usage","text":"Base process: ~50MB Per connection: ~1-2KB Per stream: ~500B Buffer sizes: 64KB (configurable)","title":"Memory Usage"},{"location":"architecture/#cpu-usage","text":"Packet processing: Minimal (quiche handles crypto) Route matching: O(N) where N = upstream count Load balancing: O(1) for random/round-robin, O(log M) for consistent hash where M = backend count Health checking: Periodic, minimal impact","title":"CPU Usage"},{"location":"architecture/#bottlenecks","text":"Current architectural bottlenecks: Synchronous backend calls : Block main thread during HTTP/2 roundtrip Full body buffering : Materializes entire request/response in memory Consistent hash ring : Rebuilds on every request Single-threaded poll loop : All QUIC processing on one thread See roadmap for planned improvements.","title":"Bottlenecks"},{"location":"architecture/#security","text":"","title":"Security"},{"location":"architecture/#tls-configuration","text":"TLS 1.3 only (via quiche) ALPN: h3 (HTTP/3) Peer verification disabled (development mode) Certificate chain loaded from PEM files","title":"TLS Configuration"},{"location":"architecture/#attack-mitigation","text":"Current protections: Connection ID randomization Idle timeout enforcement Buffer size limits Health check prevents amplification to backends Missing protections (planned): Rate limiting per client IP Request size limits DDoS protection TLS peer verification","title":"Attack Mitigation"},{"location":"architecture/#observability","text":"","title":"Observability"},{"location":"architecture/#logging","text":"Structured logging at multiple levels: Error : Critical failures, backend errors Warn : Backend health transitions, timeouts Info : Request processing, backend selection Debug : QUIC packet handling, connection state Trace : Detailed protocol messages","title":"Logging"},{"location":"architecture/#metrics","text":"Current metrics (AtomicU64): requests_total : Total requests received requests_success : Successfully forwarded requests requests_failure : Failed requests backend_timeouts : Backend timeout count backend_errors : Backend error count No metrics exporter currently implemented.","title":"Metrics"},{"location":"architecture/#debugging","text":"Connection state logging: debug!(\"Packet DCID (len={}): {:02x?}, type: {:?}, active connections: {}\", dcid_bytes.len(), &dcid_bytes, header.ty, self.connections.len()); Backend selection logging: info!(\"Selected backend {} via {}\", backend_addr, lb_name(load_balancer)); Health transition logging: info!(\"Backend {} became unhealthy\", addr);","title":"Debugging"},{"location":"architecture/#future-directions","text":"See roadmap for planned architectural improvements: Async data plane Streaming request/response bodies Multi-threaded QUIC handling Metrics export Configuration hot reload","title":"Future Directions"},{"location":"changelog/","text":"--8<-- \"../CHANGELOG.md\"","title":"Changelog"},{"location":"references/","text":"Technical references and external resources for Spooky development. Protocol Specifications QUIC and HTTP/3 RFC 9000 - QUIC: A UDP-Based Multiplexed and Secure Transport RFC 9001 - Using TLS to Secure QUIC RFC 9002 - QUIC Loss Detection and Congestion Control RFC 9114 - HTTP/3 RFC 9204 - QPACK: Field Compression for HTTP/3 HTTP/2 RFC 9113 - HTTP/2 RFC 7541 - HPACK: Header Compression for HTTP/2 TLS RFC 8446 - The Transport Layer Security (TLS) Protocol Version 1.3 RFC 7301 - Transport Layer Security (TLS) Application-Layer Protocol Negotiation Extension Core Dependencies QUIC and HTTP/3 quiche - Cloudflare's QUIC and HTTP/3 implementation quinn - Alternative async QUIC implementation HTTP/2 hyper - HTTP client and server library h2 - HTTP/2 implementation Async Runtime tokio - Asynchronous runtime for Rust futures - Async utilities Serialization serde - Serialization framework serde_yaml - YAML support for serde CLI and Configuration clap - Command-line argument parser Utilities bytes - Efficient byte buffer types http - HTTP types log - Logging facade env_logger - Logger implementation rand - Random number generation TLS rustls - Modern TLS library rustls-pki-types - TLS certificate types Load Balancing Resources Algorithms Consistent Hashing and Random Trees - Original consistent hashing paper The Power of Two Random Choices - Random selection strategy analysis Health Checking Circuit Breaker Pattern - Martin Fowler Health Checks for gRPC - gRPC health check protocol Performance and Optimization QUIC Performance QUIC at Cloudflare - Production QUIC deployment insights QUIC at Google - Chrome QUIC implementation notes HTTP/3 Optimization HTTP/3 Explained - Daniel Stenberg's HTTP/3 guide HTTP/3 Performance - Real-world performance analysis System Tuning Linux Network Stack - Kernel networking documentation UDP Performance - High-performance UDP handling Security TLS Best Practices Mozilla SSL Configuration Generator - TLS configuration recommendations Certificate Transparency - CT log monitoring QUIC Security QUIC Crypto - QUIC cryptographic design QUIC Security Considerations - RFC 9000 Section 21 Testing and Debugging Tools curl with HTTP/3 - Testing HTTP/3 endpoints h3i - Interactive HTTP/3 client Wireshark QUIC - Packet capture and analysis Load Testing h2load - HTTP/2 load testing tool wrk2 - HTTP benchmarking tool Monitoring and Observability Metrics Prometheus Documentation - Metrics collection OpenTelemetry - Observability framework Tracing Tokio Tracing - Application-level tracing Jaeger - Distributed tracing Related Projects HTTP/3 Proxies nghttpx - HTTP/2 and HTTP/3 proxy h2o - HTTP/1.x, HTTP/2, HTTP/3 server QUIC Implementations quic-go - Go QUIC implementation msquic - Microsoft QUIC implementation ngtcp2 - C QUIC library Load Balancers HAProxy - Traditional TCP/HTTP load balancer Envoy - Modern L7 proxy and load balancer Traefik - Cloud-native edge router Community Resources Rust Rust Programming Language Book Rust Async Book Tokio Tutorial QUIC and HTTP/3 QUIC Working Group - IETF QUIC standardization HTTP/3 Implementations - List of QUIC/HTTP3 implementations Academic Papers QUIC: A UDP-Based Multiplexed and Secure Transport - SIGCOMM 2017 The QUIC Transport Protocol: Design and Internet-Scale Deployment - SIGCOMM 2017 An Analysis of QUIC in the Wild - IMC 2019 Contributing To add a reference: Verify the resource is authoritative and current Add to the appropriate section Include a brief description Test all links See contributing guide for more details.","title":"References"},{"location":"references/#protocol-specifications","text":"","title":"Protocol Specifications"},{"location":"references/#quic-and-http3","text":"RFC 9000 - QUIC: A UDP-Based Multiplexed and Secure Transport RFC 9001 - Using TLS to Secure QUIC RFC 9002 - QUIC Loss Detection and Congestion Control RFC 9114 - HTTP/3 RFC 9204 - QPACK: Field Compression for HTTP/3","title":"QUIC and HTTP/3"},{"location":"references/#http2","text":"RFC 9113 - HTTP/2 RFC 7541 - HPACK: Header Compression for HTTP/2","title":"HTTP/2"},{"location":"references/#tls","text":"RFC 8446 - The Transport Layer Security (TLS) Protocol Version 1.3 RFC 7301 - Transport Layer Security (TLS) Application-Layer Protocol Negotiation Extension","title":"TLS"},{"location":"references/#core-dependencies","text":"","title":"Core Dependencies"},{"location":"references/#quic-and-http3_1","text":"quiche - Cloudflare's QUIC and HTTP/3 implementation quinn - Alternative async QUIC implementation","title":"QUIC and HTTP/3"},{"location":"references/#http2_1","text":"hyper - HTTP client and server library h2 - HTTP/2 implementation","title":"HTTP/2"},{"location":"references/#async-runtime","text":"tokio - Asynchronous runtime for Rust futures - Async utilities","title":"Async Runtime"},{"location":"references/#serialization","text":"serde - Serialization framework serde_yaml - YAML support for serde","title":"Serialization"},{"location":"references/#cli-and-configuration","text":"clap - Command-line argument parser","title":"CLI and Configuration"},{"location":"references/#utilities","text":"bytes - Efficient byte buffer types http - HTTP types log - Logging facade env_logger - Logger implementation rand - Random number generation","title":"Utilities"},{"location":"references/#tls_1","text":"rustls - Modern TLS library rustls-pki-types - TLS certificate types","title":"TLS"},{"location":"references/#load-balancing-resources","text":"","title":"Load Balancing Resources"},{"location":"references/#algorithms","text":"Consistent Hashing and Random Trees - Original consistent hashing paper The Power of Two Random Choices - Random selection strategy analysis","title":"Algorithms"},{"location":"references/#health-checking","text":"Circuit Breaker Pattern - Martin Fowler Health Checks for gRPC - gRPC health check protocol","title":"Health Checking"},{"location":"references/#performance-and-optimization","text":"","title":"Performance and Optimization"},{"location":"references/#quic-performance","text":"QUIC at Cloudflare - Production QUIC deployment insights QUIC at Google - Chrome QUIC implementation notes","title":"QUIC Performance"},{"location":"references/#http3-optimization","text":"HTTP/3 Explained - Daniel Stenberg's HTTP/3 guide HTTP/3 Performance - Real-world performance analysis","title":"HTTP/3 Optimization"},{"location":"references/#system-tuning","text":"Linux Network Stack - Kernel networking documentation UDP Performance - High-performance UDP handling","title":"System Tuning"},{"location":"references/#security","text":"","title":"Security"},{"location":"references/#tls-best-practices","text":"Mozilla SSL Configuration Generator - TLS configuration recommendations Certificate Transparency - CT log monitoring","title":"TLS Best Practices"},{"location":"references/#quic-security","text":"QUIC Crypto - QUIC cryptographic design QUIC Security Considerations - RFC 9000 Section 21","title":"QUIC Security"},{"location":"references/#testing-and-debugging","text":"","title":"Testing and Debugging"},{"location":"references/#tools","text":"curl with HTTP/3 - Testing HTTP/3 endpoints h3i - Interactive HTTP/3 client Wireshark QUIC - Packet capture and analysis","title":"Tools"},{"location":"references/#load-testing","text":"h2load - HTTP/2 load testing tool wrk2 - HTTP benchmarking tool","title":"Load Testing"},{"location":"references/#monitoring-and-observability","text":"","title":"Monitoring and Observability"},{"location":"references/#metrics","text":"Prometheus Documentation - Metrics collection OpenTelemetry - Observability framework","title":"Metrics"},{"location":"references/#tracing","text":"Tokio Tracing - Application-level tracing Jaeger - Distributed tracing","title":"Tracing"},{"location":"references/#related-projects","text":"","title":"Related Projects"},{"location":"references/#http3-proxies","text":"nghttpx - HTTP/2 and HTTP/3 proxy h2o - HTTP/1.x, HTTP/2, HTTP/3 server","title":"HTTP/3 Proxies"},{"location":"references/#quic-implementations","text":"quic-go - Go QUIC implementation msquic - Microsoft QUIC implementation ngtcp2 - C QUIC library","title":"QUIC Implementations"},{"location":"references/#load-balancers","text":"HAProxy - Traditional TCP/HTTP load balancer Envoy - Modern L7 proxy and load balancer Traefik - Cloud-native edge router","title":"Load Balancers"},{"location":"references/#community-resources","text":"","title":"Community Resources"},{"location":"references/#rust","text":"Rust Programming Language Book Rust Async Book Tokio Tutorial","title":"Rust"},{"location":"references/#quic-and-http3_2","text":"QUIC Working Group - IETF QUIC standardization HTTP/3 Implementations - List of QUIC/HTTP3 implementations","title":"QUIC and HTTP/3"},{"location":"references/#academic-papers","text":"QUIC: A UDP-Based Multiplexed and Secure Transport - SIGCOMM 2017 The QUIC Transport Protocol: Design and Internet-Scale Deployment - SIGCOMM 2017 An Analysis of QUIC in the Wild - IMC 2019","title":"Academic Papers"},{"location":"references/#contributing","text":"To add a reference: Verify the resource is authoritative and current Add to the appropriate section Include a brief description Test all links See contributing guide for more details.","title":"Contributing"},{"location":"roadmap/","text":"Current Status Experimental. Core features functional and tested. Spooky can terminate HTTP/3 connections, forward to HTTP/2 backends, and perform load balancing with health checks. It is not production-ready \u2014 significant known limitations exist (see Technical Debt below). Completed HTTP/3 termination via quiche HTTP/2 backend connectivity Path and host-based routing Multiple load balancing algorithms (random, round-robin, consistent hash) Active health checking with automatic backend management Per-upstream configuration and routing Connection ID management and QUIC packet routing TLS 1.3 with certificate chain loading Structured logging with multiple levels Configuration validation at startup Graceful shutdown with connection draining Phase 1: Operational Hardening Goal : Expand operational and scalability capabilities for production deployments. Performance Async data plane : Move backend forwarding off the main poll thread Streaming bodies : Implement incremental request/response streaming instead of full buffering Multi-threading : Support multi-threaded QUIC packet processing Connection pooling optimizations : Reduce allocation overhead in HTTP/2 pool Observability Metrics export : Prometheus endpoint for scraping metrics Distributed tracing : OpenTelemetry integration Request logging : Per-request structured logs with correlation IDs Connection metrics : Track QUIC RTT, packet loss, stream count Operational Configuration hot reload : Reload config on SIGHUP without dropping connections Health check improvements : Separate client pool for probes to avoid contention TLS certificate reload : Automatic reload on certificate rotation Admin API : HTTP endpoint for runtime statistics and control Reliability Circuit breaker : Per-backend circuit breakers to prevent cascading failures Retry logic : Configurable request retry with exponential backoff Request timeouts : Per-route timeout configuration Rate limiting : Per-IP and per-route rate limits Phase 2: Advanced Features Goal : Add advanced traffic management and operational capabilities. Traffic Management Weighted routing : Route percentage of traffic to different upstreams Header-based routing : Route by arbitrary request headers Request rewriting : URL rewriting and header manipulation Compression : Automatic response compression Load Balancing Least connections : Track active connections per backend Response time : Route based on backend latency Weighted least connection : Combine weights with connection count Cached consistent hash : Cache hash ring to avoid rebuilds Security TLS peer verification : Enable certificate verification for production mTLS support : Client certificate authentication Request validation : Size limits, header validation IP allowlist/blocklist : Simple access control Deployment Dynamic backend discovery : Service discovery integration (DNS SRV, Consul, etcd) Backend metadata : Tags and labels for flexible routing A/B testing support : Route subset of traffic to experimental backends Canary deployments : Gradually shift traffic to new backend versions Phase 3: Enterprise Features Goal : Support large-scale deployments with advanced requirements. Multi-Tenancy Namespace isolation : Separate routing tables per tenant Resource limits : Per-tenant connection and request limits Tenant routing : Route by tenant ID or subdomain Advanced Observability APM integration : Datadog, New Relic, etc. Custom metrics : User-defined metric collection Traffic replay : Record and replay production traffic Query logs : SQL-like queries over request logs Extensions WebAssembly plugins : Custom routing logic via WASM Lua scripting : Dynamic request/response transformation gRPC support : Native gRPC proxying WebSocket support : WebSocket over HTTP/3 High Availability Connection migration : Support QUIC connection migration State replication : Share connection state across instances Zero-downtime updates : Binary updates without connection loss Multi-region support : Geographic routing and failover Phase 4: Protocol Extensions Goal : Support emerging protocols and optimizations. HTTP/3 Features 0-RTT support : Enable 0-RTT with proper anti-replay measures QUIC multipath : Support multiple network paths Datagram support : QUIC DATAGRAM frames for low-latency data Priority trees : HTTP/3 priority and scheduling Additional Protocols HTTP/1.1 support : Serve HTTP/1.1 clients TCP proxy mode : Layer 4 TCP proxying UDP proxy : Forward UDP traffic MQTT support : IoT protocol support Optimizations Zero-copy : Eliminate unnecessary data copies Kernel bypass : AF_XDP or DPDK integration Hardware offload : TLS offload to NICs eBPF : Use eBPF for packet filtering and routing Implementation Priorities High Priority (Next 3 months) Async data plane - unblock main thread Metrics export - essential for production Configuration hot reload - reduce operational friction Streaming bodies - reduce memory usage TLS peer verification - production security Medium Priority (3-6 months) Circuit breakers - improve reliability Distributed tracing - debugging complex issues Rate limiting - protect backends Health check improvements - reduce contention Admin API - operational visibility Low Priority (6+ months) Dynamic backend discovery - integration complexity Advanced load balancing - diminishing returns WebAssembly plugins - adds complexity Protocol extensions - limited immediate value Multi-tenancy - niche use case Technical Debt Current Known Issues Blocking backend calls : Main thread blocks during HTTP/2 requests Full body buffering : High memory usage for large requests/responses Consistent hash rebuilds : Ring rebuilt on every request No metrics export : Metrics collected but not exposed Health check contention : Shares connection pool with production traffic Single-threaded : QUIC processing limited to one thread No TLS verification : Development-only security posture Refactoring Needs Error handling : Unify error types across crates Configuration : Type-safe config builders Testing : Expand integration test coverage Documentation : API documentation and examples Logging : Reduce debug log verbosity in hot path Non-Goals Features explicitly not planned: Full service mesh : Focus remains on edge proxying Content caching : Use CDN or dedicated cache WAF capabilities : Use dedicated security tools Database proxying : Use specialized database proxies Custom protocols : Stick to HTTP family Contributing Contributions are welcome. See contributing guide for development setup and guidelines. Priority areas for contributions: Metrics export (Prometheus) Streaming request/response bodies Configuration hot reload Integration tests Documentation and examples","title":"Roadmap"},{"location":"roadmap/#current-status","text":"Experimental. Core features functional and tested. Spooky can terminate HTTP/3 connections, forward to HTTP/2 backends, and perform load balancing with health checks. It is not production-ready \u2014 significant known limitations exist (see Technical Debt below).","title":"Current Status"},{"location":"roadmap/#completed","text":"HTTP/3 termination via quiche HTTP/2 backend connectivity Path and host-based routing Multiple load balancing algorithms (random, round-robin, consistent hash) Active health checking with automatic backend management Per-upstream configuration and routing Connection ID management and QUIC packet routing TLS 1.3 with certificate chain loading Structured logging with multiple levels Configuration validation at startup Graceful shutdown with connection draining","title":"Completed"},{"location":"roadmap/#phase-1-operational-hardening","text":"Goal : Expand operational and scalability capabilities for production deployments.","title":"Phase 1: Operational Hardening"},{"location":"roadmap/#performance","text":"Async data plane : Move backend forwarding off the main poll thread Streaming bodies : Implement incremental request/response streaming instead of full buffering Multi-threading : Support multi-threaded QUIC packet processing Connection pooling optimizations : Reduce allocation overhead in HTTP/2 pool","title":"Performance"},{"location":"roadmap/#observability","text":"Metrics export : Prometheus endpoint for scraping metrics Distributed tracing : OpenTelemetry integration Request logging : Per-request structured logs with correlation IDs Connection metrics : Track QUIC RTT, packet loss, stream count","title":"Observability"},{"location":"roadmap/#operational","text":"Configuration hot reload : Reload config on SIGHUP without dropping connections Health check improvements : Separate client pool for probes to avoid contention TLS certificate reload : Automatic reload on certificate rotation Admin API : HTTP endpoint for runtime statistics and control","title":"Operational"},{"location":"roadmap/#reliability","text":"Circuit breaker : Per-backend circuit breakers to prevent cascading failures Retry logic : Configurable request retry with exponential backoff Request timeouts : Per-route timeout configuration Rate limiting : Per-IP and per-route rate limits","title":"Reliability"},{"location":"roadmap/#phase-2-advanced-features","text":"Goal : Add advanced traffic management and operational capabilities.","title":"Phase 2: Advanced Features"},{"location":"roadmap/#traffic-management","text":"Weighted routing : Route percentage of traffic to different upstreams Header-based routing : Route by arbitrary request headers Request rewriting : URL rewriting and header manipulation Compression : Automatic response compression","title":"Traffic Management"},{"location":"roadmap/#load-balancing","text":"Least connections : Track active connections per backend Response time : Route based on backend latency Weighted least connection : Combine weights with connection count Cached consistent hash : Cache hash ring to avoid rebuilds","title":"Load Balancing"},{"location":"roadmap/#security","text":"TLS peer verification : Enable certificate verification for production mTLS support : Client certificate authentication Request validation : Size limits, header validation IP allowlist/blocklist : Simple access control","title":"Security"},{"location":"roadmap/#deployment","text":"Dynamic backend discovery : Service discovery integration (DNS SRV, Consul, etcd) Backend metadata : Tags and labels for flexible routing A/B testing support : Route subset of traffic to experimental backends Canary deployments : Gradually shift traffic to new backend versions","title":"Deployment"},{"location":"roadmap/#phase-3-enterprise-features","text":"Goal : Support large-scale deployments with advanced requirements.","title":"Phase 3: Enterprise Features"},{"location":"roadmap/#multi-tenancy","text":"Namespace isolation : Separate routing tables per tenant Resource limits : Per-tenant connection and request limits Tenant routing : Route by tenant ID or subdomain","title":"Multi-Tenancy"},{"location":"roadmap/#advanced-observability","text":"APM integration : Datadog, New Relic, etc. Custom metrics : User-defined metric collection Traffic replay : Record and replay production traffic Query logs : SQL-like queries over request logs","title":"Advanced Observability"},{"location":"roadmap/#extensions","text":"WebAssembly plugins : Custom routing logic via WASM Lua scripting : Dynamic request/response transformation gRPC support : Native gRPC proxying WebSocket support : WebSocket over HTTP/3","title":"Extensions"},{"location":"roadmap/#high-availability","text":"Connection migration : Support QUIC connection migration State replication : Share connection state across instances Zero-downtime updates : Binary updates without connection loss Multi-region support : Geographic routing and failover","title":"High Availability"},{"location":"roadmap/#phase-4-protocol-extensions","text":"Goal : Support emerging protocols and optimizations.","title":"Phase 4: Protocol Extensions"},{"location":"roadmap/#http3-features","text":"0-RTT support : Enable 0-RTT with proper anti-replay measures QUIC multipath : Support multiple network paths Datagram support : QUIC DATAGRAM frames for low-latency data Priority trees : HTTP/3 priority and scheduling","title":"HTTP/3 Features"},{"location":"roadmap/#additional-protocols","text":"HTTP/1.1 support : Serve HTTP/1.1 clients TCP proxy mode : Layer 4 TCP proxying UDP proxy : Forward UDP traffic MQTT support : IoT protocol support","title":"Additional Protocols"},{"location":"roadmap/#optimizations","text":"Zero-copy : Eliminate unnecessary data copies Kernel bypass : AF_XDP or DPDK integration Hardware offload : TLS offload to NICs eBPF : Use eBPF for packet filtering and routing","title":"Optimizations"},{"location":"roadmap/#implementation-priorities","text":"","title":"Implementation Priorities"},{"location":"roadmap/#high-priority-next-3-months","text":"Async data plane - unblock main thread Metrics export - essential for production Configuration hot reload - reduce operational friction Streaming bodies - reduce memory usage TLS peer verification - production security","title":"High Priority (Next 3 months)"},{"location":"roadmap/#medium-priority-3-6-months","text":"Circuit breakers - improve reliability Distributed tracing - debugging complex issues Rate limiting - protect backends Health check improvements - reduce contention Admin API - operational visibility","title":"Medium Priority (3-6 months)"},{"location":"roadmap/#low-priority-6-months","text":"Dynamic backend discovery - integration complexity Advanced load balancing - diminishing returns WebAssembly plugins - adds complexity Protocol extensions - limited immediate value Multi-tenancy - niche use case","title":"Low Priority (6+ months)"},{"location":"roadmap/#technical-debt","text":"","title":"Technical Debt"},{"location":"roadmap/#current-known-issues","text":"Blocking backend calls : Main thread blocks during HTTP/2 requests Full body buffering : High memory usage for large requests/responses Consistent hash rebuilds : Ring rebuilt on every request No metrics export : Metrics collected but not exposed Health check contention : Shares connection pool with production traffic Single-threaded : QUIC processing limited to one thread No TLS verification : Development-only security posture","title":"Current Known Issues"},{"location":"roadmap/#refactoring-needs","text":"Error handling : Unify error types across crates Configuration : Type-safe config builders Testing : Expand integration test coverage Documentation : API documentation and examples Logging : Reduce debug log verbosity in hot path","title":"Refactoring Needs"},{"location":"roadmap/#non-goals","text":"Features explicitly not planned: Full service mesh : Focus remains on edge proxying Content caching : Use CDN or dedicated cache WAF capabilities : Use dedicated security tools Database proxying : Use specialized database proxies Custom protocols : Stick to HTTP family","title":"Non-Goals"},{"location":"roadmap/#contributing","text":"Contributions are welcome. See contributing guide for development setup and guidelines. Priority areas for contributions: Metrics export (Prometheus) Streaming request/response bodies Configuration hot reload Integration tests Documentation and examples","title":"Contributing"},{"location":"api/overview/","text":"Spooky's programmatic interfaces and configuration APIs. Command Line Interface Basic Usage spooky --config <CONFIG> Options Option Short Type Required Description --config -c string Yes Path to configuration file --version -V boolean No Show version information --help -h boolean No Show help information Examples # Start Spooky with configuration spooky --config /etc/spooky/config.yaml # Show version spooky --version Configuration API Configuration File Format Spooky uses YAML for configuration with the following structure: # Top-level configuration schema version: 1 # Configuration format version listen: # Listener configuration (required) upstream: # Named upstream pools (required) load_balancing?: # Global load balancing (optional) log: # Logging configuration (optional, defaults applied) Type Definitions ListenConfig interface ListenConfig { protocol: \"http3\"; // Only HTTP/3 is supported port: number; address: string; tls: TLSConfig; // TLS is required for HTTP/3 } TLSConfig interface TLSConfig { cert: string; // Path to certificate file key: string; // Path to private key file ca?: string; // Path to CA certificate (client auth) } UpstreamConfig interface UpstreamConfig { route: RouteConfig; // Routing rules (required) load_balancing?: LoadBalancingConfig; // Per-upstream LB (planned, not implemented) backends: BackendConfig[]; // Backend servers (required, at least 1) } RouteConfig interface RouteConfig { host?: string; // Host header to match (optional) path_prefix?: string; // Path prefix to match (optional, but at least one of host/path_prefix required) } BackendConfig interface BackendConfig { id: string; // Unique backend identifier address: string; // Backend address (host:port) weight?: number; // Load balancing weight (default: 100) health_check?: HealthCheckConfig; } HealthCheckConfig interface HealthCheckConfig { path?: string; // Health check endpoint (default: \"/health\") interval?: number; // Check interval in ms (default: 5000) timeout_ms?: number; // Request timeout in ms (default: 1000) success_threshold?: number; // Successes to mark healthy (default: 2) failure_threshold?: number; // Failures to mark unhealthy (default: 3) method?: string; // HTTP method (default: \"GET\") } LoadBalancingConfig interface LoadBalancingConfig { type: \"random\" | \"round-robin\" | \"consistent-hash\"; key?: string; // Hash key source (planned: header:name, cookie:name, query:name, path) } LogConfig interface LogConfig { level?: string; // Log level (default: \"info\") } Supported log levels (in order of verbosity): - whisper - Trace-level logging (most verbose) - haunt - Debug-level logging - spooky - Info-level logging (default) - scream - Warning-level logging - poltergeist - Error-level logging - silence - Logging disabled Standard log levels are also supported: - trace , debug , info , warn , error , off Metrics System Spooky maintains internal performance and operational metrics tracked via atomic counters. Current Available Metrics The following metrics are currently tracked in-memory within the Metrics structure: Request Metrics requests_total (AtomicU64) - Total number of requests received and processed requests_success (AtomicU64) - Number of requests completed successfully with 2xx responses requests_failure (AtomicU64) - Number of requests that failed or returned error responses Backend Metrics backend_timeouts (AtomicU64) - Number of requests that timed out waiting for backend response backend_errors (AtomicU64) - Number of backend connection or communication errors Metrics Implementation Details All metrics use atomic operations with relaxed ordering for high-performance lock-free increment operations. Metrics are incremented through dedicated methods: inc_total() - Increment total request counter inc_success() - Increment successful request counter inc_failure() - Increment failed request counter inc_timeout() - Increment backend timeout counter inc_backend_error() - Increment backend error counter Future: Metrics API Endpoint Status : Planned feature A metrics exposition endpoint is planned for future implementation that will expose collected metrics for monitoring and observability systems. Planned Endpoint GET /metrics The endpoint will provide Prometheus-compatible metric exposition format, making Spooky metrics accessible to standard monitoring and alerting infrastructure. Planned Metric Categories Future implementations may include: Request metrics : Total requests, success/failure rates, request duration histograms Connection metrics : Active QUIC connections, HTTP/2 connection pool statistics Backend health metrics : Backend availability, health check results, response times Load balancing metrics : Backend selection distribution, algorithm performance System metrics : Process resource usage, runtime statistics Admin API Status : Future capability Administrative API endpoints are planned for runtime management and observability: Planned Admin Capabilities Health endpoint : Spooky instance health status and backend health aggregation Metrics endpoint : Real-time operational metrics exposition Configuration reload : Dynamic configuration updates without restart Connection management : View active connections, drain connections gracefully Backend management : Enable/disable backends, adjust weights dynamically Health Check API Backend Health Checks Spooky performs HTTP health checks against configured backends. Request Format GET /health HTTP/1.1 Host: backend.example.com:8080 User-Agent: spooky/0.1.0 Expected Response Healthy Response (2xx status code): HTTP/1.1 200 OK Content-Type: application/json {\"status\": \"healthy\", \"timestamp\": \"2024-01-01T12:00:00Z\"} Unhealthy Response (non-2xx status code): HTTP/1.1 503 Service Unavailable Content-Type: application/json {\"status\": \"unhealthy\", \"reason\": \"database connection failed\"} Spooky Health Endpoint Status : Future feature A dedicated health endpoint for the Spooky instance is planned for future implementation: GET /health HTTP/1.1 Planned response format: { \"status\": \"healthy\", \"version\": \"0.1.0\", \"uptime\": 3600, \"backends\": { \"web-01\": \"healthy\", \"web-02\": \"healthy\", \"api-01\": \"unhealthy\" } } Configuration Validation Startup Validation Configuration validation is performed automatically at startup before the QUIC listener is initialized. The validation process verifies: Configuration file format and syntax Required field presence Value type correctness File path existence (certificates, keys) Network address format validity Exit Codes 0 : Configuration validated successfully, normal operation 1 : Configuration validation failed or runtime error occurred Validation Output Valid Configuration : Configuration validation successful Spooky is starting Listening on 0.0.0.0:9889 Invalid Configuration : Error loading config: <error details> or Configuration validation failed. Exiting... Error Codes HTTP Status Codes Spooky may return the following HTTP status codes to clients: 200 OK : Request successful (forwarded from backend) 400 Bad Request : Malformed or invalid request 500 Internal Server Error : Internal proxy error (e.g., TLS configuration issues) 502 Bad Gateway : Backend server error 503 Service Unavailable : Backend timeout or no healthy backends available Logging Log Format Spooky uses the env_logger logging implementation with timestamped output. All log messages are written to standard output (stdout) with the following format: [YYYY-MM-DD HH:MM:SS] [LEVEL] [module::path] message Log Output Examples [2026-02-18 14:23:45] [INFO] [spooky] Spooky is starting [2026-02-18 14:23:45] [DEBUG] [spooky_edge::quic_listener] Listening on 0.0.0.0:9889 [2026-02-18 14:23:45] [DEBUG] [spooky_edge::quic_listener] Certificate loaded successfully [2026-02-18 14:23:50] [INFO] [spooky_edge::quic_listener] Length of data received: 1200 [2026-02-18 14:23:50] [DEBUG] [spooky_edge::quic_listener] Packet DCID (len=8): [00 01 02 03 04 05 06 07], type: Initial, active connections: 1 [2026-02-18 14:25:30] [INFO] [spooky_edge::quic_listener] Draining connections [2026-02-18 14:25:35] [INFO] [spooky] Spooky shutdown complete Log Levels Log verbosity is configured via the log.level configuration parameter. The following levels are available (ordered from most to least verbose): Level Standard Equivalent Use Case whisper trace Extremely detailed diagnostic information including packet hex dumps haunt debug Detailed diagnostic information for troubleshooting spooky info General informational messages about normal operation scream warn Warning messages for potentially problematic situations poltergeist error Error messages for failures and exceptions silence off Disable all logging output Standard log level names ( trace , debug , info , warn , error , off ) are also supported for compatibility. Log Configuration Configure logging in the configuration file: log: level: \"spooky\" # or \"haunt\", \"whisper\", etc. Environment Variable Control The env_logger implementation respects the RUST_LOG environment variable, which can be used to override configuration file settings or enable module-specific logging: # Override global log level RUST_LOG=debug spooky --config config.yaml # Enable debug logging for specific modules RUST_LOG=spooky_edge=debug,spooky_transport=info spooky --config config.yaml # Trace all modules RUST_LOG=trace spooky --config config.yaml Environment Variables Note : Environment variable interpolation in configuration files is not currently supported. Configuration values must be provided literally in the YAML file. For dynamic configuration, consider using external configuration management tools or templating the configuration file before loading.","title":"Overview"},{"location":"api/overview/#command-line-interface","text":"","title":"Command Line Interface"},{"location":"api/overview/#basic-usage","text":"spooky --config <CONFIG>","title":"Basic Usage"},{"location":"api/overview/#options","text":"Option Short Type Required Description --config -c string Yes Path to configuration file --version -V boolean No Show version information --help -h boolean No Show help information","title":"Options"},{"location":"api/overview/#examples","text":"# Start Spooky with configuration spooky --config /etc/spooky/config.yaml # Show version spooky --version","title":"Examples"},{"location":"api/overview/#configuration-api","text":"","title":"Configuration API"},{"location":"api/overview/#configuration-file-format","text":"Spooky uses YAML for configuration with the following structure: # Top-level configuration schema version: 1 # Configuration format version listen: # Listener configuration (required) upstream: # Named upstream pools (required) load_balancing?: # Global load balancing (optional) log: # Logging configuration (optional, defaults applied)","title":"Configuration File Format"},{"location":"api/overview/#type-definitions","text":"","title":"Type Definitions"},{"location":"api/overview/#listenconfig","text":"interface ListenConfig { protocol: \"http3\"; // Only HTTP/3 is supported port: number; address: string; tls: TLSConfig; // TLS is required for HTTP/3 }","title":"ListenConfig"},{"location":"api/overview/#tlsconfig","text":"interface TLSConfig { cert: string; // Path to certificate file key: string; // Path to private key file ca?: string; // Path to CA certificate (client auth) }","title":"TLSConfig"},{"location":"api/overview/#upstreamconfig","text":"interface UpstreamConfig { route: RouteConfig; // Routing rules (required) load_balancing?: LoadBalancingConfig; // Per-upstream LB (planned, not implemented) backends: BackendConfig[]; // Backend servers (required, at least 1) }","title":"UpstreamConfig"},{"location":"api/overview/#routeconfig","text":"interface RouteConfig { host?: string; // Host header to match (optional) path_prefix?: string; // Path prefix to match (optional, but at least one of host/path_prefix required) }","title":"RouteConfig"},{"location":"api/overview/#backendconfig","text":"interface BackendConfig { id: string; // Unique backend identifier address: string; // Backend address (host:port) weight?: number; // Load balancing weight (default: 100) health_check?: HealthCheckConfig; }","title":"BackendConfig"},{"location":"api/overview/#healthcheckconfig","text":"interface HealthCheckConfig { path?: string; // Health check endpoint (default: \"/health\") interval?: number; // Check interval in ms (default: 5000) timeout_ms?: number; // Request timeout in ms (default: 1000) success_threshold?: number; // Successes to mark healthy (default: 2) failure_threshold?: number; // Failures to mark unhealthy (default: 3) method?: string; // HTTP method (default: \"GET\") }","title":"HealthCheckConfig"},{"location":"api/overview/#loadbalancingconfig","text":"interface LoadBalancingConfig { type: \"random\" | \"round-robin\" | \"consistent-hash\"; key?: string; // Hash key source (planned: header:name, cookie:name, query:name, path) }","title":"LoadBalancingConfig"},{"location":"api/overview/#logconfig","text":"interface LogConfig { level?: string; // Log level (default: \"info\") } Supported log levels (in order of verbosity): - whisper - Trace-level logging (most verbose) - haunt - Debug-level logging - spooky - Info-level logging (default) - scream - Warning-level logging - poltergeist - Error-level logging - silence - Logging disabled Standard log levels are also supported: - trace , debug , info , warn , error , off","title":"LogConfig"},{"location":"api/overview/#metrics-system","text":"Spooky maintains internal performance and operational metrics tracked via atomic counters.","title":"Metrics System"},{"location":"api/overview/#current-available-metrics","text":"The following metrics are currently tracked in-memory within the Metrics structure:","title":"Current Available Metrics"},{"location":"api/overview/#request-metrics","text":"requests_total (AtomicU64) - Total number of requests received and processed requests_success (AtomicU64) - Number of requests completed successfully with 2xx responses requests_failure (AtomicU64) - Number of requests that failed or returned error responses","title":"Request Metrics"},{"location":"api/overview/#backend-metrics","text":"backend_timeouts (AtomicU64) - Number of requests that timed out waiting for backend response backend_errors (AtomicU64) - Number of backend connection or communication errors","title":"Backend Metrics"},{"location":"api/overview/#metrics-implementation-details","text":"All metrics use atomic operations with relaxed ordering for high-performance lock-free increment operations. Metrics are incremented through dedicated methods: inc_total() - Increment total request counter inc_success() - Increment successful request counter inc_failure() - Increment failed request counter inc_timeout() - Increment backend timeout counter inc_backend_error() - Increment backend error counter","title":"Metrics Implementation Details"},{"location":"api/overview/#future-metrics-api-endpoint","text":"Status : Planned feature A metrics exposition endpoint is planned for future implementation that will expose collected metrics for monitoring and observability systems.","title":"Future: Metrics API Endpoint"},{"location":"api/overview/#planned-endpoint","text":"GET /metrics The endpoint will provide Prometheus-compatible metric exposition format, making Spooky metrics accessible to standard monitoring and alerting infrastructure.","title":"Planned Endpoint"},{"location":"api/overview/#planned-metric-categories","text":"Future implementations may include: Request metrics : Total requests, success/failure rates, request duration histograms Connection metrics : Active QUIC connections, HTTP/2 connection pool statistics Backend health metrics : Backend availability, health check results, response times Load balancing metrics : Backend selection distribution, algorithm performance System metrics : Process resource usage, runtime statistics","title":"Planned Metric Categories"},{"location":"api/overview/#admin-api","text":"Status : Future capability Administrative API endpoints are planned for runtime management and observability:","title":"Admin API"},{"location":"api/overview/#planned-admin-capabilities","text":"Health endpoint : Spooky instance health status and backend health aggregation Metrics endpoint : Real-time operational metrics exposition Configuration reload : Dynamic configuration updates without restart Connection management : View active connections, drain connections gracefully Backend management : Enable/disable backends, adjust weights dynamically","title":"Planned Admin Capabilities"},{"location":"api/overview/#health-check-api","text":"","title":"Health Check API"},{"location":"api/overview/#backend-health-checks","text":"Spooky performs HTTP health checks against configured backends.","title":"Backend Health Checks"},{"location":"api/overview/#request-format","text":"GET /health HTTP/1.1 Host: backend.example.com:8080 User-Agent: spooky/0.1.0","title":"Request Format"},{"location":"api/overview/#expected-response","text":"Healthy Response (2xx status code): HTTP/1.1 200 OK Content-Type: application/json {\"status\": \"healthy\", \"timestamp\": \"2024-01-01T12:00:00Z\"} Unhealthy Response (non-2xx status code): HTTP/1.1 503 Service Unavailable Content-Type: application/json {\"status\": \"unhealthy\", \"reason\": \"database connection failed\"}","title":"Expected Response"},{"location":"api/overview/#spooky-health-endpoint","text":"Status : Future feature A dedicated health endpoint for the Spooky instance is planned for future implementation: GET /health HTTP/1.1 Planned response format: { \"status\": \"healthy\", \"version\": \"0.1.0\", \"uptime\": 3600, \"backends\": { \"web-01\": \"healthy\", \"web-02\": \"healthy\", \"api-01\": \"unhealthy\" } }","title":"Spooky Health Endpoint"},{"location":"api/overview/#configuration-validation","text":"","title":"Configuration Validation"},{"location":"api/overview/#startup-validation","text":"Configuration validation is performed automatically at startup before the QUIC listener is initialized. The validation process verifies: Configuration file format and syntax Required field presence Value type correctness File path existence (certificates, keys) Network address format validity","title":"Startup Validation"},{"location":"api/overview/#exit-codes","text":"0 : Configuration validated successfully, normal operation 1 : Configuration validation failed or runtime error occurred","title":"Exit Codes"},{"location":"api/overview/#validation-output","text":"Valid Configuration : Configuration validation successful Spooky is starting Listening on 0.0.0.0:9889 Invalid Configuration : Error loading config: <error details> or Configuration validation failed. Exiting...","title":"Validation Output"},{"location":"api/overview/#error-codes","text":"","title":"Error Codes"},{"location":"api/overview/#http-status-codes","text":"Spooky may return the following HTTP status codes to clients: 200 OK : Request successful (forwarded from backend) 400 Bad Request : Malformed or invalid request 500 Internal Server Error : Internal proxy error (e.g., TLS configuration issues) 502 Bad Gateway : Backend server error 503 Service Unavailable : Backend timeout or no healthy backends available","title":"HTTP Status Codes"},{"location":"api/overview/#logging","text":"","title":"Logging"},{"location":"api/overview/#log-format","text":"Spooky uses the env_logger logging implementation with timestamped output. All log messages are written to standard output (stdout) with the following format: [YYYY-MM-DD HH:MM:SS] [LEVEL] [module::path] message","title":"Log Format"},{"location":"api/overview/#log-output-examples","text":"[2026-02-18 14:23:45] [INFO] [spooky] Spooky is starting [2026-02-18 14:23:45] [DEBUG] [spooky_edge::quic_listener] Listening on 0.0.0.0:9889 [2026-02-18 14:23:45] [DEBUG] [spooky_edge::quic_listener] Certificate loaded successfully [2026-02-18 14:23:50] [INFO] [spooky_edge::quic_listener] Length of data received: 1200 [2026-02-18 14:23:50] [DEBUG] [spooky_edge::quic_listener] Packet DCID (len=8): [00 01 02 03 04 05 06 07], type: Initial, active connections: 1 [2026-02-18 14:25:30] [INFO] [spooky_edge::quic_listener] Draining connections [2026-02-18 14:25:35] [INFO] [spooky] Spooky shutdown complete","title":"Log Output Examples"},{"location":"api/overview/#log-levels","text":"Log verbosity is configured via the log.level configuration parameter. The following levels are available (ordered from most to least verbose): Level Standard Equivalent Use Case whisper trace Extremely detailed diagnostic information including packet hex dumps haunt debug Detailed diagnostic information for troubleshooting spooky info General informational messages about normal operation scream warn Warning messages for potentially problematic situations poltergeist error Error messages for failures and exceptions silence off Disable all logging output Standard log level names ( trace , debug , info , warn , error , off ) are also supported for compatibility.","title":"Log Levels"},{"location":"api/overview/#log-configuration","text":"Configure logging in the configuration file: log: level: \"spooky\" # or \"haunt\", \"whisper\", etc.","title":"Log Configuration"},{"location":"api/overview/#environment-variable-control","text":"The env_logger implementation respects the RUST_LOG environment variable, which can be used to override configuration file settings or enable module-specific logging: # Override global log level RUST_LOG=debug spooky --config config.yaml # Enable debug logging for specific modules RUST_LOG=spooky_edge=debug,spooky_transport=info spooky --config config.yaml # Trace all modules RUST_LOG=trace spooky --config config.yaml","title":"Environment Variable Control"},{"location":"api/overview/#environment-variables","text":"Note : Environment variable interpolation in configuration files is not currently supported. Configuration values must be provided literally in the YAML file. For dynamic configuration, consider using external configuration management tools or templating the configuration file before loading.","title":"Environment Variables"},{"location":"architecture/components/","text":"This document provides a detailed breakdown of Spooky's modular component architecture, including responsibilities, APIs, and implementation details for each crate. Component Overview Spooky is organized as a Rust workspace with the following crates: Crate Path Responsibility spooky spooky/ Main binary and application lifecycle spooky-edge crates/edge/ QUIC listener and HTTP/3 session management spooky-bridge crates/bridge/ HTTP/3 to HTTP/2 protocol conversion spooky-transport crates/transport/ HTTP/2 client and connection pooling spooky-lb crates/lb/ Load balancing algorithms and health tracking spooky-config crates/config/ Configuration parsing and validation spooky-utils crates/utils/ TLS utilities and logging setup spooky-errors crates/errors/ Shared error types (minimal) Main Application ( spooky ) Responsibilities Command-line argument parsing Configuration file loading Logger initialization QUIC listener creation Signal handling for graceful shutdown Event loop coordination Key Types struct Cli { config: Option<String>, } Main Flow #[tokio::main] async fn main() { // 1. Parse CLI arguments let cli = Cli::parse(); // 2. Load configuration let config = spooky_config::loader::read_config(&config_path)?; // 3. Initialize logger spooky_utils::logger::init_logger(&config.log.level); // 4. Validate configuration spooky_config::validator::validate(&config); // 5. Create QUIC listener let mut listener = spooky_edge::QUICListener::new(config)?; // 6. Setup shutdown handler let shutdown = Arc::new(AtomicBool::new(false)); tokio::spawn(signal_handler(shutdown.clone())); // 7. Main event loop while !shutdown.load(Ordering::Relaxed) { listener.poll(); } // 8. Graceful shutdown listener.start_draining(); while !listener.drain_complete() { listener.poll(); } } Dependencies clap : CLI argument parsing tokio : Async runtime log : Logging facade Edge Listener ( spooky-edge ) Responsibilities UDP socket binding and management QUIC connection lifecycle (handshake, packet processing, closure) HTTP/3 session establishment via quiche Stream state management Request envelope construction Backend request forwarding Response streaming back to client Connection draining for graceful shutdown Metrics collection Key Types pub struct QUICListener { pub socket: UdpSocket, pub config: Config, pub quic_config: quiche::Config, pub h3_config: Arc<quiche::h3::Config>, pub h2_pool: Arc<H2Pool>, pub upstream_pools: HashMap<String, Arc<Mutex<UpstreamPool>>>, pub load_balancer: LoadBalancing, pub metrics: Metrics, pub draining: bool, pub drain_start: Option<Instant>, pub recv_buf: [u8; 65535], pub send_buf: [u8; 65535], pub connections: HashMap<Vec<u8>, QuicConnection>, } pub struct QuicConnection { pub quic: quiche::Connection, pub h3: Option<quiche::h3::Connection>, pub h3_config: Arc<quiche::h3::Config>, pub streams: HashMap<u64, RequestEnvelope>, pub peer_address: SocketAddr, pub last_activity: Instant, } pub struct RequestEnvelope { pub method: String, pub path: String, pub authority: Option<String>, pub headers: Vec<(Vec<u8>, Vec<u8>)>, pub body: Vec<u8>, pub start: Instant, } pub struct Metrics { pub requests_total: AtomicU64, pub requests_success: AtomicU64, pub requests_failure: AtomicU64, pub backend_timeouts: AtomicU64, pub backend_errors: AtomicU64, } Public API impl QUICListener { /// Create new QUIC listener from configuration pub fn new(config: Config) -> Result<Self, ProxyError>; /// Process pending packets and events (main event loop) pub fn poll(&mut self); /// Begin graceful shutdown sequence pub fn start_draining(&mut self); /// Check if drain is complete pub fn drain_complete(&self) -> bool; } impl Metrics { pub fn inc_total(&self); pub fn inc_success(&self); pub fn inc_failure(&self); pub fn inc_timeout(&self); pub fn inc_backend_error(&self); } Implementation Details Socket Management: - UDP socket bound to configured address:port - Non-blocking mode for poll-based processing - Fixed-size receive/send buffers (65535 bytes, max UDP payload) QUIC Configuration: - Built using quiche::Config - TLS credentials loaded from configured cert/key paths - HTTP/3 ALPN (\"h3\") configured - Connection ID generation using HMAC-SHA256 (stateless retry tokens) Connection Tracking: - Connections keyed by Server Connection ID (SCID) - Connection state includes QUIC and HTTP/3 layers - Last activity timestamp for idle timeout detection Request Processing: 1. Receive UDP datagram 2. Identify connection by DCID or create new connection 3. Feed packet to quiche 4. Poll HTTP/3 events 5. On headers: create RequestEnvelope 6. On body data: accumulate in envelope 7. On stream finished: process complete request 8. Route to upstream, select backend, forward request 9. Stream response back to client Upstream Routing: - Routes are matched by path prefix and optional host - Longest matching path wins for overlapping routes - No match returns error to client Health Integration: - Successful backend responses call mark_success() - Failed/timed out responses call mark_failure() - Health transitions are logged Graceful Shutdown: - start_draining() sets draining flag and records start time - No new requests accepted during drain - drain_complete() returns true when all connections closed or timeout reached - Drain timeout: 5 seconds Error Handling pub enum ProxyError { Bridge(BridgeError), Transport(String), Timeout, Tls(String), } Errors are logged and result in appropriate HTTP error responses to clients where possible. Dependencies quiche : QUIC and HTTP/3 implementation tokio : Async backend request execution bytes : Efficient byte buffer handling http : HTTP types for request construction spooky-config : Configuration types spooky-lb : Load balancing and health tracking spooky-bridge : Protocol conversion spooky-transport : HTTP/2 backend communication Protocol Bridge ( spooky-bridge ) Responsibilities Convert HTTP/3 requests to HTTP/2 format Normalize headers between protocol versions Handle pseudo-headers (:method, :path, :authority, :scheme) Filter hop-by-hop headers Construct proper HTTP/2 request objects Key Types pub enum BridgeError { InvalidMethod, InvalidUri, InvalidHeader, Build(http::Error), } Public API pub fn build_h2_request( backend: &str, method: &str, path: &str, headers: &[(Vec<u8>, Vec<u8>)], body: &[u8], ) -> Result<Request<Full<Bytes>>, BridgeError>; Implementation Details Method Conversion: - HTTP/3 method string parsed into http::Method - Validation ensures method is valid HTTP method URI Construction: - Backend address combined with request path - Format: http://{backend}{path} - Empty path defaults to \"/\" - Parsed into http::Uri Header Processing: 1. Skip pseudo-headers (starting with ':') 2. Filter hop-by-hop headers (Connection, Keep-Alive, Transfer-Encoding, Upgrade) 3. Skip Content-Length (recalculated from body) 4. Ensure Host header present (from authority or backend address) 5. Set Content-Length if body is non-empty Body Handling: - Body copied into Full<Bytes> for hyper compatibility - Content-Length header set based on actual body size Edge Cases: - Missing Host: defaults to backend address - Empty path: defaults to \"/\" - Invalid headers: return BridgeError::InvalidHeader Error Handling All errors are propagated to caller with specific error types. Invalid requests are not sent to backends. Dependencies http : HTTP types (Request, Method, Uri, HeaderName, HeaderValue) http-body-util : Body utilities (Full) bytes : Byte buffer types quiche : HTTP/3 header types (NameValue) Transport Layer ( spooky-transport ) Responsibilities Maintain HTTP/2 connections to backend servers Connection pooling and reuse Request multiplexing over HTTP/2 connections Flow control via semaphore-based concurrency limiting Request forwarding with timeout handling Key Types pub struct H2Pool { backends: HashMap<String, BackendHandle>, } struct BackendHandle { client: H2Client, inflight: Arc<Semaphore>, } pub struct H2Client { client: Client<HttpConnector, Full<Bytes>>, } pub enum PoolError { UnknownBackend(String), Send(hyper_util::client::legacy::Error), } Public API impl H2Pool { /// Create new pool with specified backends and concurrency limit pub fn new<I>(backends: I, max_inflight: usize) -> Self where I: IntoIterator<Item = String>; /// Check if backend exists in pool pub fn has_backend(&self, backend: &str) -> bool; /// Send request to specified backend pub async fn send( &self, backend: &str, req: Request<Full<Bytes>>, ) -> Result<Response<Incoming>, PoolError>; } impl H2Client { /// Create new HTTP/2-only client pub fn new() -> Self; /// Send request over HTTP/2 pub async fn send( &self, req: Request<Full<Bytes>>, ) -> Result<Response<Incoming>, hyper_util::client::legacy::Error>; } Implementation Details Connection Pooling: - One BackendHandle per backend address - Handle contains HTTP/2 client and concurrency limiter - Connections created lazily on first request - Connection reuse managed automatically by hyper Concurrency Control: - Semaphore limits concurrent requests per backend - Default: 64 concurrent requests per backend - Backpressure applied when limit reached (async wait) - Permit released when request completes HTTP/2 Client: - Built using hyper legacy client API - HTTP/2-only mode enforced - HttpConnector with HTTP enforcement disabled (allows non-HTTPS URIs) - TokioExecutor for spawning connection tasks Error Handling: - Unknown backend: PoolError::UnknownBackend - Connection errors: PoolError::Send wrapping hyper error - Timeout handled by caller (edge layer) Dependencies hyper : HTTP client implementation hyper-util : Connection pooling and legacy client http-body-util : Body utilities tokio : Async runtime bytes : Byte buffers Load Balancer ( spooky-lb ) Responsibilities Backend selection algorithms (Random, Round Robin, Consistent Hash) Health state tracking per backend Failure threshold detection Recovery threshold tracking Cooldown period management Upstream pool management Key Types pub struct BackendState { address: String, weight: u32, health_check: HealthCheck, consecutive_failures: u32, health_state: HealthState, } enum HealthState { Healthy, Unhealthy { until: Instant, successes: u32 }, } pub enum HealthTransition { BecameHealthy, BecameUnhealthy, } pub struct BackendPool { backends: Vec<BackendState>, } pub struct UpstreamPool { pub pool: BackendPool, pub strategy: String, } pub enum LoadBalancing { RoundRobin(RoundRobin), ConsistentHash(ConsistentHash), Random(Random), } pub struct RoundRobin { next: usize, } pub struct ConsistentHash { replicas: u32, } pub struct Random; Public API impl BackendState { pub fn new(backend: &Backend) -> Self; pub fn is_healthy(&self) -> bool; pub fn address(&self) -> &str; pub fn health_check(&self) -> &HealthCheck; pub fn weight(&self) -> u32; pub fn record_success(&mut self) -> Option<HealthTransition>; pub fn record_failure(&mut self) -> Option<HealthTransition>; } impl BackendPool { pub fn new_from_states(backends: Vec<BackendState>) -> Self; pub fn len(&self) -> usize; pub fn is_empty(&self) -> bool; pub fn address(&self, index: usize) -> Option<&str>; pub fn mark_success(&mut self, index: usize) -> Option<HealthTransition>; pub fn mark_failure(&mut self, index: usize) -> Option<HealthTransition>; pub fn health_check(&self, index: usize) -> Option<HealthCheck>; pub fn healthy_indices(&self) -> Vec<usize>; pub fn all_indices(&self) -> Vec<usize>; pub fn backend(&self, index: usize) -> Option<&BackendState>; } impl UpstreamPool { pub fn from_upstream(upstream: &Upstream) -> Result<Self, String>; } impl LoadBalancing { pub fn from_config(value: &str) -> Result<Self, String>; pub fn pick(&mut self, key: &str, pool: &UpstreamPool) -> Option<usize>; } impl RoundRobin { pub fn new() -> Self; pub fn pick(&mut self, pool: &BackendPool) -> Option<usize>; } impl ConsistentHash { pub fn new(replicas: u32) -> Self; pub fn pick(&self, key: &str, pool: &BackendPool) -> Option<usize>; } impl Random { pub fn new() -> Self; pub fn pick(&mut self, pool: &BackendPool) -> Option<usize>; } Implementation Details Health State Machine: Healthy \u2502 \u2502 (failures >= failure_threshold) \u25bc Unhealthy { until: cooldown_end, successes: 0 } \u2502 \u2502 (now >= cooldown_end && successes >= success_threshold) \u25bc Healthy Health Tracking: - record_success() : Resets consecutive failures if healthy; increments success counter if unhealthy - record_failure() : Increments consecutive failures; transitions to unhealthy when threshold reached - Transitions return Option<HealthTransition> for logging Backend Selection: - Only healthy backends are candidates - If no healthy backends, returns None - Each algorithm filters to healthy backends before selection Round Robin: - Maintains next index counter - Wraps around when reaching end of healthy backends - Ensures even distribution across healthy backends Consistent Hash: - Builds hash ring with virtual nodes (replicas) - Replica count = base_replicas * backend_weight - Hash function: FNV-1a (fast, good distribution) - Key is hashed, closest node on ring is selected - Same key always routes to same backend (session affinity) Random: - Simple random selection from healthy backends - Uses thread-local RNG for performance - Uniform distribution Weight Support: - Weights affect consistent hash replica count - Higher weight = more virtual nodes = more traffic - Round robin and random ignore weights currently Algorithm Selection Supported configuration strings: - \"round-robin\", \"round_robin\", \"rr\" \u2192 RoundRobin - \"consistent-hash\", \"consistent_hash\", \"ch\" \u2192 ConsistentHash - \"random\" \u2192 Random Default replicas for consistent hash: 64 Dependencies rand : Random number generation spooky-config : Configuration types (Backend, HealthCheck, Upstream) Standard library: BTreeMap for hash ring, Duration/Instant for timing Configuration System ( spooky-config ) Responsibilities YAML configuration parsing Configuration structure definitions Default value provision Configuration validation Error reporting for invalid configurations Key Types pub struct Config { pub version: u32, pub listen: Listen, pub upstream: HashMap<String, Upstream>, pub load_balancing: Option<LoadBalancing>, pub log: Log, } pub struct Listen { pub protocol: String, pub port: u32, pub address: String, pub tls: Tls, } pub struct Tls { pub cert: String, pub key: String, } pub struct Upstream { pub load_balancing: LoadBalancing, pub route: RouteMatch, pub backends: Vec<Backend>, } pub struct Backend { pub id: String, pub address: String, pub weight: u32, pub health_check: HealthCheck, } pub struct RouteMatch { pub host: Option<String>, pub path_prefix: Option<String>, pub method: Option<String>, } pub struct HealthCheck { pub path: String, pub interval: u64, pub timeout_ms: u64, pub failure_threshold: u32, pub success_threshold: u32, pub cooldown_ms: u64, } pub struct LoadBalancing { pub lb_type: String, pub key: Option<String>, } pub struct Log { pub level: String, } Public API // loader.rs pub fn read_config(path: &str) -> Result<Config, String>; // validator.rs pub fn validate(config: &Config) -> bool; Implementation Details Configuration Loading: 1. Read file from path 2. Parse YAML via serde_yaml 3. Apply default values via serde defaults 4. Return Config or error message Default Values: - version: 1 - protocol: \"http3\" - port: 9889 - address: \"0.0.0.0\" - log level: \"info\" - backend weight: 100 - health check path: \"/health\" - health check interval: 5000ms - health timeout: 1000ms - failure threshold: 3 - success threshold: 2 - cooldown: 5000ms Validation Checks: 1. TLS certificate file exists and is readable 2. TLS key file exists and is readable 3. At least one upstream configured 4. Each upstream has at least one backend 5. Backend addresses are non-empty 6. Log level is valid Error Handling: - File not found: clear error message with path - Parse errors: YAML line/column information - Validation errors: specific validation failure message Dependencies serde : Serialization framework serde_yaml : YAML parsing log : Logging Utilities ( spooky-utils ) Responsibilities TLS certificate and key loading Logging initialization Common helper functions Modules tls.rs: pub fn load_certs(path: &str) -> Result<Vec<Certificate>, String>; pub fn load_private_key(path: &str) -> Result<PrivateKey, String>; logger.rs: pub fn init_logger(level: &str); Implementation Details TLS Loading: - Reads PEM files from filesystem - Parses DER-encoded certificates - Validates format - Returns rustls-compatible types Logger Initialization: - Configures env_logger with specified level - Maps custom log levels (if configured) - Enables timestamp and module path Dependencies rustls-pki-types : TLS types env_logger : Logging implementation log : Logging facade Component Interaction Flow Request Path Client HTTP/3 Request \u25bc [spooky-edge::QUICListener] \u251c\u2500 Receive QUIC packets \u251c\u2500 Decode HTTP/3 headers \u2514\u2500 Create RequestEnvelope \u25bc [spooky-edge::quic_listener::find_upstream_for_request] \u251c\u2500 Match path_prefix and host \u2514\u2500 Select upstream pool \u25bc [spooky-lb::LoadBalancing::pick] \u251c\u2500 Filter to healthy backends \u251c\u2500 Apply algorithm \u2514\u2500 Return backend index \u25bc [spooky-bridge::build_h2_request] \u251c\u2500 Convert HTTP/3 \u2192 HTTP/2 \u251c\u2500 Normalize headers \u2514\u2500 Construct Request<Full<Bytes>> \u25bc [spooky-transport::H2Pool::send] \u251c\u2500 Acquire semaphore permit \u251c\u2500 Get backend client \u2514\u2500 Forward request \u25bc Backend HTTP/2 Server \u25bc [spooky-edge::QUICListener] \u251c\u2500 Receive response \u251c\u2500 Update health state \u251c\u2500 Update metrics \u2514\u2500 Stream response to client Configuration Path [spooky::main] \u2514\u2500 Parse CLI args \u25bc [spooky-config::loader::read_config] \u251c\u2500 Read YAML file \u2514\u2500 Parse with serde \u25bc [spooky-config::validator::validate] \u251c\u2500 Check TLS files \u251c\u2500 Validate structure \u2514\u2500 Return bool \u25bc [spooky-edge::QUICListener::new] \u251c\u2500 Load TLS via spooky-utils \u251c\u2500 Create H2Pool with backends \u251c\u2500 Create UpstreamPools \u2514\u2500 Initialize load balancers \u25bc Runtime Testing Strategy Unit Tests Each crate includes unit tests for core functionality: spooky-lb: - Round robin cycling behavior - Consistent hash stability - Health state transitions - Backend recovery - Empty pool handling spooky-bridge: - Header conversion - Pseudo-header handling - URI construction - Error cases spooky-config: - YAML parsing - Default value application - Validation logic spooky-transport: - Pool initialization - Backend existence checks Integration Tests spooky-edge: - Full request/response flow - Health check integration - Upstream routing - Multiple load balancing strategies Test Execution # All tests cargo test # Specific crate cargo test -p spooky-lb # Integration tests only cargo test -p spooky-edge --test lb_integration Performance Optimization Hot Path Optimizations Zero-Copy Where Possible: - UDP receive buffer reused - QUIC packet processing avoids allocations - Header slices avoid string copies Lock-Free Metrics: - AtomicU64 for counters - No mutex on request path Connection Pooling: - HTTP/2 connection reuse - Amortize handshake cost Async I/O: - Backend requests with full body buffering (streaming planned) - Efficient task scheduling via Tokio Memory Management Fixed Buffers: - 64KB receive/send buffers per listener - No per-packet allocation Bounded Collections: - Connection map grows with active connections - Stream map per connection, cleared on completion Reference Counting: - Arc for shared config and pools - Amortize clone cost Deployment Considerations Binary Distribution Single statically-linked binary containing all components. No runtime dependencies except system TLS libraries. Resource Requirements File descriptors: 2 per backend + connection count Memory: ~1-2KB per QUIC connection + connection pools CPU: Scales with core count via Tokio runtime Network: UDP port for client traffic, TCP for backends Operational Checklist TLS certificates and keys readable by process user UDP port accessible for QUIC traffic Backend addresses reachable from proxy File descriptor limits sufficient (ulimit -n) Configuration validated before deployment Logging configured appropriately for environment Monitoring Recommendations Track requests_total, requests_success, requests_failure Monitor backend_timeouts and backend_errors Alert on health state transitions Log slow requests (duration tracking) Monitor connection count Track memory usage growth","title":"Components"},{"location":"architecture/components/#component-overview","text":"Spooky is organized as a Rust workspace with the following crates: Crate Path Responsibility spooky spooky/ Main binary and application lifecycle spooky-edge crates/edge/ QUIC listener and HTTP/3 session management spooky-bridge crates/bridge/ HTTP/3 to HTTP/2 protocol conversion spooky-transport crates/transport/ HTTP/2 client and connection pooling spooky-lb crates/lb/ Load balancing algorithms and health tracking spooky-config crates/config/ Configuration parsing and validation spooky-utils crates/utils/ TLS utilities and logging setup spooky-errors crates/errors/ Shared error types (minimal)","title":"Component Overview"},{"location":"architecture/components/#main-application-spooky","text":"","title":"Main Application (spooky)"},{"location":"architecture/components/#responsibilities","text":"Command-line argument parsing Configuration file loading Logger initialization QUIC listener creation Signal handling for graceful shutdown Event loop coordination","title":"Responsibilities"},{"location":"architecture/components/#key-types","text":"struct Cli { config: Option<String>, }","title":"Key Types"},{"location":"architecture/components/#main-flow","text":"#[tokio::main] async fn main() { // 1. Parse CLI arguments let cli = Cli::parse(); // 2. Load configuration let config = spooky_config::loader::read_config(&config_path)?; // 3. Initialize logger spooky_utils::logger::init_logger(&config.log.level); // 4. Validate configuration spooky_config::validator::validate(&config); // 5. Create QUIC listener let mut listener = spooky_edge::QUICListener::new(config)?; // 6. Setup shutdown handler let shutdown = Arc::new(AtomicBool::new(false)); tokio::spawn(signal_handler(shutdown.clone())); // 7. Main event loop while !shutdown.load(Ordering::Relaxed) { listener.poll(); } // 8. Graceful shutdown listener.start_draining(); while !listener.drain_complete() { listener.poll(); } }","title":"Main Flow"},{"location":"architecture/components/#dependencies","text":"clap : CLI argument parsing tokio : Async runtime log : Logging facade","title":"Dependencies"},{"location":"architecture/components/#edge-listener-spooky-edge","text":"","title":"Edge Listener (spooky-edge)"},{"location":"architecture/components/#responsibilities_1","text":"UDP socket binding and management QUIC connection lifecycle (handshake, packet processing, closure) HTTP/3 session establishment via quiche Stream state management Request envelope construction Backend request forwarding Response streaming back to client Connection draining for graceful shutdown Metrics collection","title":"Responsibilities"},{"location":"architecture/components/#key-types_1","text":"pub struct QUICListener { pub socket: UdpSocket, pub config: Config, pub quic_config: quiche::Config, pub h3_config: Arc<quiche::h3::Config>, pub h2_pool: Arc<H2Pool>, pub upstream_pools: HashMap<String, Arc<Mutex<UpstreamPool>>>, pub load_balancer: LoadBalancing, pub metrics: Metrics, pub draining: bool, pub drain_start: Option<Instant>, pub recv_buf: [u8; 65535], pub send_buf: [u8; 65535], pub connections: HashMap<Vec<u8>, QuicConnection>, } pub struct QuicConnection { pub quic: quiche::Connection, pub h3: Option<quiche::h3::Connection>, pub h3_config: Arc<quiche::h3::Config>, pub streams: HashMap<u64, RequestEnvelope>, pub peer_address: SocketAddr, pub last_activity: Instant, } pub struct RequestEnvelope { pub method: String, pub path: String, pub authority: Option<String>, pub headers: Vec<(Vec<u8>, Vec<u8>)>, pub body: Vec<u8>, pub start: Instant, } pub struct Metrics { pub requests_total: AtomicU64, pub requests_success: AtomicU64, pub requests_failure: AtomicU64, pub backend_timeouts: AtomicU64, pub backend_errors: AtomicU64, }","title":"Key Types"},{"location":"architecture/components/#public-api","text":"impl QUICListener { /// Create new QUIC listener from configuration pub fn new(config: Config) -> Result<Self, ProxyError>; /// Process pending packets and events (main event loop) pub fn poll(&mut self); /// Begin graceful shutdown sequence pub fn start_draining(&mut self); /// Check if drain is complete pub fn drain_complete(&self) -> bool; } impl Metrics { pub fn inc_total(&self); pub fn inc_success(&self); pub fn inc_failure(&self); pub fn inc_timeout(&self); pub fn inc_backend_error(&self); }","title":"Public API"},{"location":"architecture/components/#implementation-details","text":"Socket Management: - UDP socket bound to configured address:port - Non-blocking mode for poll-based processing - Fixed-size receive/send buffers (65535 bytes, max UDP payload) QUIC Configuration: - Built using quiche::Config - TLS credentials loaded from configured cert/key paths - HTTP/3 ALPN (\"h3\") configured - Connection ID generation using HMAC-SHA256 (stateless retry tokens) Connection Tracking: - Connections keyed by Server Connection ID (SCID) - Connection state includes QUIC and HTTP/3 layers - Last activity timestamp for idle timeout detection Request Processing: 1. Receive UDP datagram 2. Identify connection by DCID or create new connection 3. Feed packet to quiche 4. Poll HTTP/3 events 5. On headers: create RequestEnvelope 6. On body data: accumulate in envelope 7. On stream finished: process complete request 8. Route to upstream, select backend, forward request 9. Stream response back to client Upstream Routing: - Routes are matched by path prefix and optional host - Longest matching path wins for overlapping routes - No match returns error to client Health Integration: - Successful backend responses call mark_success() - Failed/timed out responses call mark_failure() - Health transitions are logged Graceful Shutdown: - start_draining() sets draining flag and records start time - No new requests accepted during drain - drain_complete() returns true when all connections closed or timeout reached - Drain timeout: 5 seconds","title":"Implementation Details"},{"location":"architecture/components/#error-handling","text":"pub enum ProxyError { Bridge(BridgeError), Transport(String), Timeout, Tls(String), } Errors are logged and result in appropriate HTTP error responses to clients where possible.","title":"Error Handling"},{"location":"architecture/components/#dependencies_1","text":"quiche : QUIC and HTTP/3 implementation tokio : Async backend request execution bytes : Efficient byte buffer handling http : HTTP types for request construction spooky-config : Configuration types spooky-lb : Load balancing and health tracking spooky-bridge : Protocol conversion spooky-transport : HTTP/2 backend communication","title":"Dependencies"},{"location":"architecture/components/#protocol-bridge-spooky-bridge","text":"","title":"Protocol Bridge (spooky-bridge)"},{"location":"architecture/components/#responsibilities_2","text":"Convert HTTP/3 requests to HTTP/2 format Normalize headers between protocol versions Handle pseudo-headers (:method, :path, :authority, :scheme) Filter hop-by-hop headers Construct proper HTTP/2 request objects","title":"Responsibilities"},{"location":"architecture/components/#key-types_2","text":"pub enum BridgeError { InvalidMethod, InvalidUri, InvalidHeader, Build(http::Error), }","title":"Key Types"},{"location":"architecture/components/#public-api_1","text":"pub fn build_h2_request( backend: &str, method: &str, path: &str, headers: &[(Vec<u8>, Vec<u8>)], body: &[u8], ) -> Result<Request<Full<Bytes>>, BridgeError>;","title":"Public API"},{"location":"architecture/components/#implementation-details_1","text":"Method Conversion: - HTTP/3 method string parsed into http::Method - Validation ensures method is valid HTTP method URI Construction: - Backend address combined with request path - Format: http://{backend}{path} - Empty path defaults to \"/\" - Parsed into http::Uri Header Processing: 1. Skip pseudo-headers (starting with ':') 2. Filter hop-by-hop headers (Connection, Keep-Alive, Transfer-Encoding, Upgrade) 3. Skip Content-Length (recalculated from body) 4. Ensure Host header present (from authority or backend address) 5. Set Content-Length if body is non-empty Body Handling: - Body copied into Full<Bytes> for hyper compatibility - Content-Length header set based on actual body size Edge Cases: - Missing Host: defaults to backend address - Empty path: defaults to \"/\" - Invalid headers: return BridgeError::InvalidHeader","title":"Implementation Details"},{"location":"architecture/components/#error-handling_1","text":"All errors are propagated to caller with specific error types. Invalid requests are not sent to backends.","title":"Error Handling"},{"location":"architecture/components/#dependencies_2","text":"http : HTTP types (Request, Method, Uri, HeaderName, HeaderValue) http-body-util : Body utilities (Full) bytes : Byte buffer types quiche : HTTP/3 header types (NameValue)","title":"Dependencies"},{"location":"architecture/components/#transport-layer-spooky-transport","text":"","title":"Transport Layer (spooky-transport)"},{"location":"architecture/components/#responsibilities_3","text":"Maintain HTTP/2 connections to backend servers Connection pooling and reuse Request multiplexing over HTTP/2 connections Flow control via semaphore-based concurrency limiting Request forwarding with timeout handling","title":"Responsibilities"},{"location":"architecture/components/#key-types_3","text":"pub struct H2Pool { backends: HashMap<String, BackendHandle>, } struct BackendHandle { client: H2Client, inflight: Arc<Semaphore>, } pub struct H2Client { client: Client<HttpConnector, Full<Bytes>>, } pub enum PoolError { UnknownBackend(String), Send(hyper_util::client::legacy::Error), }","title":"Key Types"},{"location":"architecture/components/#public-api_2","text":"impl H2Pool { /// Create new pool with specified backends and concurrency limit pub fn new<I>(backends: I, max_inflight: usize) -> Self where I: IntoIterator<Item = String>; /// Check if backend exists in pool pub fn has_backend(&self, backend: &str) -> bool; /// Send request to specified backend pub async fn send( &self, backend: &str, req: Request<Full<Bytes>>, ) -> Result<Response<Incoming>, PoolError>; } impl H2Client { /// Create new HTTP/2-only client pub fn new() -> Self; /// Send request over HTTP/2 pub async fn send( &self, req: Request<Full<Bytes>>, ) -> Result<Response<Incoming>, hyper_util::client::legacy::Error>; }","title":"Public API"},{"location":"architecture/components/#implementation-details_2","text":"Connection Pooling: - One BackendHandle per backend address - Handle contains HTTP/2 client and concurrency limiter - Connections created lazily on first request - Connection reuse managed automatically by hyper Concurrency Control: - Semaphore limits concurrent requests per backend - Default: 64 concurrent requests per backend - Backpressure applied when limit reached (async wait) - Permit released when request completes HTTP/2 Client: - Built using hyper legacy client API - HTTP/2-only mode enforced - HttpConnector with HTTP enforcement disabled (allows non-HTTPS URIs) - TokioExecutor for spawning connection tasks Error Handling: - Unknown backend: PoolError::UnknownBackend - Connection errors: PoolError::Send wrapping hyper error - Timeout handled by caller (edge layer)","title":"Implementation Details"},{"location":"architecture/components/#dependencies_3","text":"hyper : HTTP client implementation hyper-util : Connection pooling and legacy client http-body-util : Body utilities tokio : Async runtime bytes : Byte buffers","title":"Dependencies"},{"location":"architecture/components/#load-balancer-spooky-lb","text":"","title":"Load Balancer (spooky-lb)"},{"location":"architecture/components/#responsibilities_4","text":"Backend selection algorithms (Random, Round Robin, Consistent Hash) Health state tracking per backend Failure threshold detection Recovery threshold tracking Cooldown period management Upstream pool management","title":"Responsibilities"},{"location":"architecture/components/#key-types_4","text":"pub struct BackendState { address: String, weight: u32, health_check: HealthCheck, consecutive_failures: u32, health_state: HealthState, } enum HealthState { Healthy, Unhealthy { until: Instant, successes: u32 }, } pub enum HealthTransition { BecameHealthy, BecameUnhealthy, } pub struct BackendPool { backends: Vec<BackendState>, } pub struct UpstreamPool { pub pool: BackendPool, pub strategy: String, } pub enum LoadBalancing { RoundRobin(RoundRobin), ConsistentHash(ConsistentHash), Random(Random), } pub struct RoundRobin { next: usize, } pub struct ConsistentHash { replicas: u32, } pub struct Random;","title":"Key Types"},{"location":"architecture/components/#public-api_3","text":"impl BackendState { pub fn new(backend: &Backend) -> Self; pub fn is_healthy(&self) -> bool; pub fn address(&self) -> &str; pub fn health_check(&self) -> &HealthCheck; pub fn weight(&self) -> u32; pub fn record_success(&mut self) -> Option<HealthTransition>; pub fn record_failure(&mut self) -> Option<HealthTransition>; } impl BackendPool { pub fn new_from_states(backends: Vec<BackendState>) -> Self; pub fn len(&self) -> usize; pub fn is_empty(&self) -> bool; pub fn address(&self, index: usize) -> Option<&str>; pub fn mark_success(&mut self, index: usize) -> Option<HealthTransition>; pub fn mark_failure(&mut self, index: usize) -> Option<HealthTransition>; pub fn health_check(&self, index: usize) -> Option<HealthCheck>; pub fn healthy_indices(&self) -> Vec<usize>; pub fn all_indices(&self) -> Vec<usize>; pub fn backend(&self, index: usize) -> Option<&BackendState>; } impl UpstreamPool { pub fn from_upstream(upstream: &Upstream) -> Result<Self, String>; } impl LoadBalancing { pub fn from_config(value: &str) -> Result<Self, String>; pub fn pick(&mut self, key: &str, pool: &UpstreamPool) -> Option<usize>; } impl RoundRobin { pub fn new() -> Self; pub fn pick(&mut self, pool: &BackendPool) -> Option<usize>; } impl ConsistentHash { pub fn new(replicas: u32) -> Self; pub fn pick(&self, key: &str, pool: &BackendPool) -> Option<usize>; } impl Random { pub fn new() -> Self; pub fn pick(&mut self, pool: &BackendPool) -> Option<usize>; }","title":"Public API"},{"location":"architecture/components/#implementation-details_3","text":"Health State Machine: Healthy \u2502 \u2502 (failures >= failure_threshold) \u25bc Unhealthy { until: cooldown_end, successes: 0 } \u2502 \u2502 (now >= cooldown_end && successes >= success_threshold) \u25bc Healthy Health Tracking: - record_success() : Resets consecutive failures if healthy; increments success counter if unhealthy - record_failure() : Increments consecutive failures; transitions to unhealthy when threshold reached - Transitions return Option<HealthTransition> for logging Backend Selection: - Only healthy backends are candidates - If no healthy backends, returns None - Each algorithm filters to healthy backends before selection Round Robin: - Maintains next index counter - Wraps around when reaching end of healthy backends - Ensures even distribution across healthy backends Consistent Hash: - Builds hash ring with virtual nodes (replicas) - Replica count = base_replicas * backend_weight - Hash function: FNV-1a (fast, good distribution) - Key is hashed, closest node on ring is selected - Same key always routes to same backend (session affinity) Random: - Simple random selection from healthy backends - Uses thread-local RNG for performance - Uniform distribution Weight Support: - Weights affect consistent hash replica count - Higher weight = more virtual nodes = more traffic - Round robin and random ignore weights currently","title":"Implementation Details"},{"location":"architecture/components/#algorithm-selection","text":"Supported configuration strings: - \"round-robin\", \"round_robin\", \"rr\" \u2192 RoundRobin - \"consistent-hash\", \"consistent_hash\", \"ch\" \u2192 ConsistentHash - \"random\" \u2192 Random Default replicas for consistent hash: 64","title":"Algorithm Selection"},{"location":"architecture/components/#dependencies_4","text":"rand : Random number generation spooky-config : Configuration types (Backend, HealthCheck, Upstream) Standard library: BTreeMap for hash ring, Duration/Instant for timing","title":"Dependencies"},{"location":"architecture/components/#configuration-system-spooky-config","text":"","title":"Configuration System (spooky-config)"},{"location":"architecture/components/#responsibilities_5","text":"YAML configuration parsing Configuration structure definitions Default value provision Configuration validation Error reporting for invalid configurations","title":"Responsibilities"},{"location":"architecture/components/#key-types_5","text":"pub struct Config { pub version: u32, pub listen: Listen, pub upstream: HashMap<String, Upstream>, pub load_balancing: Option<LoadBalancing>, pub log: Log, } pub struct Listen { pub protocol: String, pub port: u32, pub address: String, pub tls: Tls, } pub struct Tls { pub cert: String, pub key: String, } pub struct Upstream { pub load_balancing: LoadBalancing, pub route: RouteMatch, pub backends: Vec<Backend>, } pub struct Backend { pub id: String, pub address: String, pub weight: u32, pub health_check: HealthCheck, } pub struct RouteMatch { pub host: Option<String>, pub path_prefix: Option<String>, pub method: Option<String>, } pub struct HealthCheck { pub path: String, pub interval: u64, pub timeout_ms: u64, pub failure_threshold: u32, pub success_threshold: u32, pub cooldown_ms: u64, } pub struct LoadBalancing { pub lb_type: String, pub key: Option<String>, } pub struct Log { pub level: String, }","title":"Key Types"},{"location":"architecture/components/#public-api_4","text":"// loader.rs pub fn read_config(path: &str) -> Result<Config, String>; // validator.rs pub fn validate(config: &Config) -> bool;","title":"Public API"},{"location":"architecture/components/#implementation-details_4","text":"Configuration Loading: 1. Read file from path 2. Parse YAML via serde_yaml 3. Apply default values via serde defaults 4. Return Config or error message Default Values: - version: 1 - protocol: \"http3\" - port: 9889 - address: \"0.0.0.0\" - log level: \"info\" - backend weight: 100 - health check path: \"/health\" - health check interval: 5000ms - health timeout: 1000ms - failure threshold: 3 - success threshold: 2 - cooldown: 5000ms Validation Checks: 1. TLS certificate file exists and is readable 2. TLS key file exists and is readable 3. At least one upstream configured 4. Each upstream has at least one backend 5. Backend addresses are non-empty 6. Log level is valid Error Handling: - File not found: clear error message with path - Parse errors: YAML line/column information - Validation errors: specific validation failure message","title":"Implementation Details"},{"location":"architecture/components/#dependencies_5","text":"serde : Serialization framework serde_yaml : YAML parsing log : Logging","title":"Dependencies"},{"location":"architecture/components/#utilities-spooky-utils","text":"","title":"Utilities (spooky-utils)"},{"location":"architecture/components/#responsibilities_6","text":"TLS certificate and key loading Logging initialization Common helper functions","title":"Responsibilities"},{"location":"architecture/components/#modules","text":"tls.rs: pub fn load_certs(path: &str) -> Result<Vec<Certificate>, String>; pub fn load_private_key(path: &str) -> Result<PrivateKey, String>; logger.rs: pub fn init_logger(level: &str);","title":"Modules"},{"location":"architecture/components/#implementation-details_5","text":"TLS Loading: - Reads PEM files from filesystem - Parses DER-encoded certificates - Validates format - Returns rustls-compatible types Logger Initialization: - Configures env_logger with specified level - Maps custom log levels (if configured) - Enables timestamp and module path","title":"Implementation Details"},{"location":"architecture/components/#dependencies_6","text":"rustls-pki-types : TLS types env_logger : Logging implementation log : Logging facade","title":"Dependencies"},{"location":"architecture/components/#component-interaction-flow","text":"","title":"Component Interaction Flow"},{"location":"architecture/components/#request-path","text":"Client HTTP/3 Request \u25bc [spooky-edge::QUICListener] \u251c\u2500 Receive QUIC packets \u251c\u2500 Decode HTTP/3 headers \u2514\u2500 Create RequestEnvelope \u25bc [spooky-edge::quic_listener::find_upstream_for_request] \u251c\u2500 Match path_prefix and host \u2514\u2500 Select upstream pool \u25bc [spooky-lb::LoadBalancing::pick] \u251c\u2500 Filter to healthy backends \u251c\u2500 Apply algorithm \u2514\u2500 Return backend index \u25bc [spooky-bridge::build_h2_request] \u251c\u2500 Convert HTTP/3 \u2192 HTTP/2 \u251c\u2500 Normalize headers \u2514\u2500 Construct Request<Full<Bytes>> \u25bc [spooky-transport::H2Pool::send] \u251c\u2500 Acquire semaphore permit \u251c\u2500 Get backend client \u2514\u2500 Forward request \u25bc Backend HTTP/2 Server \u25bc [spooky-edge::QUICListener] \u251c\u2500 Receive response \u251c\u2500 Update health state \u251c\u2500 Update metrics \u2514\u2500 Stream response to client","title":"Request Path"},{"location":"architecture/components/#configuration-path","text":"[spooky::main] \u2514\u2500 Parse CLI args \u25bc [spooky-config::loader::read_config] \u251c\u2500 Read YAML file \u2514\u2500 Parse with serde \u25bc [spooky-config::validator::validate] \u251c\u2500 Check TLS files \u251c\u2500 Validate structure \u2514\u2500 Return bool \u25bc [spooky-edge::QUICListener::new] \u251c\u2500 Load TLS via spooky-utils \u251c\u2500 Create H2Pool with backends \u251c\u2500 Create UpstreamPools \u2514\u2500 Initialize load balancers \u25bc Runtime","title":"Configuration Path"},{"location":"architecture/components/#testing-strategy","text":"","title":"Testing Strategy"},{"location":"architecture/components/#unit-tests","text":"Each crate includes unit tests for core functionality: spooky-lb: - Round robin cycling behavior - Consistent hash stability - Health state transitions - Backend recovery - Empty pool handling spooky-bridge: - Header conversion - Pseudo-header handling - URI construction - Error cases spooky-config: - YAML parsing - Default value application - Validation logic spooky-transport: - Pool initialization - Backend existence checks","title":"Unit Tests"},{"location":"architecture/components/#integration-tests","text":"spooky-edge: - Full request/response flow - Health check integration - Upstream routing - Multiple load balancing strategies","title":"Integration Tests"},{"location":"architecture/components/#test-execution","text":"# All tests cargo test # Specific crate cargo test -p spooky-lb # Integration tests only cargo test -p spooky-edge --test lb_integration","title":"Test Execution"},{"location":"architecture/components/#performance-optimization","text":"","title":"Performance Optimization"},{"location":"architecture/components/#hot-path-optimizations","text":"Zero-Copy Where Possible: - UDP receive buffer reused - QUIC packet processing avoids allocations - Header slices avoid string copies Lock-Free Metrics: - AtomicU64 for counters - No mutex on request path Connection Pooling: - HTTP/2 connection reuse - Amortize handshake cost Async I/O: - Backend requests with full body buffering (streaming planned) - Efficient task scheduling via Tokio","title":"Hot Path Optimizations"},{"location":"architecture/components/#memory-management","text":"Fixed Buffers: - 64KB receive/send buffers per listener - No per-packet allocation Bounded Collections: - Connection map grows with active connections - Stream map per connection, cleared on completion Reference Counting: - Arc for shared config and pools - Amortize clone cost","title":"Memory Management"},{"location":"architecture/components/#deployment-considerations","text":"","title":"Deployment Considerations"},{"location":"architecture/components/#binary-distribution","text":"Single statically-linked binary containing all components. No runtime dependencies except system TLS libraries.","title":"Binary Distribution"},{"location":"architecture/components/#resource-requirements","text":"File descriptors: 2 per backend + connection count Memory: ~1-2KB per QUIC connection + connection pools CPU: Scales with core count via Tokio runtime Network: UDP port for client traffic, TCP for backends","title":"Resource Requirements"},{"location":"architecture/components/#operational-checklist","text":"TLS certificates and keys readable by process user UDP port accessible for QUIC traffic Backend addresses reachable from proxy File descriptor limits sufficient (ulimit -n) Configuration validated before deployment Logging configured appropriately for environment","title":"Operational Checklist"},{"location":"architecture/components/#monitoring-recommendations","text":"Track requests_total, requests_success, requests_failure Monitor backend_timeouts and backend_errors Alert on health state transitions Log slow requests (duration tracking) Monitor connection count Track memory usage growth","title":"Monitoring Recommendations"},{"location":"architecture/overview/","text":"Introduction Spooky is an HTTP/3 to HTTP/2 reverse proxy and load balancer implemented in Rust. It terminates QUIC connections at the edge and forwards HTTP requests to HTTP/2 backend servers, enabling modern HTTP/3 clients to communicate with existing HTTP/2 infrastructure without requiring backend modifications. Design Principles Performance Spooky is designed for high-performance operation with minimal overhead: - Zero-copy packet processing where possible - Lock-free data structures for hot paths - Asynchronous I/O throughout the stack - Connection pooling and multiplexing - Memory-efficient buffer management Safety Built on Rust's memory safety guarantees: - No unsafe code in core proxy logic - Type-safe protocol conversions - Structured error handling with explicit failure modes - Resource lifetime tracking via ownership Operational Simplicity Simple to deploy and operate: - Single binary deployment - YAML-based configuration with validation - Graceful shutdown with connection draining - Hot configuration reload (planned) - Comprehensive metrics and logging Modularity Clear separation of concerns across crate boundaries: - Independent protocol layer implementations - Pluggable load balancing algorithms - Isolated configuration management - Reusable utility components System Architecture High-Level View \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 HTTP/3 Clients \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 QUIC/UDP \u2502 TLS 1.3 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Spooky Edge \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 QUIC Listener (quiche) \u2502 \u2502 \u2502 \u2502 - Connection management \u2502 \u2502 \u2502 \u2502 - Stream multiplexing \u2502 \u2502 \u2502 \u2502 - TLS termination \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Protocol Bridge \u2502 \u2502 \u2502 \u2502 - HTTP/3 \u2192 HTTP/2 conversion \u2502 \u2502 \u2502 \u2502 - Header normalization \u2502 \u2502 \u2502 \u2502 - Full body buffering (streaming planned) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Router & Load Balancer \u2502 \u2502 \u2502 \u2502 - Path/host matching \u2502 \u2502 \u2502 \u2502 - Upstream selection \u2502 \u2502 \u2502 \u2502 - Health tracking \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 HTTP/2 Connection Pool \u2502 \u2502 \u2502 \u2502 - Connection reuse \u2502 \u2502 \u2502 \u2502 - Request forwarding \u2502 \u2502 \u2502 \u2502 - Full response buffering (streaming planned) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 HTTP/2 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Backend Pool \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Data Plane and Control Plane The architecture separates data plane operations (request forwarding) from control plane operations (configuration, health checks, metrics): Data Plane: - QUIC packet processing - HTTP/3 stream handling - Protocol conversion - Backend request forwarding - Response streaming Control Plane: - Configuration loading and validation - Health check execution - Backend state management - Metrics collection - Connection lifecycle management This separation ensures that control plane operations do not block request processing on the hot path. Request Processing Pipeline 1. Connection Establishment When a client initiates a connection: 1. UDP packets arrive at the bound socket 2. QUIC handshake is performed using quiche 3. TLS 1.3 credentials are validated 4. HTTP/3 session is established over QUIC 5. Connection state is tracked in the connections HashMap 2. Request Reception For each incoming HTTP/3 stream: 1. QUIC stream data is received 2. HTTP/3 headers are decoded via QPACK 3. Request envelope is created with method, path, authority, headers 4. Body data is accumulated as stream frames arrive 5. Stream state is maintained until request is complete 3. Routing and Backend Selection Once request headers are available: 1. Router matches request path and host against upstream pool routes 2. Longest matching path prefix wins for overlapping routes 3. Host-based routing is applied if configured 4. Selected upstream pool's load balancing strategy is invoked 5. Backend index is selected from healthy backends only 6. Backend address is retrieved from pool 4. Protocol Translation Before forwarding to backend: 1. HTTP/3 pseudo-headers (:method, :path, :authority) are extracted 2. HTTP/2 request is built with proper URI and method 3. Regular headers are copied, filtering hop-by-hop headers 4. Content-Length is set based on body size 5. Host header is ensured (using authority or backend address) 5. Backend Forwarding Request is sent to selected backend: 1. HTTP/2 connection pool provides connection for backend address 2. Semaphore-based flow control limits concurrent requests per backend 3. Request is sent over HTTP/2 connection 4. Timeout is enforced at the transport layer 5. Backend response is awaited 6. Response Handling Backend response is processed: 1. HTTP/2 response is received from backend 2. Status code and headers are extracted 3. Response is written back to HTTP/3 stream 4. Body is buffered from backend and sent to client (streaming planned) 5. Stream is finalized when response is complete 7. Health Management Backend health is tracked continuously: 1. Successful requests increment success counter 2. Failed requests increment failure counter 3. Consecutive failures beyond threshold mark backend unhealthy 4. Unhealthy backends enter cooldown period 5. Successful requests during recovery increment recovery counter 6. Backends return to healthy state after success threshold is met 8. Metrics Collection Throughout the pipeline, metrics are recorded: - Total requests received - Successful responses - Failed requests - Backend timeouts - Backend errors - Request latency (start to completion time) Concurrency Model Async Runtime Spooky uses Tokio as its asynchronous runtime: - Multi-threaded work-stealing scheduler - Event-driven I/O with epoll/kqueue - Timer wheel for timeout management - Cooperative task scheduling State Management Shared state is managed carefully: - Arc<T> for shared ownership - Mutex<T> for mutable shared state (upstream pools) - AtomicU64 for lock-free counters (metrics) - Single-threaded UDP socket polling (no lock contention) Task Structure The main event loop runs on the primary thread: - poll() processes UDP packets synchronously - QUIC connections are managed in-process - Backend requests spawn async tasks via Tokio - Graceful shutdown coordinated via AtomicBool This design avoids thread synchronization overhead on the packet processing path while leveraging Tokio's async capabilities for backend I/O. Error Handling Strategy Error Categories Configuration Errors: - Detected at startup during validation - Cause process to exit before binding sockets - Examples: invalid TLS paths, malformed YAML, missing required fields Protocol Errors: - QUIC connection failures, stream errors, invalid HTTP/3 - Isolated to individual connections or streams - Do not affect other active connections - Logged for debugging Transport Errors: - Backend connection failures, timeouts, HTTP/2 errors - Trigger backend health state changes - May cause retry to different backend - Increment error metrics System Errors: - Socket errors, TLS failures, resource exhaustion - May require process restart depending on severity - Logged at error level with context Recovery Mechanisms Stream-Level Recovery: - Invalid stream fails with HTTP error to client - Connection remains active for other streams - Error logged with stream ID Backend-Level Recovery: - Failed backend marked unhealthy - Requests routed to healthy backends - Backend enters cooldown, recovers after success threshold - Health transitions logged Connection-Level Recovery: - Failed QUIC connection is closed - Other connections unaffected - Client may reconnect Process-Level Recovery: - Graceful shutdown on SIGTERM/SIGINT - Drain period allows in-flight requests to complete - Socket closure after drain timeout Configuration Architecture Structure Configuration is hierarchical: Config \u251c\u2500\u2500 version: u32 \u251c\u2500\u2500 listen: Listen (protocol, port, address, TLS) \u251c\u2500\u2500 upstream: HashMap<String, Upstream> \u2502 \u2514\u2500\u2500 Upstream \u2502 \u251c\u2500\u2500 load_balancing: LoadBalancing \u2502 \u251c\u2500\u2500 route: RouteMatch (host, path_prefix) \u2502 \u2514\u2500\u2500 backends: Vec<Backend> \u2502 \u2514\u2500\u2500 Backend (id, address, weight, health_check) \u2514\u2500\u2500 log: Log (level) Validation Configuration validation occurs before runtime: 1. YAML parsing with serde 2. TLS certificate/key file existence checks 3. Backend address format validation 4. Load balancing mode validation 5. Route conflict detection (planned) Runtime Behavior Current configuration is immutable at runtime: - Loaded once at startup - Shared via Arc across components - Hot reload not yet implemented (requires atomic swap) Security Considerations Transport Security TLS 1.3 required for all client connections Certificate chain validation via rustls Private key protection (file permissions) ALPN negotiation ensures HTTP/3 Backend Communication Currently plaintext HTTP/2 Mutual TLS to backends (planned) Connection reuse reduces handshake overhead Attack Surface UDP amplification: QUIC includes mitigation (connection ID validation) Resource exhaustion: connection limits, per-backend semaphores Request smuggling: strict HTTP/3 to HTTP/2 conversion rules Header injection: header validation in bridge module Observability Logging Structured logging via Rust's log crate: - Levels: trace, debug, info, warn, error - Context includes: connection ID, stream ID, backend, duration - Configurable log level at startup Metrics Atomic counters for key metrics: - requests_total : all requests received - requests_success : successful responses - requests_failure : failed requests - backend_timeouts : timed out backend requests - backend_errors : backend error responses Metrics export via Prometheus format (planned). Tracing Request-level tracing: - RequestEnvelope tracks start time - Duration calculated on completion - Logged with request details Distributed tracing via OpenTelemetry (planned). Performance Characteristics Latency QUIC handshake: 1-RTT with TLS 1.3 Proxy overhead: sub-millisecond (header conversion, routing) Backend latency: dependent on backend response time End-to-end: dominated by backend latency Throughput Concurrent connections: 10,000+ QUIC connections Requests per second: 100,000+ on multi-core hardware Per-connection overhead: 1-2KB memory CPU: primarily driven by QUIC crypto and serialization Scalability Horizontal: stateless design allows multiple instances Vertical: work-stealing scheduler utilizes all cores Backend scaling: dynamic health-based routing Connection scaling: bounded by file descriptors and memory Future Enhancements Planned Features Hot configuration reload without restart Prometheus metrics endpoint OpenTelemetry distributed tracing Mutual TLS to backends Active health check probes (TCP/HTTP) Rate limiting per client Circuit breaker pattern for failing backends Admin API for runtime inspection Architectural Improvements Lock-free routing table Connection state persistence for zero-downtime restart eBPF integration for packet-level optimizations QUIC 0-RTT support for returning clients","title":"High-Level Design"},{"location":"architecture/overview/#introduction","text":"Spooky is an HTTP/3 to HTTP/2 reverse proxy and load balancer implemented in Rust. It terminates QUIC connections at the edge and forwards HTTP requests to HTTP/2 backend servers, enabling modern HTTP/3 clients to communicate with existing HTTP/2 infrastructure without requiring backend modifications.","title":"Introduction"},{"location":"architecture/overview/#design-principles","text":"","title":"Design Principles"},{"location":"architecture/overview/#performance","text":"Spooky is designed for high-performance operation with minimal overhead: - Zero-copy packet processing where possible - Lock-free data structures for hot paths - Asynchronous I/O throughout the stack - Connection pooling and multiplexing - Memory-efficient buffer management","title":"Performance"},{"location":"architecture/overview/#safety","text":"Built on Rust's memory safety guarantees: - No unsafe code in core proxy logic - Type-safe protocol conversions - Structured error handling with explicit failure modes - Resource lifetime tracking via ownership","title":"Safety"},{"location":"architecture/overview/#operational-simplicity","text":"Simple to deploy and operate: - Single binary deployment - YAML-based configuration with validation - Graceful shutdown with connection draining - Hot configuration reload (planned) - Comprehensive metrics and logging","title":"Operational Simplicity"},{"location":"architecture/overview/#modularity","text":"Clear separation of concerns across crate boundaries: - Independent protocol layer implementations - Pluggable load balancing algorithms - Isolated configuration management - Reusable utility components","title":"Modularity"},{"location":"architecture/overview/#system-architecture","text":"","title":"System Architecture"},{"location":"architecture/overview/#high-level-view","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 HTTP/3 Clients \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 QUIC/UDP \u2502 TLS 1.3 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Spooky Edge \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 QUIC Listener (quiche) \u2502 \u2502 \u2502 \u2502 - Connection management \u2502 \u2502 \u2502 \u2502 - Stream multiplexing \u2502 \u2502 \u2502 \u2502 - TLS termination \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Protocol Bridge \u2502 \u2502 \u2502 \u2502 - HTTP/3 \u2192 HTTP/2 conversion \u2502 \u2502 \u2502 \u2502 - Header normalization \u2502 \u2502 \u2502 \u2502 - Full body buffering (streaming planned) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Router & Load Balancer \u2502 \u2502 \u2502 \u2502 - Path/host matching \u2502 \u2502 \u2502 \u2502 - Upstream selection \u2502 \u2502 \u2502 \u2502 - Health tracking \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 HTTP/2 Connection Pool \u2502 \u2502 \u2502 \u2502 - Connection reuse \u2502 \u2502 \u2502 \u2502 - Request forwarding \u2502 \u2502 \u2502 \u2502 - Full response buffering (streaming planned) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 HTTP/2 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Backend Pool \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"High-Level View"},{"location":"architecture/overview/#data-plane-and-control-plane","text":"The architecture separates data plane operations (request forwarding) from control plane operations (configuration, health checks, metrics): Data Plane: - QUIC packet processing - HTTP/3 stream handling - Protocol conversion - Backend request forwarding - Response streaming Control Plane: - Configuration loading and validation - Health check execution - Backend state management - Metrics collection - Connection lifecycle management This separation ensures that control plane operations do not block request processing on the hot path.","title":"Data Plane and Control Plane"},{"location":"architecture/overview/#request-processing-pipeline","text":"","title":"Request Processing Pipeline"},{"location":"architecture/overview/#1-connection-establishment","text":"When a client initiates a connection: 1. UDP packets arrive at the bound socket 2. QUIC handshake is performed using quiche 3. TLS 1.3 credentials are validated 4. HTTP/3 session is established over QUIC 5. Connection state is tracked in the connections HashMap","title":"1. Connection Establishment"},{"location":"architecture/overview/#2-request-reception","text":"For each incoming HTTP/3 stream: 1. QUIC stream data is received 2. HTTP/3 headers are decoded via QPACK 3. Request envelope is created with method, path, authority, headers 4. Body data is accumulated as stream frames arrive 5. Stream state is maintained until request is complete","title":"2. Request Reception"},{"location":"architecture/overview/#3-routing-and-backend-selection","text":"Once request headers are available: 1. Router matches request path and host against upstream pool routes 2. Longest matching path prefix wins for overlapping routes 3. Host-based routing is applied if configured 4. Selected upstream pool's load balancing strategy is invoked 5. Backend index is selected from healthy backends only 6. Backend address is retrieved from pool","title":"3. Routing and Backend Selection"},{"location":"architecture/overview/#4-protocol-translation","text":"Before forwarding to backend: 1. HTTP/3 pseudo-headers (:method, :path, :authority) are extracted 2. HTTP/2 request is built with proper URI and method 3. Regular headers are copied, filtering hop-by-hop headers 4. Content-Length is set based on body size 5. Host header is ensured (using authority or backend address)","title":"4. Protocol Translation"},{"location":"architecture/overview/#5-backend-forwarding","text":"Request is sent to selected backend: 1. HTTP/2 connection pool provides connection for backend address 2. Semaphore-based flow control limits concurrent requests per backend 3. Request is sent over HTTP/2 connection 4. Timeout is enforced at the transport layer 5. Backend response is awaited","title":"5. Backend Forwarding"},{"location":"architecture/overview/#6-response-handling","text":"Backend response is processed: 1. HTTP/2 response is received from backend 2. Status code and headers are extracted 3. Response is written back to HTTP/3 stream 4. Body is buffered from backend and sent to client (streaming planned) 5. Stream is finalized when response is complete","title":"6. Response Handling"},{"location":"architecture/overview/#7-health-management","text":"Backend health is tracked continuously: 1. Successful requests increment success counter 2. Failed requests increment failure counter 3. Consecutive failures beyond threshold mark backend unhealthy 4. Unhealthy backends enter cooldown period 5. Successful requests during recovery increment recovery counter 6. Backends return to healthy state after success threshold is met","title":"7. Health Management"},{"location":"architecture/overview/#8-metrics-collection","text":"Throughout the pipeline, metrics are recorded: - Total requests received - Successful responses - Failed requests - Backend timeouts - Backend errors - Request latency (start to completion time)","title":"8. Metrics Collection"},{"location":"architecture/overview/#concurrency-model","text":"","title":"Concurrency Model"},{"location":"architecture/overview/#async-runtime","text":"Spooky uses Tokio as its asynchronous runtime: - Multi-threaded work-stealing scheduler - Event-driven I/O with epoll/kqueue - Timer wheel for timeout management - Cooperative task scheduling","title":"Async Runtime"},{"location":"architecture/overview/#state-management","text":"Shared state is managed carefully: - Arc<T> for shared ownership - Mutex<T> for mutable shared state (upstream pools) - AtomicU64 for lock-free counters (metrics) - Single-threaded UDP socket polling (no lock contention)","title":"State Management"},{"location":"architecture/overview/#task-structure","text":"The main event loop runs on the primary thread: - poll() processes UDP packets synchronously - QUIC connections are managed in-process - Backend requests spawn async tasks via Tokio - Graceful shutdown coordinated via AtomicBool This design avoids thread synchronization overhead on the packet processing path while leveraging Tokio's async capabilities for backend I/O.","title":"Task Structure"},{"location":"architecture/overview/#error-handling-strategy","text":"","title":"Error Handling Strategy"},{"location":"architecture/overview/#error-categories","text":"Configuration Errors: - Detected at startup during validation - Cause process to exit before binding sockets - Examples: invalid TLS paths, malformed YAML, missing required fields Protocol Errors: - QUIC connection failures, stream errors, invalid HTTP/3 - Isolated to individual connections or streams - Do not affect other active connections - Logged for debugging Transport Errors: - Backend connection failures, timeouts, HTTP/2 errors - Trigger backend health state changes - May cause retry to different backend - Increment error metrics System Errors: - Socket errors, TLS failures, resource exhaustion - May require process restart depending on severity - Logged at error level with context","title":"Error Categories"},{"location":"architecture/overview/#recovery-mechanisms","text":"Stream-Level Recovery: - Invalid stream fails with HTTP error to client - Connection remains active for other streams - Error logged with stream ID Backend-Level Recovery: - Failed backend marked unhealthy - Requests routed to healthy backends - Backend enters cooldown, recovers after success threshold - Health transitions logged Connection-Level Recovery: - Failed QUIC connection is closed - Other connections unaffected - Client may reconnect Process-Level Recovery: - Graceful shutdown on SIGTERM/SIGINT - Drain period allows in-flight requests to complete - Socket closure after drain timeout","title":"Recovery Mechanisms"},{"location":"architecture/overview/#configuration-architecture","text":"","title":"Configuration Architecture"},{"location":"architecture/overview/#structure","text":"Configuration is hierarchical: Config \u251c\u2500\u2500 version: u32 \u251c\u2500\u2500 listen: Listen (protocol, port, address, TLS) \u251c\u2500\u2500 upstream: HashMap<String, Upstream> \u2502 \u2514\u2500\u2500 Upstream \u2502 \u251c\u2500\u2500 load_balancing: LoadBalancing \u2502 \u251c\u2500\u2500 route: RouteMatch (host, path_prefix) \u2502 \u2514\u2500\u2500 backends: Vec<Backend> \u2502 \u2514\u2500\u2500 Backend (id, address, weight, health_check) \u2514\u2500\u2500 log: Log (level)","title":"Structure"},{"location":"architecture/overview/#validation","text":"Configuration validation occurs before runtime: 1. YAML parsing with serde 2. TLS certificate/key file existence checks 3. Backend address format validation 4. Load balancing mode validation 5. Route conflict detection (planned)","title":"Validation"},{"location":"architecture/overview/#runtime-behavior","text":"Current configuration is immutable at runtime: - Loaded once at startup - Shared via Arc across components - Hot reload not yet implemented (requires atomic swap)","title":"Runtime Behavior"},{"location":"architecture/overview/#security-considerations","text":"","title":"Security Considerations"},{"location":"architecture/overview/#transport-security","text":"TLS 1.3 required for all client connections Certificate chain validation via rustls Private key protection (file permissions) ALPN negotiation ensures HTTP/3","title":"Transport Security"},{"location":"architecture/overview/#backend-communication","text":"Currently plaintext HTTP/2 Mutual TLS to backends (planned) Connection reuse reduces handshake overhead","title":"Backend Communication"},{"location":"architecture/overview/#attack-surface","text":"UDP amplification: QUIC includes mitigation (connection ID validation) Resource exhaustion: connection limits, per-backend semaphores Request smuggling: strict HTTP/3 to HTTP/2 conversion rules Header injection: header validation in bridge module","title":"Attack Surface"},{"location":"architecture/overview/#observability","text":"","title":"Observability"},{"location":"architecture/overview/#logging","text":"Structured logging via Rust's log crate: - Levels: trace, debug, info, warn, error - Context includes: connection ID, stream ID, backend, duration - Configurable log level at startup","title":"Logging"},{"location":"architecture/overview/#metrics","text":"Atomic counters for key metrics: - requests_total : all requests received - requests_success : successful responses - requests_failure : failed requests - backend_timeouts : timed out backend requests - backend_errors : backend error responses Metrics export via Prometheus format (planned).","title":"Metrics"},{"location":"architecture/overview/#tracing","text":"Request-level tracing: - RequestEnvelope tracks start time - Duration calculated on completion - Logged with request details Distributed tracing via OpenTelemetry (planned).","title":"Tracing"},{"location":"architecture/overview/#performance-characteristics","text":"","title":"Performance Characteristics"},{"location":"architecture/overview/#latency","text":"QUIC handshake: 1-RTT with TLS 1.3 Proxy overhead: sub-millisecond (header conversion, routing) Backend latency: dependent on backend response time End-to-end: dominated by backend latency","title":"Latency"},{"location":"architecture/overview/#throughput","text":"Concurrent connections: 10,000+ QUIC connections Requests per second: 100,000+ on multi-core hardware Per-connection overhead: 1-2KB memory CPU: primarily driven by QUIC crypto and serialization","title":"Throughput"},{"location":"architecture/overview/#scalability","text":"Horizontal: stateless design allows multiple instances Vertical: work-stealing scheduler utilizes all cores Backend scaling: dynamic health-based routing Connection scaling: bounded by file descriptors and memory","title":"Scalability"},{"location":"architecture/overview/#future-enhancements","text":"","title":"Future Enhancements"},{"location":"architecture/overview/#planned-features","text":"Hot configuration reload without restart Prometheus metrics endpoint OpenTelemetry distributed tracing Mutual TLS to backends Active health check probes (TCP/HTTP) Rate limiting per client Circuit breaker pattern for failing backends Admin API for runtime inspection","title":"Planned Features"},{"location":"architecture/overview/#architectural-improvements","text":"Lock-free routing table Connection state persistence for zero-downtime restart eBPF integration for packet-level optimizations QUIC 0-RTT support for returning clients","title":"Architectural Improvements"},{"location":"configuration/reference/","text":"Complete reference for all Spooky configuration options. Configuration File Format Spooky uses YAML for configuration. Specify the configuration file using the --config flag: spooky --config /path/to/config.yaml Complete Configuration Example version: 1 listen: protocol: http3 address: \"0.0.0.0\" port: 9889 tls: cert: \"/etc/spooky/certs/fullchain.pem\" key: \"/etc/spooky/certs/privkey.pem\" upstream: api_pool: load_balancing: type: \"consistent-hash\" # key: \"header:x-user-id\" # Planned feature, not currently supported route: host: \"api.example.com\" path_prefix: \"/api\" backends: - id: \"api-01\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 timeout_ms: 2000 failure_threshold: 3 success_threshold: 2 cooldown_ms: 5000 - id: \"api-02\" address: \"10.0.1.11:8080\" weight: 150 health_check: path: \"/health\" interval: 5000 default_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/\" backends: - id: \"web-01\" address: \"10.0.2.10:8080\" weight: 100 health_check: path: \"/status\" interval: 10000 log: level: info Top-Level Configuration version Configuration schema version. Property Type Required Default Description version integer No 1 Configuration schema version listen Server listening configuration. Defines the protocol, address, and port for incoming client connections. upstream Named upstream pool definitions. Each key represents a unique upstream pool with its own routing rules, load balancing strategy, and backend servers. load_balancing Deprecated. This top-level field is accepted by the parser for backward compatibility but has no effect at runtime. Configure load balancing strategy per upstream pool via upstream.<name>.load_balancing instead. log Logging configuration. Controls log level and output formatting. Default Values The following table lists all default configuration values used when properties are not explicitly specified: Property Default Value Description version 1 Configuration format version listen.protocol \"http3\" Network protocol listen.port 9889 Listening port listen.address \"0.0.0.0\" Listening address listen.tls.cert_file Required TLS certificate file path listen.tls.key_file Required TLS private key file path upstream[].route.path_prefix \"/\" Path prefix for routing upstream[].backends[].weight 100 Backend weight for load balancing upstream[].backends[].health_check.path \"/health\" Health check endpoint upstream[].backends[].health_check.interval 5000 Health check interval (ms) upstream[].backends[].health_check.timeout_ms 1000 Health check timeout (ms) upstream[].backends[].health_check.failure_threshold 3 Failures to mark unhealthy upstream[].backends[].health_check.success_threshold 2 Successes to mark healthy upstream[].backends[].health_check.cooldown_ms 5000 Cooldown after failure (ms) upstream[].load_balancing.type \"round-robin\" Per-upstream load balancing algorithm log.level \"info\" Logging verbosity level log.file.enabled false Write logs to file instead of stderr log.file.path \"/var/log/spooky/spooky.log\" Log file path (used when log.file.enabled is true) Listen Configuration Configures the listening interface for incoming client connections. HTTP/3 requires TLS configuration. Properties Property Type Required Default Description protocol string No http3 Protocol to listen on address string No 0.0.0.0 IP address to bind to port integer No 9889 Port to bind to tls object Yes - TLS configuration (required for HTTP/3) Protocol Values http3 : HTTP/3 over QUIC (recommended) TLS Configuration Property Type Required Description cert string Yes Path to TLS certificate file (PEM format) key string Yes Path to TLS private key file (PEM format, PKCS#8 recommended) Examples # Standard HTTP/3 configuration listen: protocol: http3 address: \"0.0.0.0\" port: 9889 tls: cert: \"/etc/spooky/certs/server.crt\" key: \"/etc/spooky/certs/server.key\" # Localhost-only development listen: protocol: http3 address: \"127.0.0.1\" port: 9889 tls: cert: \"certs/localhost.crt\" key: \"certs/localhost.key\" Upstream Configuration Upstream pools define groups of backend servers with routing rules and load balancing strategies. Each upstream pool is identified by a unique name and contains routing criteria, load balancing configuration, and backend definitions. Structure upstream: pool_name: load_balancing: <LoadBalancing> route: <RouteMatch> backends: [<Backend>] Properties Property Type Required Default Description load_balancing object No round-robin Per-upstream load balancing algorithm configuration route object Yes - Route matching criteria backends array Yes - List of backend servers Route Matching Route matching determines which upstream pool handles a request. Routes are evaluated by longest-prefix matching across all configured upstreams, selecting the route with the most specific (longest) path prefix. RouteMatch Properties Property Type Required Default Description host string No - Host header to match (e.g., api.example.com ) path_prefix string No - Path prefix to match (e.g., /api ) method string No - HTTP method to match (reserved for future use) Route matching rules: If host is specified, the request Host header must match exactly If path_prefix is specified, the request path must start with the prefix If both are specified, both conditions must match Routes are evaluated by longest-prefix matching - the route with the most specific (longest) path prefix is selected For routes with equal-length prefixes, selection depends on HashMap iteration order (not deterministic by configuration order) Route Examples # Host-based routing upstream: api_pool: route: host: \"api.example.com\" backends: [...] web_pool: route: host: \"www.example.com\" backends: [...] # Path-based routing upstream: api_pool: route: path_prefix: \"/api\" backends: [...] admin_pool: route: path_prefix: \"/admin\" backends: [...] default_pool: route: path_prefix: \"/\" backends: [...] # Combined host and path routing upstream: api_v2_pool: route: host: \"api.example.com\" path_prefix: \"/v2\" backends: [...] api_v1_pool: route: host: \"api.example.com\" path_prefix: \"/v1\" backends: [...] Backend Configuration Each backend represents an upstream server that can handle requests. Backend Properties Property Type Required Default Description id string Yes - Unique identifier for the backend address string Yes - Backend server address in host:port format weight integer No 100 Load balancing weight (higher values receive more traffic) health_check object Yes - Health check configuration Health Check Configuration Health checks monitor backend availability and automatically remove unhealthy backends from the pool. Property Type Required Default Description path string No /health HTTP path for health check requests interval integer No 5000 Health check interval in milliseconds timeout_ms integer No 1000 Health check timeout in milliseconds failure_threshold integer No 3 Consecutive failures before marking unhealthy success_threshold integer No 2 Consecutive successes before marking healthy cooldown_ms integer No 5000 Cooldown period after marking unhealthy (milliseconds) Health check behavior: Health checks are performed at the specified interval A backend is marked unhealthy after failure_threshold consecutive failures An unhealthy backend enters cooldown for cooldown_ms milliseconds After cooldown, health checks resume A backend is marked healthy after success_threshold consecutive successes Backend Examples # Minimal backend configuration backends: - id: \"backend1\" address: \"10.0.1.10:8080\" health_check: path: \"/health\" # Weighted backend with custom health checks backends: - id: \"backend1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/api/health\" interval: 10000 timeout_ms: 2000 failure_threshold: 5 success_threshold: 3 cooldown_ms: 10000 - id: \"backend2\" address: \"10.0.1.11:8080\" weight: 200 health_check: path: \"/api/health\" interval: 10000 # Multiple backends with different health endpoints backends: - id: \"primary\" address: \"10.0.1.10:8080\" weight: 150 health_check: path: \"/status\" interval: 5000 - id: \"secondary\" address: \"10.0.1.11:8080\" weight: 100 health_check: path: \"/healthz\" interval: 5000 Load Balancing Configuration Load balancing determines how requests are distributed across healthy backends within an upstream pool. Each pool configures its own strategy independently. Properties Property Type Required Default Description type string Yes - Load balancing algorithm key string No - Hash key source for consistent hashing (planned feature) Supported Algorithms random Selects a backend randomly from all healthy backends. Weight values are currently ignored (weighted random is planned for future release). upstream: my_pool: load_balancing: type: \"random\" round-robin Distributes requests evenly across all healthy backends in sequential order. Weight values are currently ignored (weighted round-robin is planned for future release). upstream: my_pool: load_balancing: type: \"round-robin\" consistent-hash Routes requests using consistent hashing based on a fixed key derived from the request. Currently uses request authority (if present), otherwise request path, otherwise HTTP method. Note : Configurable key sources (headers, cookies, query parameters) are planned for future implementation. upstream: my_pool: load_balancing: type: \"consistent-hash\" # key: \"header:x-user-id\" # Planned feature, not currently supported Algorithm Selection Use random for simple stateless load distribution Use round-robin for even distribution across backends Use consistent-hash when session affinity or request consistency is required Examples upstream: api_pool: load_balancing: type: \"consistent-hash\" route: path_prefix: \"/api\" backends: [...] default_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/\" backends: [...] Logging Configuration Controls logging output, verbosity, and destination. Properties Property Type Required Default Description level string No info Log level file.enabled bool No false Write logs to a file instead of stderr file.path string No /var/log/spooky/spooky.log Log file path (used when file.enabled is true ) Log Levels Log levels in order of increasing verbosity: silence : No logging output poltergeist : Error messages only scream : Warnings and errors spooky : Informational messages, warnings, and errors haunt : Debug information whisper : Trace-level debugging Standard log level mapping: silence = off poltergeist = error scream = warn spooky = info haunt = debug whisper = trace Examples # stderr only (default) log: level: info # Write to file log: level: info file: enabled: true path: /var/log/spooky/spooky.log # Development \u2014 debug to stderr log: level: haunt # debug level # Troubleshooting \u2014 trace to file log: level: whisper # trace level file: enabled: true path: /tmp/spooky-trace.log Configuration Validation Spooky validates configuration at startup and reports errors before attempting to start the server. Common Validation Errors Missing required fields TLS certificate or key paths not specified Backend address or ID missing Route configuration empty Invalid file paths TLS certificate file not found or not readable TLS key file not found or not readable Incorrect file permissions Invalid values Port number out of range (1-65535) Invalid IP address format Invalid backend address format (must be host:port ) Duplicate backend IDs within a pool Configuration conflicts Port already in use Duplicate upstream pool names Overlapping or ambiguous route definitions Testing Configuration Validate configuration without starting the server: spooky --config <path> The command exits with status 0 if configuration is valid, or prints detailed error messages and exits with non-zero status if invalid. Complete Working Example version: 1 listen: protocol: http3 address: \"0.0.0.0\" port: 9889 tls: cert: \"certs/proxy-fullchain.pem\" key: \"certs/proxy-key-pkcs8.pem\" upstream: api_pool: load_balancing: type: \"consistent-hash\" route: path_prefix: \"/api\" backends: - id: \"backend1\" address: \"127.0.0.1:7001\" weight: 100 health_check: path: \"/health\" interval: 5000 - id: \"backend2\" address: \"127.0.0.1:7002\" weight: 50 health_check: path: \"/status\" interval: 10000 default_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/\" backends: - id: \"auth1\" address: \"127.0.0.1:8001\" weight: 100 health_check: path: \"/health\" interval: 5000 log: level: debug","title":"Reference"},{"location":"configuration/reference/#configuration-file-format","text":"Spooky uses YAML for configuration. Specify the configuration file using the --config flag: spooky --config /path/to/config.yaml","title":"Configuration File Format"},{"location":"configuration/reference/#complete-configuration-example","text":"version: 1 listen: protocol: http3 address: \"0.0.0.0\" port: 9889 tls: cert: \"/etc/spooky/certs/fullchain.pem\" key: \"/etc/spooky/certs/privkey.pem\" upstream: api_pool: load_balancing: type: \"consistent-hash\" # key: \"header:x-user-id\" # Planned feature, not currently supported route: host: \"api.example.com\" path_prefix: \"/api\" backends: - id: \"api-01\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 timeout_ms: 2000 failure_threshold: 3 success_threshold: 2 cooldown_ms: 5000 - id: \"api-02\" address: \"10.0.1.11:8080\" weight: 150 health_check: path: \"/health\" interval: 5000 default_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/\" backends: - id: \"web-01\" address: \"10.0.2.10:8080\" weight: 100 health_check: path: \"/status\" interval: 10000 log: level: info","title":"Complete Configuration Example"},{"location":"configuration/reference/#top-level-configuration","text":"","title":"Top-Level Configuration"},{"location":"configuration/reference/#version","text":"Configuration schema version. Property Type Required Default Description version integer No 1 Configuration schema version","title":"version"},{"location":"configuration/reference/#listen","text":"Server listening configuration. Defines the protocol, address, and port for incoming client connections.","title":"listen"},{"location":"configuration/reference/#upstream","text":"Named upstream pool definitions. Each key represents a unique upstream pool with its own routing rules, load balancing strategy, and backend servers.","title":"upstream"},{"location":"configuration/reference/#load_balancing","text":"Deprecated. This top-level field is accepted by the parser for backward compatibility but has no effect at runtime. Configure load balancing strategy per upstream pool via upstream.<name>.load_balancing instead.","title":"load_balancing"},{"location":"configuration/reference/#log","text":"Logging configuration. Controls log level and output formatting.","title":"log"},{"location":"configuration/reference/#default-values","text":"The following table lists all default configuration values used when properties are not explicitly specified: Property Default Value Description version 1 Configuration format version listen.protocol \"http3\" Network protocol listen.port 9889 Listening port listen.address \"0.0.0.0\" Listening address listen.tls.cert_file Required TLS certificate file path listen.tls.key_file Required TLS private key file path upstream[].route.path_prefix \"/\" Path prefix for routing upstream[].backends[].weight 100 Backend weight for load balancing upstream[].backends[].health_check.path \"/health\" Health check endpoint upstream[].backends[].health_check.interval 5000 Health check interval (ms) upstream[].backends[].health_check.timeout_ms 1000 Health check timeout (ms) upstream[].backends[].health_check.failure_threshold 3 Failures to mark unhealthy upstream[].backends[].health_check.success_threshold 2 Successes to mark healthy upstream[].backends[].health_check.cooldown_ms 5000 Cooldown after failure (ms) upstream[].load_balancing.type \"round-robin\" Per-upstream load balancing algorithm log.level \"info\" Logging verbosity level log.file.enabled false Write logs to file instead of stderr log.file.path \"/var/log/spooky/spooky.log\" Log file path (used when log.file.enabled is true)","title":"Default Values"},{"location":"configuration/reference/#listen-configuration","text":"Configures the listening interface for incoming client connections. HTTP/3 requires TLS configuration.","title":"Listen Configuration"},{"location":"configuration/reference/#properties","text":"Property Type Required Default Description protocol string No http3 Protocol to listen on address string No 0.0.0.0 IP address to bind to port integer No 9889 Port to bind to tls object Yes - TLS configuration (required for HTTP/3)","title":"Properties"},{"location":"configuration/reference/#protocol-values","text":"http3 : HTTP/3 over QUIC (recommended)","title":"Protocol Values"},{"location":"configuration/reference/#tls-configuration","text":"Property Type Required Description cert string Yes Path to TLS certificate file (PEM format) key string Yes Path to TLS private key file (PEM format, PKCS#8 recommended)","title":"TLS Configuration"},{"location":"configuration/reference/#examples","text":"# Standard HTTP/3 configuration listen: protocol: http3 address: \"0.0.0.0\" port: 9889 tls: cert: \"/etc/spooky/certs/server.crt\" key: \"/etc/spooky/certs/server.key\" # Localhost-only development listen: protocol: http3 address: \"127.0.0.1\" port: 9889 tls: cert: \"certs/localhost.crt\" key: \"certs/localhost.key\"","title":"Examples"},{"location":"configuration/reference/#upstream-configuration","text":"Upstream pools define groups of backend servers with routing rules and load balancing strategies. Each upstream pool is identified by a unique name and contains routing criteria, load balancing configuration, and backend definitions.","title":"Upstream Configuration"},{"location":"configuration/reference/#structure","text":"upstream: pool_name: load_balancing: <LoadBalancing> route: <RouteMatch> backends: [<Backend>]","title":"Structure"},{"location":"configuration/reference/#properties_1","text":"Property Type Required Default Description load_balancing object No round-robin Per-upstream load balancing algorithm configuration route object Yes - Route matching criteria backends array Yes - List of backend servers","title":"Properties"},{"location":"configuration/reference/#route-matching","text":"Route matching determines which upstream pool handles a request. Routes are evaluated by longest-prefix matching across all configured upstreams, selecting the route with the most specific (longest) path prefix.","title":"Route Matching"},{"location":"configuration/reference/#routematch-properties","text":"Property Type Required Default Description host string No - Host header to match (e.g., api.example.com ) path_prefix string No - Path prefix to match (e.g., /api ) method string No - HTTP method to match (reserved for future use) Route matching rules: If host is specified, the request Host header must match exactly If path_prefix is specified, the request path must start with the prefix If both are specified, both conditions must match Routes are evaluated by longest-prefix matching - the route with the most specific (longest) path prefix is selected For routes with equal-length prefixes, selection depends on HashMap iteration order (not deterministic by configuration order)","title":"RouteMatch Properties"},{"location":"configuration/reference/#route-examples","text":"# Host-based routing upstream: api_pool: route: host: \"api.example.com\" backends: [...] web_pool: route: host: \"www.example.com\" backends: [...] # Path-based routing upstream: api_pool: route: path_prefix: \"/api\" backends: [...] admin_pool: route: path_prefix: \"/admin\" backends: [...] default_pool: route: path_prefix: \"/\" backends: [...] # Combined host and path routing upstream: api_v2_pool: route: host: \"api.example.com\" path_prefix: \"/v2\" backends: [...] api_v1_pool: route: host: \"api.example.com\" path_prefix: \"/v1\" backends: [...]","title":"Route Examples"},{"location":"configuration/reference/#backend-configuration","text":"Each backend represents an upstream server that can handle requests.","title":"Backend Configuration"},{"location":"configuration/reference/#backend-properties","text":"Property Type Required Default Description id string Yes - Unique identifier for the backend address string Yes - Backend server address in host:port format weight integer No 100 Load balancing weight (higher values receive more traffic) health_check object Yes - Health check configuration","title":"Backend Properties"},{"location":"configuration/reference/#health-check-configuration","text":"Health checks monitor backend availability and automatically remove unhealthy backends from the pool. Property Type Required Default Description path string No /health HTTP path for health check requests interval integer No 5000 Health check interval in milliseconds timeout_ms integer No 1000 Health check timeout in milliseconds failure_threshold integer No 3 Consecutive failures before marking unhealthy success_threshold integer No 2 Consecutive successes before marking healthy cooldown_ms integer No 5000 Cooldown period after marking unhealthy (milliseconds) Health check behavior: Health checks are performed at the specified interval A backend is marked unhealthy after failure_threshold consecutive failures An unhealthy backend enters cooldown for cooldown_ms milliseconds After cooldown, health checks resume A backend is marked healthy after success_threshold consecutive successes","title":"Health Check Configuration"},{"location":"configuration/reference/#backend-examples","text":"# Minimal backend configuration backends: - id: \"backend1\" address: \"10.0.1.10:8080\" health_check: path: \"/health\" # Weighted backend with custom health checks backends: - id: \"backend1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/api/health\" interval: 10000 timeout_ms: 2000 failure_threshold: 5 success_threshold: 3 cooldown_ms: 10000 - id: \"backend2\" address: \"10.0.1.11:8080\" weight: 200 health_check: path: \"/api/health\" interval: 10000 # Multiple backends with different health endpoints backends: - id: \"primary\" address: \"10.0.1.10:8080\" weight: 150 health_check: path: \"/status\" interval: 5000 - id: \"secondary\" address: \"10.0.1.11:8080\" weight: 100 health_check: path: \"/healthz\" interval: 5000","title":"Backend Examples"},{"location":"configuration/reference/#load-balancing-configuration","text":"Load balancing determines how requests are distributed across healthy backends within an upstream pool. Each pool configures its own strategy independently.","title":"Load Balancing Configuration"},{"location":"configuration/reference/#properties_2","text":"Property Type Required Default Description type string Yes - Load balancing algorithm key string No - Hash key source for consistent hashing (planned feature)","title":"Properties"},{"location":"configuration/reference/#supported-algorithms","text":"","title":"Supported Algorithms"},{"location":"configuration/reference/#random","text":"Selects a backend randomly from all healthy backends. Weight values are currently ignored (weighted random is planned for future release). upstream: my_pool: load_balancing: type: \"random\"","title":"random"},{"location":"configuration/reference/#round-robin","text":"Distributes requests evenly across all healthy backends in sequential order. Weight values are currently ignored (weighted round-robin is planned for future release). upstream: my_pool: load_balancing: type: \"round-robin\"","title":"round-robin"},{"location":"configuration/reference/#consistent-hash","text":"Routes requests using consistent hashing based on a fixed key derived from the request. Currently uses request authority (if present), otherwise request path, otherwise HTTP method. Note : Configurable key sources (headers, cookies, query parameters) are planned for future implementation. upstream: my_pool: load_balancing: type: \"consistent-hash\" # key: \"header:x-user-id\" # Planned feature, not currently supported","title":"consistent-hash"},{"location":"configuration/reference/#algorithm-selection","text":"Use random for simple stateless load distribution Use round-robin for even distribution across backends Use consistent-hash when session affinity or request consistency is required","title":"Algorithm Selection"},{"location":"configuration/reference/#examples_1","text":"upstream: api_pool: load_balancing: type: \"consistent-hash\" route: path_prefix: \"/api\" backends: [...] default_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/\" backends: [...]","title":"Examples"},{"location":"configuration/reference/#logging-configuration","text":"Controls logging output, verbosity, and destination.","title":"Logging Configuration"},{"location":"configuration/reference/#properties_3","text":"Property Type Required Default Description level string No info Log level file.enabled bool No false Write logs to a file instead of stderr file.path string No /var/log/spooky/spooky.log Log file path (used when file.enabled is true )","title":"Properties"},{"location":"configuration/reference/#log-levels","text":"Log levels in order of increasing verbosity: silence : No logging output poltergeist : Error messages only scream : Warnings and errors spooky : Informational messages, warnings, and errors haunt : Debug information whisper : Trace-level debugging Standard log level mapping: silence = off poltergeist = error scream = warn spooky = info haunt = debug whisper = trace","title":"Log Levels"},{"location":"configuration/reference/#examples_2","text":"# stderr only (default) log: level: info # Write to file log: level: info file: enabled: true path: /var/log/spooky/spooky.log # Development \u2014 debug to stderr log: level: haunt # debug level # Troubleshooting \u2014 trace to file log: level: whisper # trace level file: enabled: true path: /tmp/spooky-trace.log","title":"Examples"},{"location":"configuration/reference/#configuration-validation","text":"Spooky validates configuration at startup and reports errors before attempting to start the server.","title":"Configuration Validation"},{"location":"configuration/reference/#common-validation-errors","text":"Missing required fields TLS certificate or key paths not specified Backend address or ID missing Route configuration empty Invalid file paths TLS certificate file not found or not readable TLS key file not found or not readable Incorrect file permissions Invalid values Port number out of range (1-65535) Invalid IP address format Invalid backend address format (must be host:port ) Duplicate backend IDs within a pool Configuration conflicts Port already in use Duplicate upstream pool names Overlapping or ambiguous route definitions","title":"Common Validation Errors"},{"location":"configuration/reference/#testing-configuration","text":"Validate configuration without starting the server: spooky --config <path> The command exits with status 0 if configuration is valid, or prints detailed error messages and exits with non-zero status if invalid.","title":"Testing Configuration"},{"location":"configuration/reference/#complete-working-example","text":"version: 1 listen: protocol: http3 address: \"0.0.0.0\" port: 9889 tls: cert: \"certs/proxy-fullchain.pem\" key: \"certs/proxy-key-pkcs8.pem\" upstream: api_pool: load_balancing: type: \"consistent-hash\" route: path_prefix: \"/api\" backends: - id: \"backend1\" address: \"127.0.0.1:7001\" weight: 100 health_check: path: \"/health\" interval: 5000 - id: \"backend2\" address: \"127.0.0.1:7002\" weight: 50 health_check: path: \"/status\" interval: 10000 default_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/\" backends: - id: \"auth1\" address: \"127.0.0.1:8001\" weight: 100 health_check: path: \"/health\" interval: 5000 log: level: debug","title":"Complete Working Example"},{"location":"configuration/tls/","text":"Guide for configuring TLS certificates for HTTP/3 connections in Spooky. Overview HTTP/3 uses QUIC as its transport protocol, which requires TLS 1.3 for encryption and authentication. Spooky requires valid TLS certificates to establish secure connections with clients. Requirements Protocol Requirements TLS 1.3 (required for QUIC/HTTP3) ALPN (Application-Layer Protocol Negotiation) support SNI (Server Name Indication) support Supported Formats Certificates : PEM-encoded X.509 certificates Private Keys : PEM-encoded PKCS#8 format (recommended) or traditional RSA/ECDSA formats Key Types : RSA (2048-bit minimum) or ECDSA (P-256, P-384) Certificate Generation Development: Self-Signed Certificates with mkcert For local development, mkcert generates locally-trusted certificates: # Install mkcert # Ubuntu/Debian sudo apt install mkcert # macOS brew install mkcert # Install local CA mkcert -install # Generate certificate for localhost mkdir -p certs cd certs mkcert -key-file server.key -cert-file server.crt localhost 127.0.0.1 ::1 # Verify generation ls -lh server.crt server.key Configuration: listen: protocol: http3 port: 9889 address: \"127.0.0.1\" tls: cert: \"certs/server.crt\" key: \"certs/server.key\" Development: Self-Signed Certificates with OpenSSL For environments where mkcert is not available: # Create certificate directory mkdir -p certs cd certs # Generate private key (RSA 2048-bit) openssl genrsa -out server.key 2048 # Generate certificate signing request openssl req -new -key server.key -out server.csr \\ -subj \"/C=US/ST=State/L=City/O=Development/CN=localhost\" # Generate self-signed certificate (valid 365 days) openssl x509 -req -in server.csr -signkey server.key \\ -out server.crt -days 365 -sha256 # Convert key to PKCS#8 format (recommended) openssl pkcs8 -topk8 -nocrypt -in server.key -out server-pkcs8.key # Verify certificate openssl x509 -in server.crt -text -noout # Clean up CSR rm server.csr Configuration: listen: protocol: http3 port: 9889 address: \"127.0.0.1\" tls: cert: \"certs/server.crt\" key: \"certs/server-pkcs8.key\" Production: Let's Encrypt For production deployments with public domains: # Install certbot sudo apt update sudo apt install certbot # Option 1: Standalone mode (requires port 80 available) sudo certbot certonly --standalone \\ -d example.com \\ -d www.example.com # Option 2: DNS challenge (no port requirements) sudo certbot certonly --manual \\ --preferred-challenges dns \\ -d example.com # Certificates are saved to: # Certificate: /etc/letsencrypt/live/example.com/fullchain.pem # Private Key: /etc/letsencrypt/live/example.com/privkey.pem Configuration: listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"/etc/letsencrypt/live/example.com/fullchain.pem\" key: \"/etc/letsencrypt/live/example.com/privkey.pem\" Production: ECDSA Certificates ECDSA certificates offer better performance than RSA: # Generate ECDSA private key (P-256) openssl ecparam -genkey -name prime256v1 -out server-ec.key # Convert to PKCS#8 format openssl pkcs8 -topk8 -nocrypt -in server-ec.key -out server-ec-pkcs8.key # Generate CSR openssl req -new -key server-ec-pkcs8.key -out server-ec.csr \\ -subj \"/C=US/ST=State/L=City/O=Organization/CN=example.com\" # Generate self-signed certificate (or send CSR to CA) openssl x509 -req -in server-ec.csr -signkey server-ec-pkcs8.key \\ -out server-ec.crt -days 365 -sha256 Certificate Configuration Basic Configuration Minimal TLS configuration for HTTP/3: listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"/path/to/certificate.pem\" key: \"/path/to/private-key.pem\" Path Specifications Paths can be absolute or relative: # Absolute paths (recommended for production) tls: cert: \"/etc/spooky/certs/fullchain.pem\" key: \"/etc/spooky/certs/privkey.pem\" # Relative paths (relative to working directory) tls: cert: \"certs/server.crt\" key: \"certs/server.key\" Multi-Domain Certificates For certificates covering multiple domains (SAN certificates): # Generate certificate with Subject Alternative Names openssl req -new -x509 -key server.key -out server.crt -days 365 \\ -subj \"/CN=example.com\" \\ -addext \"subjectAltName=DNS:example.com,DNS:www.example.com,DNS:api.example.com\" Configuration remains the same: tls: cert: \"/etc/spooky/certs/multi-domain.crt\" key: \"/etc/spooky/certs/multi-domain.key\" File Permissions and Security Recommended Permissions Restrict access to certificate files: # Create dedicated certificate directory sudo mkdir -p /etc/spooky/certs sudo chown spooky:spooky /etc/spooky/certs sudo chmod 700 /etc/spooky/certs # Set certificate permissions sudo chmod 644 /etc/spooky/certs/server.crt sudo chmod 600 /etc/spooky/certs/server.key # Verify permissions ls -l /etc/spooky/certs/ Expected output: drwx------ 2 spooky spooky 4096 Dec 15 10:00 . -rw-r--r-- 1 spooky spooky 1234 Dec 15 10:00 server.crt -rw------- 1 spooky spooky 1704 Dec 15 10:00 server.key Security Best Practices Private Key Protection Never commit private keys to version control Use restrictive file permissions (600) Store keys on encrypted filesystems Consider using hardware security modules (HSM) for production Certificate Chain Validation Use complete certificate chains (fullchain.pem with Let's Encrypt) Include intermediate certificates Verify chain with openssl verify Certificate Monitoring Monitor expiration dates Set up renewal automation for Let's Encrypt Implement alerting for certificates expiring within 30 days Certificate Validation Verify Certificate and Key Match Ensure certificate and private key are paired correctly: # Extract modulus from certificate cert_modulus=$(openssl x509 -noout -modulus -in server.crt | md5sum) # Extract modulus from private key key_modulus=$(openssl rsa -noout -modulus -in server.key | md5sum) # Compare (should be identical) echo \"Certificate: $cert_modulus\" echo \"Private Key: $key_modulus\" For ECDSA keys: # Verify ECDSA private key openssl ec -in server-ec.key -check # Verify certificate openssl x509 -in server-ec.crt -text -noout Verify Certificate Properties Check certificate details: # Display certificate information openssl x509 -in server.crt -text -noout # Check expiration date openssl x509 -in server.crt -noout -enddate # Check subject and issuer openssl x509 -in server.crt -noout -subject -issuer # Verify certificate chain openssl verify -CAfile ca.crt server.crt Test Configuration Verify Spooky can load certificates: # Test configuration validity spooky --config config.yaml # Run in debug mode to see TLS initialization # Set log level in config.yaml (log.level) or via RUST_LOG=debug spooky --config config.yaml Certificate Rotation and Renewal Let's Encrypt Automatic Renewal Let's Encrypt certificates are valid for 90 days. Set up automatic renewal: # Test renewal process sudo certbot renew --dry-run # Enable automatic renewal (certbot installs systemd timer) sudo systemctl status certbot.timer # Manually renew certificates sudo certbot renew # Restart Spooky after renewal (hot reload planned) sudo systemctl restart spooky Manual Certificate Rotation For manually-managed certificates: # Backup current certificates sudo cp /etc/spooky/certs/server.crt /etc/spooky/certs/server.crt.backup sudo cp /etc/spooky/certs/server.key /etc/spooky/certs/server.key.backup # Install new certificates sudo cp new-server.crt /etc/spooky/certs/server.crt sudo cp new-server.key /etc/spooky/certs/server.key # Set permissions sudo chmod 644 /etc/spooky/certs/server.crt sudo chmod 600 /etc/spooky/certs/server.key # Restart Spooky (hot reload planned for future release) sudo systemctl restart spooky # Verify new certificates are loaded openssl s_client -connect localhost:9889 -servername localhost < /dev/null 2>/dev/null | openssl x509 -noout -dates Monitoring Certificate Expiry Check certificate expiration: # Check days until expiry openssl x509 -in /etc/spooky/certs/server.crt -noout -enddate # Calculate days remaining days_left=$(( ($(date -d \"$(openssl x509 -in /etc/spooky/certs/server.crt -noout -enddate | cut -d= -f2)\" +%s) - $(date +%s)) / 86400 )) echo \"Certificate expires in $days_left days\" # Alert if less than 30 days if [ $days_left -lt 30 ]; then echo \"WARNING: Certificate expires soon!\" fi Troubleshooting Common Issues Certificate File Not Found Error: failed to read certificate file: No such file or directory Solution: # Verify file exists ls -l /etc/spooky/certs/server.crt # Check path in configuration cat config.yaml | grep -A2 tls # Use absolute paths realpath certs/server.crt Permission Denied Error: failed to read certificate file: Permission denied Solution: # Check file permissions ls -l /etc/spooky/certs/ # Fix permissions sudo chown spooky:spooky /etc/spooky/certs/server.{crt,key} sudo chmod 644 /etc/spooky/certs/server.crt sudo chmod 600 /etc/spooky/certs/server.key # Verify Spooky user can read files sudo -u spooky cat /etc/spooky/certs/server.crt > /dev/null Invalid Certificate Format Error: failed to parse certificate: invalid PEM format Solution: # Verify PEM format openssl x509 -in server.crt -text -noout # Check file encoding file server.crt # Convert DER to PEM if needed openssl x509 -inform DER -in server.der -out server.pem Certificate and Key Mismatch Error: certificate and private key do not match Solution: # Verify certificate and key match (RSA) openssl x509 -noout -modulus -in server.crt | md5sum openssl rsa -noout -modulus -in server.key | md5sum # Verify ECDSA key openssl ec -in server.key -pubout -out server-pub.pem openssl x509 -in server.crt -pubkey -noout -out cert-pub.pem diff server-pub.pem cert-pub.pem PKCS#8 Format Required Some systems require PKCS#8 format: # Convert traditional RSA to PKCS#8 openssl pkcs8 -topk8 -nocrypt -in server.key -out server-pkcs8.key # Update configuration to use PKCS#8 key Testing TLS Connections Test with OpenSSL # Test TLS 1.3 connection echo -e \"GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n\" | \\ openssl s_client -connect localhost:9889 -servername localhost -tls1_3 # Display certificate chain openssl s_client -connect localhost:9889 -servername localhost -showcerts < /dev/null # Check ALPN negotiation openssl s_client -connect localhost:9889 -servername localhost -alpn h3 < /dev/null Test with cURL (HTTP/3 Support) If curl is built with HTTP/3 support: # Test HTTP/3 connection curl --http3 https://localhost:9889/ # Verbose output for debugging curl --http3 -v https://localhost:9889/ # Test with self-signed certificate curl --http3 -k https://localhost:9889/ Debug Logging Enable debug logging to troubleshoot TLS issues: log: level: debug Look for log entries related to: Certificate loading TLS handshake QUIC connection establishment ALPN negotiation Common Error Messages Error Cause Solution certificate has expired Certificate validity period ended Renew certificate certificate is not yet valid System clock incorrect or certificate future-dated Check system time unable to get local issuer certificate Missing intermediate certificate Use fullchain.pem self signed certificate Client doesn't trust self-signed cert Use CA-signed cert or add to client trust store wrong signature type Key algorithm mismatch Ensure certificate and key use same algorithm Reference Configuration Schema listen: tls: cert: string # Path to PEM certificate file (required) key: string # Path to PEM private key file (required) Supported Key Algorithms RSA 2048-bit (minimum) RSA 4096-bit (recommended for long-term use) ECDSA P-256 (secp256r1) ECDSA P-384 (secp384r1) Certificate Requirements PEM encoding X.509 format Valid date range (not expired, not future-dated) Subject Alternative Names (SAN) for multi-domain support Complete certificate chain (including intermediates)","title":"TLS Setup"},{"location":"configuration/tls/#overview","text":"HTTP/3 uses QUIC as its transport protocol, which requires TLS 1.3 for encryption and authentication. Spooky requires valid TLS certificates to establish secure connections with clients.","title":"Overview"},{"location":"configuration/tls/#requirements","text":"","title":"Requirements"},{"location":"configuration/tls/#protocol-requirements","text":"TLS 1.3 (required for QUIC/HTTP3) ALPN (Application-Layer Protocol Negotiation) support SNI (Server Name Indication) support","title":"Protocol Requirements"},{"location":"configuration/tls/#supported-formats","text":"Certificates : PEM-encoded X.509 certificates Private Keys : PEM-encoded PKCS#8 format (recommended) or traditional RSA/ECDSA formats Key Types : RSA (2048-bit minimum) or ECDSA (P-256, P-384)","title":"Supported Formats"},{"location":"configuration/tls/#certificate-generation","text":"","title":"Certificate Generation"},{"location":"configuration/tls/#development-self-signed-certificates-with-mkcert","text":"For local development, mkcert generates locally-trusted certificates: # Install mkcert # Ubuntu/Debian sudo apt install mkcert # macOS brew install mkcert # Install local CA mkcert -install # Generate certificate for localhost mkdir -p certs cd certs mkcert -key-file server.key -cert-file server.crt localhost 127.0.0.1 ::1 # Verify generation ls -lh server.crt server.key Configuration: listen: protocol: http3 port: 9889 address: \"127.0.0.1\" tls: cert: \"certs/server.crt\" key: \"certs/server.key\"","title":"Development: Self-Signed Certificates with mkcert"},{"location":"configuration/tls/#development-self-signed-certificates-with-openssl","text":"For environments where mkcert is not available: # Create certificate directory mkdir -p certs cd certs # Generate private key (RSA 2048-bit) openssl genrsa -out server.key 2048 # Generate certificate signing request openssl req -new -key server.key -out server.csr \\ -subj \"/C=US/ST=State/L=City/O=Development/CN=localhost\" # Generate self-signed certificate (valid 365 days) openssl x509 -req -in server.csr -signkey server.key \\ -out server.crt -days 365 -sha256 # Convert key to PKCS#8 format (recommended) openssl pkcs8 -topk8 -nocrypt -in server.key -out server-pkcs8.key # Verify certificate openssl x509 -in server.crt -text -noout # Clean up CSR rm server.csr Configuration: listen: protocol: http3 port: 9889 address: \"127.0.0.1\" tls: cert: \"certs/server.crt\" key: \"certs/server-pkcs8.key\"","title":"Development: Self-Signed Certificates with OpenSSL"},{"location":"configuration/tls/#production-lets-encrypt","text":"For production deployments with public domains: # Install certbot sudo apt update sudo apt install certbot # Option 1: Standalone mode (requires port 80 available) sudo certbot certonly --standalone \\ -d example.com \\ -d www.example.com # Option 2: DNS challenge (no port requirements) sudo certbot certonly --manual \\ --preferred-challenges dns \\ -d example.com # Certificates are saved to: # Certificate: /etc/letsencrypt/live/example.com/fullchain.pem # Private Key: /etc/letsencrypt/live/example.com/privkey.pem Configuration: listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"/etc/letsencrypt/live/example.com/fullchain.pem\" key: \"/etc/letsencrypt/live/example.com/privkey.pem\"","title":"Production: Let's Encrypt"},{"location":"configuration/tls/#production-ecdsa-certificates","text":"ECDSA certificates offer better performance than RSA: # Generate ECDSA private key (P-256) openssl ecparam -genkey -name prime256v1 -out server-ec.key # Convert to PKCS#8 format openssl pkcs8 -topk8 -nocrypt -in server-ec.key -out server-ec-pkcs8.key # Generate CSR openssl req -new -key server-ec-pkcs8.key -out server-ec.csr \\ -subj \"/C=US/ST=State/L=City/O=Organization/CN=example.com\" # Generate self-signed certificate (or send CSR to CA) openssl x509 -req -in server-ec.csr -signkey server-ec-pkcs8.key \\ -out server-ec.crt -days 365 -sha256","title":"Production: ECDSA Certificates"},{"location":"configuration/tls/#certificate-configuration","text":"","title":"Certificate Configuration"},{"location":"configuration/tls/#basic-configuration","text":"Minimal TLS configuration for HTTP/3: listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"/path/to/certificate.pem\" key: \"/path/to/private-key.pem\"","title":"Basic Configuration"},{"location":"configuration/tls/#path-specifications","text":"Paths can be absolute or relative: # Absolute paths (recommended for production) tls: cert: \"/etc/spooky/certs/fullchain.pem\" key: \"/etc/spooky/certs/privkey.pem\" # Relative paths (relative to working directory) tls: cert: \"certs/server.crt\" key: \"certs/server.key\"","title":"Path Specifications"},{"location":"configuration/tls/#multi-domain-certificates","text":"For certificates covering multiple domains (SAN certificates): # Generate certificate with Subject Alternative Names openssl req -new -x509 -key server.key -out server.crt -days 365 \\ -subj \"/CN=example.com\" \\ -addext \"subjectAltName=DNS:example.com,DNS:www.example.com,DNS:api.example.com\" Configuration remains the same: tls: cert: \"/etc/spooky/certs/multi-domain.crt\" key: \"/etc/spooky/certs/multi-domain.key\"","title":"Multi-Domain Certificates"},{"location":"configuration/tls/#file-permissions-and-security","text":"","title":"File Permissions and Security"},{"location":"configuration/tls/#recommended-permissions","text":"Restrict access to certificate files: # Create dedicated certificate directory sudo mkdir -p /etc/spooky/certs sudo chown spooky:spooky /etc/spooky/certs sudo chmod 700 /etc/spooky/certs # Set certificate permissions sudo chmod 644 /etc/spooky/certs/server.crt sudo chmod 600 /etc/spooky/certs/server.key # Verify permissions ls -l /etc/spooky/certs/ Expected output: drwx------ 2 spooky spooky 4096 Dec 15 10:00 . -rw-r--r-- 1 spooky spooky 1234 Dec 15 10:00 server.crt -rw------- 1 spooky spooky 1704 Dec 15 10:00 server.key","title":"Recommended Permissions"},{"location":"configuration/tls/#security-best-practices","text":"Private Key Protection Never commit private keys to version control Use restrictive file permissions (600) Store keys on encrypted filesystems Consider using hardware security modules (HSM) for production Certificate Chain Validation Use complete certificate chains (fullchain.pem with Let's Encrypt) Include intermediate certificates Verify chain with openssl verify Certificate Monitoring Monitor expiration dates Set up renewal automation for Let's Encrypt Implement alerting for certificates expiring within 30 days","title":"Security Best Practices"},{"location":"configuration/tls/#certificate-validation","text":"","title":"Certificate Validation"},{"location":"configuration/tls/#verify-certificate-and-key-match","text":"Ensure certificate and private key are paired correctly: # Extract modulus from certificate cert_modulus=$(openssl x509 -noout -modulus -in server.crt | md5sum) # Extract modulus from private key key_modulus=$(openssl rsa -noout -modulus -in server.key | md5sum) # Compare (should be identical) echo \"Certificate: $cert_modulus\" echo \"Private Key: $key_modulus\" For ECDSA keys: # Verify ECDSA private key openssl ec -in server-ec.key -check # Verify certificate openssl x509 -in server-ec.crt -text -noout","title":"Verify Certificate and Key Match"},{"location":"configuration/tls/#verify-certificate-properties","text":"Check certificate details: # Display certificate information openssl x509 -in server.crt -text -noout # Check expiration date openssl x509 -in server.crt -noout -enddate # Check subject and issuer openssl x509 -in server.crt -noout -subject -issuer # Verify certificate chain openssl verify -CAfile ca.crt server.crt","title":"Verify Certificate Properties"},{"location":"configuration/tls/#test-configuration","text":"Verify Spooky can load certificates: # Test configuration validity spooky --config config.yaml # Run in debug mode to see TLS initialization # Set log level in config.yaml (log.level) or via RUST_LOG=debug spooky --config config.yaml","title":"Test Configuration"},{"location":"configuration/tls/#certificate-rotation-and-renewal","text":"","title":"Certificate Rotation and Renewal"},{"location":"configuration/tls/#lets-encrypt-automatic-renewal","text":"Let's Encrypt certificates are valid for 90 days. Set up automatic renewal: # Test renewal process sudo certbot renew --dry-run # Enable automatic renewal (certbot installs systemd timer) sudo systemctl status certbot.timer # Manually renew certificates sudo certbot renew # Restart Spooky after renewal (hot reload planned) sudo systemctl restart spooky","title":"Let's Encrypt Automatic Renewal"},{"location":"configuration/tls/#manual-certificate-rotation","text":"For manually-managed certificates: # Backup current certificates sudo cp /etc/spooky/certs/server.crt /etc/spooky/certs/server.crt.backup sudo cp /etc/spooky/certs/server.key /etc/spooky/certs/server.key.backup # Install new certificates sudo cp new-server.crt /etc/spooky/certs/server.crt sudo cp new-server.key /etc/spooky/certs/server.key # Set permissions sudo chmod 644 /etc/spooky/certs/server.crt sudo chmod 600 /etc/spooky/certs/server.key # Restart Spooky (hot reload planned for future release) sudo systemctl restart spooky # Verify new certificates are loaded openssl s_client -connect localhost:9889 -servername localhost < /dev/null 2>/dev/null | openssl x509 -noout -dates","title":"Manual Certificate Rotation"},{"location":"configuration/tls/#monitoring-certificate-expiry","text":"Check certificate expiration: # Check days until expiry openssl x509 -in /etc/spooky/certs/server.crt -noout -enddate # Calculate days remaining days_left=$(( ($(date -d \"$(openssl x509 -in /etc/spooky/certs/server.crt -noout -enddate | cut -d= -f2)\" +%s) - $(date +%s)) / 86400 )) echo \"Certificate expires in $days_left days\" # Alert if less than 30 days if [ $days_left -lt 30 ]; then echo \"WARNING: Certificate expires soon!\" fi","title":"Monitoring Certificate Expiry"},{"location":"configuration/tls/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"configuration/tls/#common-issues","text":"","title":"Common Issues"},{"location":"configuration/tls/#certificate-file-not-found","text":"Error: failed to read certificate file: No such file or directory Solution: # Verify file exists ls -l /etc/spooky/certs/server.crt # Check path in configuration cat config.yaml | grep -A2 tls # Use absolute paths realpath certs/server.crt","title":"Certificate File Not Found"},{"location":"configuration/tls/#permission-denied","text":"Error: failed to read certificate file: Permission denied Solution: # Check file permissions ls -l /etc/spooky/certs/ # Fix permissions sudo chown spooky:spooky /etc/spooky/certs/server.{crt,key} sudo chmod 644 /etc/spooky/certs/server.crt sudo chmod 600 /etc/spooky/certs/server.key # Verify Spooky user can read files sudo -u spooky cat /etc/spooky/certs/server.crt > /dev/null","title":"Permission Denied"},{"location":"configuration/tls/#invalid-certificate-format","text":"Error: failed to parse certificate: invalid PEM format Solution: # Verify PEM format openssl x509 -in server.crt -text -noout # Check file encoding file server.crt # Convert DER to PEM if needed openssl x509 -inform DER -in server.der -out server.pem","title":"Invalid Certificate Format"},{"location":"configuration/tls/#certificate-and-key-mismatch","text":"Error: certificate and private key do not match Solution: # Verify certificate and key match (RSA) openssl x509 -noout -modulus -in server.crt | md5sum openssl rsa -noout -modulus -in server.key | md5sum # Verify ECDSA key openssl ec -in server.key -pubout -out server-pub.pem openssl x509 -in server.crt -pubkey -noout -out cert-pub.pem diff server-pub.pem cert-pub.pem","title":"Certificate and Key Mismatch"},{"location":"configuration/tls/#pkcs8-format-required","text":"Some systems require PKCS#8 format: # Convert traditional RSA to PKCS#8 openssl pkcs8 -topk8 -nocrypt -in server.key -out server-pkcs8.key # Update configuration to use PKCS#8 key","title":"PKCS#8 Format Required"},{"location":"configuration/tls/#testing-tls-connections","text":"","title":"Testing TLS Connections"},{"location":"configuration/tls/#test-with-openssl","text":"# Test TLS 1.3 connection echo -e \"GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n\" | \\ openssl s_client -connect localhost:9889 -servername localhost -tls1_3 # Display certificate chain openssl s_client -connect localhost:9889 -servername localhost -showcerts < /dev/null # Check ALPN negotiation openssl s_client -connect localhost:9889 -servername localhost -alpn h3 < /dev/null","title":"Test with OpenSSL"},{"location":"configuration/tls/#test-with-curl-http3-support","text":"If curl is built with HTTP/3 support: # Test HTTP/3 connection curl --http3 https://localhost:9889/ # Verbose output for debugging curl --http3 -v https://localhost:9889/ # Test with self-signed certificate curl --http3 -k https://localhost:9889/","title":"Test with cURL (HTTP/3 Support)"},{"location":"configuration/tls/#debug-logging","text":"Enable debug logging to troubleshoot TLS issues: log: level: debug Look for log entries related to: Certificate loading TLS handshake QUIC connection establishment ALPN negotiation","title":"Debug Logging"},{"location":"configuration/tls/#common-error-messages","text":"Error Cause Solution certificate has expired Certificate validity period ended Renew certificate certificate is not yet valid System clock incorrect or certificate future-dated Check system time unable to get local issuer certificate Missing intermediate certificate Use fullchain.pem self signed certificate Client doesn't trust self-signed cert Use CA-signed cert or add to client trust store wrong signature type Key algorithm mismatch Ensure certificate and key use same algorithm","title":"Common Error Messages"},{"location":"configuration/tls/#reference","text":"","title":"Reference"},{"location":"configuration/tls/#configuration-schema","text":"listen: tls: cert: string # Path to PEM certificate file (required) key: string # Path to PEM private key file (required)","title":"Configuration Schema"},{"location":"configuration/tls/#supported-key-algorithms","text":"RSA 2048-bit (minimum) RSA 4096-bit (recommended for long-term use) ECDSA P-256 (secp256r1) ECDSA P-384 (secp384r1)","title":"Supported Key Algorithms"},{"location":"configuration/tls/#certificate-requirements","text":"PEM encoding X.509 format Valid date range (not expired, not future-dated) Subject Alternative Names (SAN) for multi-domain support Complete certificate chain (including intermediates)","title":"Certificate Requirements"},{"location":"deployment/production/","text":"Warning: Spooky is experimental software. It is not production-ready. This guide documents deployment procedures for evaluation and staging environments. Do not use Spooky in production without thoroughly understanding its current limitations (see Roadmap for known issues). This guide covers deployment procedures, system configuration, and operational considerations for Spooky HTTP/3 load balancer deployments. Pre-Deployment Checklist Infrastructure Requirements Compute Resources - CPU: 4 cores minimum (8+ for high-throughput deployments) - Memory: 4GB minimum (8GB+ recommended, ~1-2KB per concurrent connection) - Disk: 10GB minimum (configuration, logs, and binary storage) - OS: Linux kernel 5.0+ (Ubuntu 20.04 LTS, RHEL 8+, or equivalent) Network Requirements - UDP ingress on designated QUIC port (typically 443) - HTTP/2 egress to backend pool networks - Low-latency connectivity between proxy tier and backends (<5ms RTT preferred) - MTU considerations: 1500 byte minimum, jumbo frames (9000 bytes) beneficial for high-throughput scenarios Certificate Infrastructure - Valid TLS certificates with full chain - Automated renewal mechanism (Let's Encrypt, internal PKI, or certificate management platform) - Certificate rotation procedures documented and tested Pre-Deployment Validation Before deploying to production, verify the following: Configuration validated with spooky --config <path> (startup validation happens before serving) Backend health check endpoints operational and returning expected responses TLS certificates valid with appropriate SANs and expiration dates Firewall rules permit required traffic flows Service account and filesystem permissions configured Monitoring and alerting infrastructure ready to receive metrics Runbooks prepared for common failure scenarios System Configuration Binary Installation Production deployments should use compiled release binaries: # Download release binary VERSION=\"0.1.0\" ARCH=\"x86_64\" wget \"https://github.com/nishujangra/spooky/releases/download/v${VERSION}/spooky-linux-${ARCH}.tar.gz\" tar xzf \"spooky-linux-${ARCH}.tar.gz\" # Verify checksum sha256sum -c \"spooky-linux-${ARCH}.tar.gz.sha256\" # Install to system path sudo install -m 755 -o root -g root spooky /usr/local/bin/spooky # Create dedicated service account sudo useradd --system --shell /usr/sbin/nologin \\ --home-dir /var/lib/spooky --create-home spooky # Initialize directory structure sudo mkdir -p /etc/spooky/certs /var/log/spooky sudo chown -R root:spooky /etc/spooky sudo chmod 750 /etc/spooky sudo chown spooky:spooky /var/log/spooky sudo chmod 750 /var/log/spooky # Note: Spooky logs to stdout/stderr by default (collected by journald). # The /var/log/spooky directory is for optional file-based logging. Kernel Parameter Tuning UDP and QUIC workloads benefit from increased buffer sizes and connection tracking limits: # /etc/sysctl.d/99-spooky.conf # UDP receive/send buffer tuning net.core.rmem_max = 67108864 net.core.wmem_max = 67108864 net.core.rmem_default = 16777216 net.core.wmem_default = 16777216 # Network device backlog net.core.netdev_max_backlog = 65536 net.core.netdev_budget = 50000 net.core.netdev_budget_usecs = 5000 # Connection tracking (if using conntrack) net.netfilter.nf_conntrack_max = 2097152 net.netfilter.nf_conntrack_tcp_timeout_established = 7200 net.netfilter.nf_conntrack_udp_timeout = 60 net.netfilter.nf_conntrack_udp_timeout_stream = 120 # TCP tuning for HTTP/2 backend connections net.ipv4.tcp_rmem = 8192 262144 33554432 net.ipv4.tcp_wmem = 8192 262144 33554432 net.ipv4.tcp_max_syn_backlog = 8192 net.ipv4.tcp_slow_start_after_idle = 0 net.ipv4.tcp_mtu_probing = 1 # File descriptor limits fs.file-max = 2097152 # Apply configuration sudo sysctl -p /etc/sysctl.d/99-spooky.conf Resource Limits Configure ulimits for the spooky service account: # /etc/security/limits.d/spooky.conf spooky soft nofile 1048576 spooky hard nofile 1048576 spooky soft nproc 16384 spooky hard nproc 16384 spooky soft memlock unlimited spooky hard memlock unlimited Production Configuration # /etc/spooky/config.yaml version: 1 listen: protocol: http3 address: \"0.0.0.0\" port: 443 tls: cert: \"/etc/spooky/certs/fullchain.pem\" key: \"/etc/spooky/certs/privkey.pem\" # Define upstream pools with health checking upstream: # API backend pool with consistent hashing for session affinity api_pool: load_balancing: type: \"consistent-hash\" route: path_prefix: \"/api\" backends: - id: \"api-01\" address: \"10.0.10.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 timeout_ms: 2000 failure_threshold: 3 success_threshold: 2 - id: \"api-02\" address: \"10.0.10.11:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 timeout_ms: 2000 failure_threshold: 3 success_threshold: 2 # Static content pool with round-robin static_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/static\" backends: - id: \"static-01\" address: \"10.0.20.10:8080\" weight: 100 health_check: path: \"/\" interval: 10000 - id: \"static-02\" address: \"10.0.20.11:8080\" weight: 100 health_check: path: \"/\" interval: 10000 # Default backend pool default_pool: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"web-01\" address: \"10.0.30.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 # Logging configuration log: level: info # Use 'warn' for production to reduce I/O # Connection tuning (if supported by configuration schema) # Adjust based on backend capacity and expected load # max_concurrent_connections: 10000 # backend_connection_pool_size: 100 Configuration Notes: - Route matching uses longest-prefix: more specific paths take precedence - Health check intervals balance detection speed vs. backend load - Adjust failure_threshold and success_threshold based on backend stability - Weight distribution should reflect backend capacity - Consistent hashing is appropriate for stateful backends requiring session affinity TLS Certificate Management Certificate Acquisition Let's Encrypt (ACME) # Install certbot sudo apt-get install -y certbot # Obtain certificate (HTTP-01 challenge, requires port 80) sudo certbot certonly --standalone \\ --preferred-challenges http \\ --email ops@example.com \\ --agree-tos \\ --non-interactive \\ -d proxy.example.com # Copy to spooky directory sudo cp /etc/letsencrypt/live/proxy.example.com/fullchain.pem /etc/spooky/certs/ sudo cp /etc/letsencrypt/live/proxy.example.com/privkey.pem /etc/spooky/certs/ sudo chown root:spooky /etc/spooky/certs/*.pem sudo chmod 640 /etc/spooky/certs/privkey.pem sudo chmod 644 /etc/spooky/certs/fullchain.pem Automated Renewal # Create renewal hook sudo tee /etc/letsencrypt/renewal-hooks/deploy/spooky-reload.sh << 'EOF' #!/bin/bash set -e CERT_DOMAIN=\"proxy.example.com\" SPOOKY_CERT_DIR=\"/etc/spooky/certs\" # Copy renewed certificates cp \"/etc/letsencrypt/live/${CERT_DOMAIN}/fullchain.pem\" \"${SPOOKY_CERT_DIR}/\" cp \"/etc/letsencrypt/live/${CERT_DOMAIN}/privkey.pem\" \"${SPOOKY_CERT_DIR}/\" # Set permissions chown root:spooky \"${SPOOKY_CERT_DIR}\"/*.pem chmod 640 \"${SPOOKY_CERT_DIR}/privkey.pem\" chmod 644 \"${SPOOKY_CERT_DIR}/fullchain.pem\" # Reload spooky (graceful reload if supported, otherwise restart) systemctl reload-or-restart spooky logger -t spooky-cert-renewal \"TLS certificates renewed and spooky reloaded\" EOF sudo chmod +x /etc/letsencrypt/renewal-hooks/deploy/spooky-reload.sh # Test renewal process sudo certbot renew --dry-run Certificate Validation Before deploying new certificates: # Verify certificate and key match openssl x509 -noout -modulus -in /etc/spooky/certs/fullchain.pem | openssl md5 openssl rsa -noout -modulus -in /etc/spooky/certs/privkey.pem | openssl md5 # Verify certificate chain openssl verify -CAfile /etc/spooky/certs/fullchain.pem /etc/spooky/certs/fullchain.pem # Check expiration openssl x509 -noout -dates -in /etc/spooky/certs/fullchain.pem # Verify SAN entries openssl x509 -noout -text -in /etc/spooky/certs/fullchain.pem | grep -A1 \"Subject Alternative Name\" Systemd Service Configuration Service Unit # /etc/systemd/system/spooky.service [Unit] Description=Spooky HTTP/3 to HTTP/2 Proxy Documentation=https://github.com/nishujangra/spooky After=network-online.target Wants=network-online.target [Service] Type=simple User=spooky Group=spooky # Binary and configuration ExecStart=/usr/local/bin/spooky --config /etc/spooky/config.yaml # Note: Hot reload not currently supported, use restart instead # ExecReload=/bin/kill -HUP $MAINPID # Restart policy Restart=always RestartSec=5s StartLimitBurst=3 StartLimitIntervalSec=60s # Resource limits LimitNOFILE=1048576 LimitNPROC=16384 TasksMax=16384 # Security hardening NoNewPrivileges=true PrivateTmp=true ProtectSystem=strict ProtectHome=true ReadWritePaths=/var/log/spooky ProtectKernelTunables=true ProtectKernelModules=true ProtectKernelLogs=true ProtectControlGroups=true ProtectProc=invisible ProcSubset=pid RestrictAddressFamilies=AF_INET AF_INET6 AF_UNIX RestrictNamespaces=true RestrictRealtime=true RestrictSUIDSGID=true LockPersonality=true SystemCallArchitectures=native SystemCallFilter=@system-service SystemCallFilter=~@privileged @resources # Logging StandardOutput=journal StandardError=journal SyslogIdentifier=spooky [Install] WantedBy=multi-user.target Service Management # Install and enable service sudo systemctl daemon-reload sudo systemctl enable spooky.service # Start service sudo systemctl start spooky.service # Verify status sudo systemctl status spooky.service # View logs sudo journalctl -u spooky.service -f # Restart for configuration changes (hot reload planned) sudo systemctl restart spooky.service # Full restart sudo systemctl restart spooky.service Security Hardening Network Security Firewall Configuration (nftables) # /etc/nftables.conf (example rules) table inet filter { chain input { type filter hook input priority filter; policy drop; # Allow established/related connections ct state established,related accept ct state invalid drop # Allow loopback iif lo accept # Allow SSH (restrict to management network) ip saddr 10.0.0.0/24 tcp dport 22 ct state new accept # Allow QUIC/HTTP3 udp dport 443 accept # Allow health checks from monitoring (optional) ip saddr 10.0.0.0/24 tcp dport 8080 ct state new accept # Rate limiting for new connections ct state new limit rate over 1000/second burst 2000 packets drop } chain forward { type filter hook forward priority filter; policy drop; } chain output { type filter hook output priority filter; policy accept; } } Application Security Filesystem Permissions # Configuration immutable after validation sudo chown root:spooky /etc/spooky/config.yaml sudo chmod 640 /etc/spooky/config.yaml sudo chattr +i /etc/spooky/config.yaml # Immutable (remove with -i for updates) # Certificate protection sudo chmod 640 /etc/spooky/certs/privkey.pem sudo chmod 644 /etc/spooky/certs/fullchain.pem TLS Configuration Ensure TLS 1.3 is enforced with strong cipher suites. Note: cipher suite configuration may be limited by the underlying QUIC library (quiche). Verify supported options in the Spooky documentation. SELinux / AppArmor For environments requiring mandatory access control, create appropriate policies. Example AppArmor profile skeleton: # /etc/apparmor.d/usr.local.bin.spooky #include <tunables/global> /usr/local/bin/spooky { #include <abstractions/base> #include <abstractions/nameservice> capability net_bind_service, capability setuid, capability setgid, /usr/local/bin/spooky mr, /etc/spooky/** r, /var/log/spooky/** rw, network inet dgram, network inet6 dgram, network inet stream, network inet6 stream, } Monitoring and Observability Metrics Exposition Note : Metrics exposition is planned for future releases but not currently implemented. Spooky currently maintains internal counters only. When metrics are implemented, they will follow Prometheus exposition format for easy integration with monitoring systems. Example configuration (for future reference): # prometheus.yml (planned) scrape_configs: - job_name: 'spooky' scrape_interval: 15s scrape_timeout: 10s metrics_path: '/metrics' # To be implemented static_configs: - targets: ['spooky-01.internal:9090', 'spooky-02.internal:9090'] labels: environment: 'production' service: 'proxy' Key Metrics to Monitor Throughput Metrics - Requests per second (by route, backend, status code) - Bytes transferred (ingress/egress) - Active connections (QUIC, HTTP/2) Latency Metrics - Request duration percentiles (p50, p95, p99) - Backend response time - Connection establishment time - TLS handshake duration Error Metrics - HTTP 5xx error rate - Backend connection failures - Health check failure count - TLS handshake failures Resource Metrics - CPU utilization - Memory usage (RSS, heap) - File descriptor usage - Network buffer utilization Alerting Rules # prometheus-alerts.yml groups: - name: spooky-availability rules: - alert: SpookyInstanceDown expr: up{job=\"spooky\"} == 0 for: 1m labels: severity: critical annotations: summary: \"Spooky instance {{ $labels.instance }} is down\" description: \"Instance has been unreachable for 1 minute\" - alert: SpookyHighErrorRate expr: | ( sum(rate(http_requests_total{job=\"spooky\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{job=\"spooky\"}[5m])) ) > 0.05 for: 5m labels: severity: warning annotations: summary: \"High 5xx error rate on Spooky\" description: \"Error rate is {{ $value | humanizePercentage }}\" - alert: SpookyBackendAllDown expr: | sum by (upstream_pool) (backend_healthy{job=\"spooky\"}) == 0 for: 2m labels: severity: critical annotations: summary: \"All backends down for pool {{ $labels.upstream_pool }}\" - alert: SpookyLatencyHigh expr: | histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job=\"spooky\"}[5m])) by (le) ) > 1.0 for: 10m labels: severity: warning annotations: summary: \"High request latency (p95 > 1s)\" - alert: SpookyFileDescriptorExhaustion expr: process_open_fds{job=\"spooky\"} / process_max_fds{job=\"spooky\"} > 0.8 for: 5m labels: severity: warning annotations: summary: \"File descriptor usage high on {{ $labels.instance }}\" - name: spooky-capacity rules: - alert: SpookyCPUSaturation expr: rate(process_cpu_seconds_total{job=\"spooky\"}[5m]) > 0.8 for: 15m labels: severity: warning annotations: summary: \"CPU saturation on {{ $labels.instance }}\" - alert: SpookyMemoryPressure expr: | process_resident_memory_bytes{job=\"spooky\"} / node_memory_MemTotal_bytes > 0.8 for: 10m labels: severity: warning annotations: summary: \"Memory pressure on {{ $labels.instance }}\" Log Management Structured Logging Configure JSON output for log aggregation: log: level: info format: json # If supported Log Aggregation Ship logs to centralized logging (ELK, Loki, Splunk): # Example: journald to Loki via Promtail # /etc/promtail/config.yml server: http_listen_port: 9080 positions: filename: /var/lib/promtail/positions.yaml clients: - url: http://loki.internal:3100/loki/api/v1/push scrape_configs: - job_name: systemd-journal journal: max_age: 12h labels: job: systemd-journal relabel_configs: - source_labels: ['__journal__systemd_unit'] target_label: 'unit' - source_labels: ['__journal_syslog_identifier'] target_label: 'syslog_identifier' pipeline_stages: - match: selector: '{syslog_identifier=\"spooky\"}' stages: - json: expressions: level: level path: path backend: backend duration: duration - labels: level: path: backend: Log Rotation Configure log rotation for file-based logging (when stdout/stderr is redirected to files): # /etc/logrotate.d/spooky /var/log/spooky/*.log { daily rotate 14 compress delaycompress missingok notifempty create 0640 spooky spooky sharedscripts postrotate /bin/systemctl restart spooky.service > /dev/null 2>&1 || true endscript } High Availability Architecture Active-Active Configuration Deploy multiple Spooky instances behind a UDP-capable load balancer: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 DNS/GLB \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510 \u2502 L4 LB 1 \u2502 \u2502 L4 LB 2 \u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u2502Spooky 1\u2502 \u2502Spooky 2\u2502 ... \u2502Spooky N\u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Backend Pool \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Layer 4 Load Balancer Options: - ECMP routing with consistent hashing - Anycast with BGP - Cloud provider UDP load balancers (AWS NLB, GCP Network Load Balancer) Configuration Synchronization Maintain consistent configuration across instances: # Use configuration management (Ansible example) # playbooks/deploy-spooky.yml - hosts: proxy_tier become: yes tasks: - name: Deploy Spooky configuration template: src: templates/spooky-config.yaml.j2 dest: /etc/spooky/config.yaml owner: root group: spooky mode: '0640' # Note: Configuration validation happens during startup notify: restart spooky - name: Deploy TLS certificates copy: src: \"{{ item.src }}\" dest: \"{{ item.dest }}\" owner: root group: spooky mode: \"{{ item.mode }}\" with_items: - { src: 'certs/fullchain.pem', dest: '/etc/spooky/certs/fullchain.pem', mode: '0644' } - { src: 'certs/privkey.pem', dest: '/etc/spooky/certs/privkey.pem', mode: '0640' } notify: restart spooky handlers: - name: restart spooky systemd: name: spooky state: restarted Health Checking Implement external health checks for load balancer integration: # Health check script for L4 LB integration # /usr/local/bin/spooky-healthcheck.sh #!/bin/bash set -euo pipefail # Check if process is running if ! pgrep -x spooky > /dev/null; then exit 1 fi # Check if listening on QUIC port (example: port 443) if ! ss -ulpn | grep -q \":443 \"; then exit 1 fi # Optional: Check internal health endpoint if available # curl -sf http://localhost:8080/health || exit 1 exit 0 Performance Optimization Connection Pooling Optimize HTTP/2 backend connection pool based on backend capacity: Typical ratio: 1 backend connection per 50-100 concurrent QUIC connections Monitor backend connection state and adjust pool size accordingly Consider backend connection limits and TCP socket exhaustion QUIC Tuning QUIC performance depends on UDP buffer sizes and packet processing: # Increase UDP receive buffer for high packet rates # Already covered in kernel tuning section, but worth emphasizing: # net.core.rmem_max = 67108864 # net.core.rmem_default = 16777216 # Verify current settings sysctl net.core.rmem_max sysctl net.core.rmem_default # Monitor UDP receive buffer overflows netstat -su | grep \"receive errors\" CPU Affinity For multi-instance deployments on large systems, consider CPU pinning: # /etc/systemd/system/spooky@.service (template unit for multiple instances) [Service] # Pin instance 0 to CPUs 0-3, instance 1 to CPUs 4-7, etc. CPUAffinity=%i-$(((%i+1)*4-1)) Memory Management Monitor heap usage and consider tuning allocator behavior: # If using jemalloc (check with ldd /usr/local/bin/spooky) # Set environment variables for memory profiling # Environment=/usr/bin/env MALLOC_CONF=prof:true,prof_prefix:/var/log/spooky/jeprof Operational Procedures Deployment Process Configuration Validation : Validate new configuration in staging environment Gradual Rollout : Deploy to canary instance first, monitor error rates and latency Progressive Deployment : Roll out to remaining instances with staggered timing Rollback Plan : Keep previous binary version and configuration for rapid rollback # Deployment script example #!/bin/bash set -euo pipefail NEW_VERSION=\"$1\" INSTANCES=(\"spooky-01\" \"spooky-02\" \"spooky-03\") # Deploy to canary echo \"Deploying to canary: ${INSTANCES[0]}\" ssh \"${INSTANCES[0]}\" \"sudo systemctl stop spooky && \\ sudo cp /usr/local/bin/spooky /usr/local/bin/spooky.prev && \\ sudo wget -O /usr/local/bin/spooky https://releases.example.com/spooky-${NEW_VERSION} && \\ sudo systemctl start spooky\" echo \"Canary deployed. Monitor metrics for 5 minutes...\" sleep 300 # Check canary health if ! curl -sf \"http://${INSTANCES[0]}:8080/health\"; then echo \"Canary health check failed. Rolling back.\" ssh \"${INSTANCES[0]}\" \"sudo systemctl stop spooky && \\ sudo mv /usr/local/bin/spooky.prev /usr/local/bin/spooky && \\ sudo systemctl start spooky\" exit 1 fi # Deploy to remaining instances for instance in \"${INSTANCES[@]:1}\"; do echo \"Deploying to ${instance}\" ssh \"${instance}\" \"sudo systemctl stop spooky && \\ sudo cp /usr/local/bin/spooky /usr/local/bin/spooky.prev && \\ sudo wget -O /usr/local/bin/spooky https://releases.example.com/spooky-${NEW_VERSION} && \\ sudo systemctl start spooky\" sleep 30 done echo \"Deployment complete.\" Configuration Changes # Test configuration before applying (startup validation will happen) sudo -u spooky spooky --config /etc/spooky/config.yaml.new # Atomic configuration update sudo mv /etc/spooky/config.yaml /etc/spooky/config.yaml.backup sudo mv /etc/spooky/config.yaml.new /etc/spooky/config.yaml # Restart service (hot reload planned for future release) sudo systemctl restart spooky.service # Verify reload success sudo systemctl status spooky.service sudo journalctl -u spooky.service -n 50 --no-pager Incident Response High Error Rate Check backend health: sudo journalctl -u spooky.service | grep \"health check\" Verify backend connectivity: curl -v http://<backend-ip>:<port>/health Review recent configuration changes Check for backend capacity issues (CPU, memory, connection limits) If necessary, remove unhealthy backends from pool or rollback configuration Connection Exhaustion Check file descriptor usage: ls /proc/$(pgrep spooky)/fd | wc -l Review ulimits: cat /proc/$(pgrep spooky)/limits Identify connection leaks: ss -anp | grep spooky | wc -l Restart service if connection leak suspected: sudo systemctl restart spooky.service Memory Leak Monitor RSS over time: ps aux | grep spooky Capture heap profile if using jemalloc Review recent traffic patterns for anomalies Restart service to recover capacity, engage upstream support Capacity Planning Monitor these indicators for scaling decisions: Scale Horizontally (Add Instances) When: - CPU utilization sustained >70% across all instances - Network bandwidth saturation - Request queueing observed (increasing latency at constant RPS) Scale Vertically (Increase Resources) When: - Memory usage approaching limits - Context switching rate high with available CPU - Single-instance throughput below theoretical maximum Scaling Methodology: 1. Baseline current performance metrics 2. Load test with synthetic traffic at 2x current peak 3. Identify bottleneck (CPU, memory, network, backend capacity) 4. Size new deployment for 3x current peak with headroom 5. Implement autoscaling based on CPU/RPS metrics if using cloud infrastructure Troubleshooting Diagnostic Commands # Process information ps aux | grep spooky pstree -p $(pgrep spooky) # Open file descriptors ls -l /proc/$(pgrep spooky)/fd | wc -l lsof -p $(pgrep spooky) | head -20 # Network connections ss -anp | grep spooky | grep ESTABLISHED | wc -l ss -anp | grep spooky | grep TIME-WAIT | wc -l ss -su # UDP socket statistics # System calls and performance strace -c -p $(pgrep spooky) -e trace=network # 10 second sample perf top -p $(pgrep spooky) # Memory analysis cat /proc/$(pgrep spooky)/status | grep -E \"Vm|Rss\" pmap -x $(pgrep spooky) # Configuration verification sudo -u spooky spooky --config /etc/spooky/config.yaml Common Issues Service Fails to Start Symptoms: systemd reports failure, process exits immediately Diagnosis: # Check systemd logs sudo journalctl -u spooky.service -n 100 --no-pager # Test configuration manually sudo -u spooky /usr/local/bin/spooky --config /etc/spooky/config.yaml # Check certificate validity openssl x509 -noout -dates -in /etc/spooky/certs/fullchain.pem # Verify file permissions ls -la /etc/spooky/certs/ Resolution: Address configuration errors, certificate issues, or permission problems identified above. High Latency Symptoms: Increased p95/p99 request duration Diagnosis: # Check backend latency curl -w \"@curl-format.txt\" -o /dev/null -s http://<backend>/<path> # Network path latency mtr <backend-ip> # System resource contention top -p $(pgrep spooky) iostat -x 1 10 # Connection state distribution ss -anp | grep spooky | awk '{print $2}' | sort | uniq -c Resolution: Investigate backend performance, network conditions, or system resource exhaustion. Backend Connection Failures Symptoms: 502/503 errors, \"connection refused\" in logs Diagnosis: # Verify backend reachability nc -zv <backend-ip> <backend-port> # Check backend process state ssh <backend> \"systemctl status <backend-service>\" # Firewall/security group verification sudo iptables -L -n -v | grep <backend-ip> # Review health check logs sudo journalctl -u spooky.service | grep \"health check\" Resolution: Restore backend service, fix network connectivity, or adjust health check parameters. Backup and Disaster Recovery Configuration Backup # /usr/local/bin/backup-spooky-config.sh #!/bin/bash set -euo pipefail BACKUP_ROOT=\"/var/backups/spooky\" TIMESTAMP=$(date +%Y%m%d-%H%M%S) BACKUP_DIR=\"${BACKUP_ROOT}/${TIMESTAMP}\" mkdir -p \"${BACKUP_DIR}\" # Backup configuration cp -a /etc/spooky/config.yaml \"${BACKUP_DIR}/\" # Backup certificates (excluding private keys for security) cp /etc/spooky/certs/fullchain.pem \"${BACKUP_DIR}/\" # Store metadata cat > \"${BACKUP_DIR}/metadata.txt\" << EOF Backup Date: $(date -Is) Hostname: $(hostname -f) Spooky Version: $(spooky --version 2>&1 || echo \"unknown\") EOF # Compress backup tar czf \"${BACKUP_ROOT}/spooky-config-${TIMESTAMP}.tar.gz\" -C \"${BACKUP_ROOT}\" \"${TIMESTAMP}\" rm -rf \"${BACKUP_DIR}\" # Rotate old backups (keep 30 days) find \"${BACKUP_ROOT}\" -name \"spooky-config-*.tar.gz\" -mtime +30 -delete echo \"Backup completed: ${BACKUP_ROOT}/spooky-config-${TIMESTAMP}.tar.gz\" Schedule via cron: # Run daily at 2 AM 0 2 * * * /usr/local/bin/backup-spooky-config.sh >> /var/log/spooky/backup.log 2>&1 Recovery Procedures Configuration Restoration # Extract backup tar xzf /var/backups/spooky/spooky-config-YYYYMMDD-HHMMSS.tar.gz -C /tmp # Restore configuration sudo cp /tmp/YYYYMMDD-HHMMSS/config.yaml /etc/spooky/config.yaml sudo chown root:spooky /etc/spooky/config.yaml sudo chmod 640 /etc/spooky/config.yaml # Restart service (hot reload not currently supported) sudo systemctl restart spooky.service Complete System Rebuild 1. Provision new host with OS installation 2. Apply system configuration (kernel tuning, resource limits) 3. Install Spooky binary 4. Restore configuration from backup 5. Install TLS certificates 6. Start service and verify health 7. Update load balancer to include new instance Recovery Time Objective (RTO): Target <15 minutes with automation Recovery Point Objective (RPO): Configuration changes backed up daily Maintenance Windows Planned Maintenance Checklist Pre-Maintenance - [ ] Notify stakeholders of maintenance window - [ ] Verify backup procedures completed successfully - [ ] Review rollback procedures - [ ] Prepare configuration changes or binary updates - [ ] Verify staging environment changes successful During Maintenance - [ ] Remove instance from load balancer (if applicable) - [ ] Drain existing connections (if graceful shutdown supported) - [ ] Apply updates (configuration, binary, certificates) - [ ] Restart service - [ ] Verify service health and connectivity - [ ] Monitor error rates and latency for 5 minutes - [ ] Return instance to load balancer Post-Maintenance - [ ] Confirm all instances operational - [ ] Review metrics for anomalies - [ ] Update change log - [ ] Close maintenance notification Additional Resources Spooky Configuration Reference: /docs/configuration/reference.md Load Balancing Strategies: /docs/user-guide/load-balancing.md Troubleshooting Guide: /docs/troubleshooting/common-issues.md Architecture Overview: /docs/architecture/overview.md For issues not covered in this guide, consult the project repository issue tracker or engage with the development team.","title":"Production"},{"location":"deployment/production/#pre-deployment-checklist","text":"","title":"Pre-Deployment Checklist"},{"location":"deployment/production/#infrastructure-requirements","text":"Compute Resources - CPU: 4 cores minimum (8+ for high-throughput deployments) - Memory: 4GB minimum (8GB+ recommended, ~1-2KB per concurrent connection) - Disk: 10GB minimum (configuration, logs, and binary storage) - OS: Linux kernel 5.0+ (Ubuntu 20.04 LTS, RHEL 8+, or equivalent) Network Requirements - UDP ingress on designated QUIC port (typically 443) - HTTP/2 egress to backend pool networks - Low-latency connectivity between proxy tier and backends (<5ms RTT preferred) - MTU considerations: 1500 byte minimum, jumbo frames (9000 bytes) beneficial for high-throughput scenarios Certificate Infrastructure - Valid TLS certificates with full chain - Automated renewal mechanism (Let's Encrypt, internal PKI, or certificate management platform) - Certificate rotation procedures documented and tested","title":"Infrastructure Requirements"},{"location":"deployment/production/#pre-deployment-validation","text":"Before deploying to production, verify the following: Configuration validated with spooky --config <path> (startup validation happens before serving) Backend health check endpoints operational and returning expected responses TLS certificates valid with appropriate SANs and expiration dates Firewall rules permit required traffic flows Service account and filesystem permissions configured Monitoring and alerting infrastructure ready to receive metrics Runbooks prepared for common failure scenarios","title":"Pre-Deployment Validation"},{"location":"deployment/production/#system-configuration","text":"","title":"System Configuration"},{"location":"deployment/production/#binary-installation","text":"Production deployments should use compiled release binaries: # Download release binary VERSION=\"0.1.0\" ARCH=\"x86_64\" wget \"https://github.com/nishujangra/spooky/releases/download/v${VERSION}/spooky-linux-${ARCH}.tar.gz\" tar xzf \"spooky-linux-${ARCH}.tar.gz\" # Verify checksum sha256sum -c \"spooky-linux-${ARCH}.tar.gz.sha256\" # Install to system path sudo install -m 755 -o root -g root spooky /usr/local/bin/spooky # Create dedicated service account sudo useradd --system --shell /usr/sbin/nologin \\ --home-dir /var/lib/spooky --create-home spooky # Initialize directory structure sudo mkdir -p /etc/spooky/certs /var/log/spooky sudo chown -R root:spooky /etc/spooky sudo chmod 750 /etc/spooky sudo chown spooky:spooky /var/log/spooky sudo chmod 750 /var/log/spooky # Note: Spooky logs to stdout/stderr by default (collected by journald). # The /var/log/spooky directory is for optional file-based logging.","title":"Binary Installation"},{"location":"deployment/production/#kernel-parameter-tuning","text":"UDP and QUIC workloads benefit from increased buffer sizes and connection tracking limits: # /etc/sysctl.d/99-spooky.conf # UDP receive/send buffer tuning net.core.rmem_max = 67108864 net.core.wmem_max = 67108864 net.core.rmem_default = 16777216 net.core.wmem_default = 16777216 # Network device backlog net.core.netdev_max_backlog = 65536 net.core.netdev_budget = 50000 net.core.netdev_budget_usecs = 5000 # Connection tracking (if using conntrack) net.netfilter.nf_conntrack_max = 2097152 net.netfilter.nf_conntrack_tcp_timeout_established = 7200 net.netfilter.nf_conntrack_udp_timeout = 60 net.netfilter.nf_conntrack_udp_timeout_stream = 120 # TCP tuning for HTTP/2 backend connections net.ipv4.tcp_rmem = 8192 262144 33554432 net.ipv4.tcp_wmem = 8192 262144 33554432 net.ipv4.tcp_max_syn_backlog = 8192 net.ipv4.tcp_slow_start_after_idle = 0 net.ipv4.tcp_mtu_probing = 1 # File descriptor limits fs.file-max = 2097152 # Apply configuration sudo sysctl -p /etc/sysctl.d/99-spooky.conf","title":"Kernel Parameter Tuning"},{"location":"deployment/production/#resource-limits","text":"Configure ulimits for the spooky service account: # /etc/security/limits.d/spooky.conf spooky soft nofile 1048576 spooky hard nofile 1048576 spooky soft nproc 16384 spooky hard nproc 16384 spooky soft memlock unlimited spooky hard memlock unlimited","title":"Resource Limits"},{"location":"deployment/production/#production-configuration","text":"# /etc/spooky/config.yaml version: 1 listen: protocol: http3 address: \"0.0.0.0\" port: 443 tls: cert: \"/etc/spooky/certs/fullchain.pem\" key: \"/etc/spooky/certs/privkey.pem\" # Define upstream pools with health checking upstream: # API backend pool with consistent hashing for session affinity api_pool: load_balancing: type: \"consistent-hash\" route: path_prefix: \"/api\" backends: - id: \"api-01\" address: \"10.0.10.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 timeout_ms: 2000 failure_threshold: 3 success_threshold: 2 - id: \"api-02\" address: \"10.0.10.11:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 timeout_ms: 2000 failure_threshold: 3 success_threshold: 2 # Static content pool with round-robin static_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/static\" backends: - id: \"static-01\" address: \"10.0.20.10:8080\" weight: 100 health_check: path: \"/\" interval: 10000 - id: \"static-02\" address: \"10.0.20.11:8080\" weight: 100 health_check: path: \"/\" interval: 10000 # Default backend pool default_pool: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"web-01\" address: \"10.0.30.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 # Logging configuration log: level: info # Use 'warn' for production to reduce I/O # Connection tuning (if supported by configuration schema) # Adjust based on backend capacity and expected load # max_concurrent_connections: 10000 # backend_connection_pool_size: 100 Configuration Notes: - Route matching uses longest-prefix: more specific paths take precedence - Health check intervals balance detection speed vs. backend load - Adjust failure_threshold and success_threshold based on backend stability - Weight distribution should reflect backend capacity - Consistent hashing is appropriate for stateful backends requiring session affinity","title":"Production Configuration"},{"location":"deployment/production/#tls-certificate-management","text":"","title":"TLS Certificate Management"},{"location":"deployment/production/#certificate-acquisition","text":"","title":"Certificate Acquisition"},{"location":"deployment/production/#lets-encrypt-acme","text":"# Install certbot sudo apt-get install -y certbot # Obtain certificate (HTTP-01 challenge, requires port 80) sudo certbot certonly --standalone \\ --preferred-challenges http \\ --email ops@example.com \\ --agree-tos \\ --non-interactive \\ -d proxy.example.com # Copy to spooky directory sudo cp /etc/letsencrypt/live/proxy.example.com/fullchain.pem /etc/spooky/certs/ sudo cp /etc/letsencrypt/live/proxy.example.com/privkey.pem /etc/spooky/certs/ sudo chown root:spooky /etc/spooky/certs/*.pem sudo chmod 640 /etc/spooky/certs/privkey.pem sudo chmod 644 /etc/spooky/certs/fullchain.pem","title":"Let's Encrypt (ACME)"},{"location":"deployment/production/#automated-renewal","text":"# Create renewal hook sudo tee /etc/letsencrypt/renewal-hooks/deploy/spooky-reload.sh << 'EOF' #!/bin/bash set -e CERT_DOMAIN=\"proxy.example.com\" SPOOKY_CERT_DIR=\"/etc/spooky/certs\" # Copy renewed certificates cp \"/etc/letsencrypt/live/${CERT_DOMAIN}/fullchain.pem\" \"${SPOOKY_CERT_DIR}/\" cp \"/etc/letsencrypt/live/${CERT_DOMAIN}/privkey.pem\" \"${SPOOKY_CERT_DIR}/\" # Set permissions chown root:spooky \"${SPOOKY_CERT_DIR}\"/*.pem chmod 640 \"${SPOOKY_CERT_DIR}/privkey.pem\" chmod 644 \"${SPOOKY_CERT_DIR}/fullchain.pem\" # Reload spooky (graceful reload if supported, otherwise restart) systemctl reload-or-restart spooky logger -t spooky-cert-renewal \"TLS certificates renewed and spooky reloaded\" EOF sudo chmod +x /etc/letsencrypt/renewal-hooks/deploy/spooky-reload.sh # Test renewal process sudo certbot renew --dry-run","title":"Automated Renewal"},{"location":"deployment/production/#certificate-validation","text":"Before deploying new certificates: # Verify certificate and key match openssl x509 -noout -modulus -in /etc/spooky/certs/fullchain.pem | openssl md5 openssl rsa -noout -modulus -in /etc/spooky/certs/privkey.pem | openssl md5 # Verify certificate chain openssl verify -CAfile /etc/spooky/certs/fullchain.pem /etc/spooky/certs/fullchain.pem # Check expiration openssl x509 -noout -dates -in /etc/spooky/certs/fullchain.pem # Verify SAN entries openssl x509 -noout -text -in /etc/spooky/certs/fullchain.pem | grep -A1 \"Subject Alternative Name\"","title":"Certificate Validation"},{"location":"deployment/production/#systemd-service-configuration","text":"","title":"Systemd Service Configuration"},{"location":"deployment/production/#service-unit","text":"# /etc/systemd/system/spooky.service [Unit] Description=Spooky HTTP/3 to HTTP/2 Proxy Documentation=https://github.com/nishujangra/spooky After=network-online.target Wants=network-online.target [Service] Type=simple User=spooky Group=spooky # Binary and configuration ExecStart=/usr/local/bin/spooky --config /etc/spooky/config.yaml # Note: Hot reload not currently supported, use restart instead # ExecReload=/bin/kill -HUP $MAINPID # Restart policy Restart=always RestartSec=5s StartLimitBurst=3 StartLimitIntervalSec=60s # Resource limits LimitNOFILE=1048576 LimitNPROC=16384 TasksMax=16384 # Security hardening NoNewPrivileges=true PrivateTmp=true ProtectSystem=strict ProtectHome=true ReadWritePaths=/var/log/spooky ProtectKernelTunables=true ProtectKernelModules=true ProtectKernelLogs=true ProtectControlGroups=true ProtectProc=invisible ProcSubset=pid RestrictAddressFamilies=AF_INET AF_INET6 AF_UNIX RestrictNamespaces=true RestrictRealtime=true RestrictSUIDSGID=true LockPersonality=true SystemCallArchitectures=native SystemCallFilter=@system-service SystemCallFilter=~@privileged @resources # Logging StandardOutput=journal StandardError=journal SyslogIdentifier=spooky [Install] WantedBy=multi-user.target","title":"Service Unit"},{"location":"deployment/production/#service-management","text":"# Install and enable service sudo systemctl daemon-reload sudo systemctl enable spooky.service # Start service sudo systemctl start spooky.service # Verify status sudo systemctl status spooky.service # View logs sudo journalctl -u spooky.service -f # Restart for configuration changes (hot reload planned) sudo systemctl restart spooky.service # Full restart sudo systemctl restart spooky.service","title":"Service Management"},{"location":"deployment/production/#security-hardening","text":"","title":"Security Hardening"},{"location":"deployment/production/#network-security","text":"Firewall Configuration (nftables) # /etc/nftables.conf (example rules) table inet filter { chain input { type filter hook input priority filter; policy drop; # Allow established/related connections ct state established,related accept ct state invalid drop # Allow loopback iif lo accept # Allow SSH (restrict to management network) ip saddr 10.0.0.0/24 tcp dport 22 ct state new accept # Allow QUIC/HTTP3 udp dport 443 accept # Allow health checks from monitoring (optional) ip saddr 10.0.0.0/24 tcp dport 8080 ct state new accept # Rate limiting for new connections ct state new limit rate over 1000/second burst 2000 packets drop } chain forward { type filter hook forward priority filter; policy drop; } chain output { type filter hook output priority filter; policy accept; } }","title":"Network Security"},{"location":"deployment/production/#application-security","text":"Filesystem Permissions # Configuration immutable after validation sudo chown root:spooky /etc/spooky/config.yaml sudo chmod 640 /etc/spooky/config.yaml sudo chattr +i /etc/spooky/config.yaml # Immutable (remove with -i for updates) # Certificate protection sudo chmod 640 /etc/spooky/certs/privkey.pem sudo chmod 644 /etc/spooky/certs/fullchain.pem TLS Configuration Ensure TLS 1.3 is enforced with strong cipher suites. Note: cipher suite configuration may be limited by the underlying QUIC library (quiche). Verify supported options in the Spooky documentation.","title":"Application Security"},{"location":"deployment/production/#selinux-apparmor","text":"For environments requiring mandatory access control, create appropriate policies. Example AppArmor profile skeleton: # /etc/apparmor.d/usr.local.bin.spooky #include <tunables/global> /usr/local/bin/spooky { #include <abstractions/base> #include <abstractions/nameservice> capability net_bind_service, capability setuid, capability setgid, /usr/local/bin/spooky mr, /etc/spooky/** r, /var/log/spooky/** rw, network inet dgram, network inet6 dgram, network inet stream, network inet6 stream, }","title":"SELinux / AppArmor"},{"location":"deployment/production/#monitoring-and-observability","text":"","title":"Monitoring and Observability"},{"location":"deployment/production/#metrics-exposition","text":"Note : Metrics exposition is planned for future releases but not currently implemented. Spooky currently maintains internal counters only. When metrics are implemented, they will follow Prometheus exposition format for easy integration with monitoring systems. Example configuration (for future reference): # prometheus.yml (planned) scrape_configs: - job_name: 'spooky' scrape_interval: 15s scrape_timeout: 10s metrics_path: '/metrics' # To be implemented static_configs: - targets: ['spooky-01.internal:9090', 'spooky-02.internal:9090'] labels: environment: 'production' service: 'proxy'","title":"Metrics Exposition"},{"location":"deployment/production/#key-metrics-to-monitor","text":"Throughput Metrics - Requests per second (by route, backend, status code) - Bytes transferred (ingress/egress) - Active connections (QUIC, HTTP/2) Latency Metrics - Request duration percentiles (p50, p95, p99) - Backend response time - Connection establishment time - TLS handshake duration Error Metrics - HTTP 5xx error rate - Backend connection failures - Health check failure count - TLS handshake failures Resource Metrics - CPU utilization - Memory usage (RSS, heap) - File descriptor usage - Network buffer utilization","title":"Key Metrics to Monitor"},{"location":"deployment/production/#alerting-rules","text":"# prometheus-alerts.yml groups: - name: spooky-availability rules: - alert: SpookyInstanceDown expr: up{job=\"spooky\"} == 0 for: 1m labels: severity: critical annotations: summary: \"Spooky instance {{ $labels.instance }} is down\" description: \"Instance has been unreachable for 1 minute\" - alert: SpookyHighErrorRate expr: | ( sum(rate(http_requests_total{job=\"spooky\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{job=\"spooky\"}[5m])) ) > 0.05 for: 5m labels: severity: warning annotations: summary: \"High 5xx error rate on Spooky\" description: \"Error rate is {{ $value | humanizePercentage }}\" - alert: SpookyBackendAllDown expr: | sum by (upstream_pool) (backend_healthy{job=\"spooky\"}) == 0 for: 2m labels: severity: critical annotations: summary: \"All backends down for pool {{ $labels.upstream_pool }}\" - alert: SpookyLatencyHigh expr: | histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job=\"spooky\"}[5m])) by (le) ) > 1.0 for: 10m labels: severity: warning annotations: summary: \"High request latency (p95 > 1s)\" - alert: SpookyFileDescriptorExhaustion expr: process_open_fds{job=\"spooky\"} / process_max_fds{job=\"spooky\"} > 0.8 for: 5m labels: severity: warning annotations: summary: \"File descriptor usage high on {{ $labels.instance }}\" - name: spooky-capacity rules: - alert: SpookyCPUSaturation expr: rate(process_cpu_seconds_total{job=\"spooky\"}[5m]) > 0.8 for: 15m labels: severity: warning annotations: summary: \"CPU saturation on {{ $labels.instance }}\" - alert: SpookyMemoryPressure expr: | process_resident_memory_bytes{job=\"spooky\"} / node_memory_MemTotal_bytes > 0.8 for: 10m labels: severity: warning annotations: summary: \"Memory pressure on {{ $labels.instance }}\"","title":"Alerting Rules"},{"location":"deployment/production/#log-management","text":"Structured Logging Configure JSON output for log aggregation: log: level: info format: json # If supported Log Aggregation Ship logs to centralized logging (ELK, Loki, Splunk): # Example: journald to Loki via Promtail # /etc/promtail/config.yml server: http_listen_port: 9080 positions: filename: /var/lib/promtail/positions.yaml clients: - url: http://loki.internal:3100/loki/api/v1/push scrape_configs: - job_name: systemd-journal journal: max_age: 12h labels: job: systemd-journal relabel_configs: - source_labels: ['__journal__systemd_unit'] target_label: 'unit' - source_labels: ['__journal_syslog_identifier'] target_label: 'syslog_identifier' pipeline_stages: - match: selector: '{syslog_identifier=\"spooky\"}' stages: - json: expressions: level: level path: path backend: backend duration: duration - labels: level: path: backend: Log Rotation Configure log rotation for file-based logging (when stdout/stderr is redirected to files): # /etc/logrotate.d/spooky /var/log/spooky/*.log { daily rotate 14 compress delaycompress missingok notifempty create 0640 spooky spooky sharedscripts postrotate /bin/systemctl restart spooky.service > /dev/null 2>&1 || true endscript }","title":"Log Management"},{"location":"deployment/production/#high-availability-architecture","text":"","title":"High Availability Architecture"},{"location":"deployment/production/#active-active-configuration","text":"Deploy multiple Spooky instances behind a UDP-capable load balancer: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 DNS/GLB \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510 \u2502 L4 LB 1 \u2502 \u2502 L4 LB 2 \u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u2502Spooky 1\u2502 \u2502Spooky 2\u2502 ... \u2502Spooky N\u2502 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Backend Pool \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Layer 4 Load Balancer Options: - ECMP routing with consistent hashing - Anycast with BGP - Cloud provider UDP load balancers (AWS NLB, GCP Network Load Balancer)","title":"Active-Active Configuration"},{"location":"deployment/production/#configuration-synchronization","text":"Maintain consistent configuration across instances: # Use configuration management (Ansible example) # playbooks/deploy-spooky.yml - hosts: proxy_tier become: yes tasks: - name: Deploy Spooky configuration template: src: templates/spooky-config.yaml.j2 dest: /etc/spooky/config.yaml owner: root group: spooky mode: '0640' # Note: Configuration validation happens during startup notify: restart spooky - name: Deploy TLS certificates copy: src: \"{{ item.src }}\" dest: \"{{ item.dest }}\" owner: root group: spooky mode: \"{{ item.mode }}\" with_items: - { src: 'certs/fullchain.pem', dest: '/etc/spooky/certs/fullchain.pem', mode: '0644' } - { src: 'certs/privkey.pem', dest: '/etc/spooky/certs/privkey.pem', mode: '0640' } notify: restart spooky handlers: - name: restart spooky systemd: name: spooky state: restarted","title":"Configuration Synchronization"},{"location":"deployment/production/#health-checking","text":"Implement external health checks for load balancer integration: # Health check script for L4 LB integration # /usr/local/bin/spooky-healthcheck.sh #!/bin/bash set -euo pipefail # Check if process is running if ! pgrep -x spooky > /dev/null; then exit 1 fi # Check if listening on QUIC port (example: port 443) if ! ss -ulpn | grep -q \":443 \"; then exit 1 fi # Optional: Check internal health endpoint if available # curl -sf http://localhost:8080/health || exit 1 exit 0","title":"Health Checking"},{"location":"deployment/production/#performance-optimization","text":"","title":"Performance Optimization"},{"location":"deployment/production/#connection-pooling","text":"Optimize HTTP/2 backend connection pool based on backend capacity: Typical ratio: 1 backend connection per 50-100 concurrent QUIC connections Monitor backend connection state and adjust pool size accordingly Consider backend connection limits and TCP socket exhaustion","title":"Connection Pooling"},{"location":"deployment/production/#quic-tuning","text":"QUIC performance depends on UDP buffer sizes and packet processing: # Increase UDP receive buffer for high packet rates # Already covered in kernel tuning section, but worth emphasizing: # net.core.rmem_max = 67108864 # net.core.rmem_default = 16777216 # Verify current settings sysctl net.core.rmem_max sysctl net.core.rmem_default # Monitor UDP receive buffer overflows netstat -su | grep \"receive errors\"","title":"QUIC Tuning"},{"location":"deployment/production/#cpu-affinity","text":"For multi-instance deployments on large systems, consider CPU pinning: # /etc/systemd/system/spooky@.service (template unit for multiple instances) [Service] # Pin instance 0 to CPUs 0-3, instance 1 to CPUs 4-7, etc. CPUAffinity=%i-$(((%i+1)*4-1))","title":"CPU Affinity"},{"location":"deployment/production/#memory-management","text":"Monitor heap usage and consider tuning allocator behavior: # If using jemalloc (check with ldd /usr/local/bin/spooky) # Set environment variables for memory profiling # Environment=/usr/bin/env MALLOC_CONF=prof:true,prof_prefix:/var/log/spooky/jeprof","title":"Memory Management"},{"location":"deployment/production/#operational-procedures","text":"","title":"Operational Procedures"},{"location":"deployment/production/#deployment-process","text":"Configuration Validation : Validate new configuration in staging environment Gradual Rollout : Deploy to canary instance first, monitor error rates and latency Progressive Deployment : Roll out to remaining instances with staggered timing Rollback Plan : Keep previous binary version and configuration for rapid rollback # Deployment script example #!/bin/bash set -euo pipefail NEW_VERSION=\"$1\" INSTANCES=(\"spooky-01\" \"spooky-02\" \"spooky-03\") # Deploy to canary echo \"Deploying to canary: ${INSTANCES[0]}\" ssh \"${INSTANCES[0]}\" \"sudo systemctl stop spooky && \\ sudo cp /usr/local/bin/spooky /usr/local/bin/spooky.prev && \\ sudo wget -O /usr/local/bin/spooky https://releases.example.com/spooky-${NEW_VERSION} && \\ sudo systemctl start spooky\" echo \"Canary deployed. Monitor metrics for 5 minutes...\" sleep 300 # Check canary health if ! curl -sf \"http://${INSTANCES[0]}:8080/health\"; then echo \"Canary health check failed. Rolling back.\" ssh \"${INSTANCES[0]}\" \"sudo systemctl stop spooky && \\ sudo mv /usr/local/bin/spooky.prev /usr/local/bin/spooky && \\ sudo systemctl start spooky\" exit 1 fi # Deploy to remaining instances for instance in \"${INSTANCES[@]:1}\"; do echo \"Deploying to ${instance}\" ssh \"${instance}\" \"sudo systemctl stop spooky && \\ sudo cp /usr/local/bin/spooky /usr/local/bin/spooky.prev && \\ sudo wget -O /usr/local/bin/spooky https://releases.example.com/spooky-${NEW_VERSION} && \\ sudo systemctl start spooky\" sleep 30 done echo \"Deployment complete.\"","title":"Deployment Process"},{"location":"deployment/production/#configuration-changes","text":"# Test configuration before applying (startup validation will happen) sudo -u spooky spooky --config /etc/spooky/config.yaml.new # Atomic configuration update sudo mv /etc/spooky/config.yaml /etc/spooky/config.yaml.backup sudo mv /etc/spooky/config.yaml.new /etc/spooky/config.yaml # Restart service (hot reload planned for future release) sudo systemctl restart spooky.service # Verify reload success sudo systemctl status spooky.service sudo journalctl -u spooky.service -n 50 --no-pager","title":"Configuration Changes"},{"location":"deployment/production/#incident-response","text":"High Error Rate Check backend health: sudo journalctl -u spooky.service | grep \"health check\" Verify backend connectivity: curl -v http://<backend-ip>:<port>/health Review recent configuration changes Check for backend capacity issues (CPU, memory, connection limits) If necessary, remove unhealthy backends from pool or rollback configuration Connection Exhaustion Check file descriptor usage: ls /proc/$(pgrep spooky)/fd | wc -l Review ulimits: cat /proc/$(pgrep spooky)/limits Identify connection leaks: ss -anp | grep spooky | wc -l Restart service if connection leak suspected: sudo systemctl restart spooky.service Memory Leak Monitor RSS over time: ps aux | grep spooky Capture heap profile if using jemalloc Review recent traffic patterns for anomalies Restart service to recover capacity, engage upstream support","title":"Incident Response"},{"location":"deployment/production/#capacity-planning","text":"Monitor these indicators for scaling decisions: Scale Horizontally (Add Instances) When: - CPU utilization sustained >70% across all instances - Network bandwidth saturation - Request queueing observed (increasing latency at constant RPS) Scale Vertically (Increase Resources) When: - Memory usage approaching limits - Context switching rate high with available CPU - Single-instance throughput below theoretical maximum Scaling Methodology: 1. Baseline current performance metrics 2. Load test with synthetic traffic at 2x current peak 3. Identify bottleneck (CPU, memory, network, backend capacity) 4. Size new deployment for 3x current peak with headroom 5. Implement autoscaling based on CPU/RPS metrics if using cloud infrastructure","title":"Capacity Planning"},{"location":"deployment/production/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"deployment/production/#diagnostic-commands","text":"# Process information ps aux | grep spooky pstree -p $(pgrep spooky) # Open file descriptors ls -l /proc/$(pgrep spooky)/fd | wc -l lsof -p $(pgrep spooky) | head -20 # Network connections ss -anp | grep spooky | grep ESTABLISHED | wc -l ss -anp | grep spooky | grep TIME-WAIT | wc -l ss -su # UDP socket statistics # System calls and performance strace -c -p $(pgrep spooky) -e trace=network # 10 second sample perf top -p $(pgrep spooky) # Memory analysis cat /proc/$(pgrep spooky)/status | grep -E \"Vm|Rss\" pmap -x $(pgrep spooky) # Configuration verification sudo -u spooky spooky --config /etc/spooky/config.yaml","title":"Diagnostic Commands"},{"location":"deployment/production/#common-issues","text":"Service Fails to Start Symptoms: systemd reports failure, process exits immediately Diagnosis: # Check systemd logs sudo journalctl -u spooky.service -n 100 --no-pager # Test configuration manually sudo -u spooky /usr/local/bin/spooky --config /etc/spooky/config.yaml # Check certificate validity openssl x509 -noout -dates -in /etc/spooky/certs/fullchain.pem # Verify file permissions ls -la /etc/spooky/certs/ Resolution: Address configuration errors, certificate issues, or permission problems identified above. High Latency Symptoms: Increased p95/p99 request duration Diagnosis: # Check backend latency curl -w \"@curl-format.txt\" -o /dev/null -s http://<backend>/<path> # Network path latency mtr <backend-ip> # System resource contention top -p $(pgrep spooky) iostat -x 1 10 # Connection state distribution ss -anp | grep spooky | awk '{print $2}' | sort | uniq -c Resolution: Investigate backend performance, network conditions, or system resource exhaustion. Backend Connection Failures Symptoms: 502/503 errors, \"connection refused\" in logs Diagnosis: # Verify backend reachability nc -zv <backend-ip> <backend-port> # Check backend process state ssh <backend> \"systemctl status <backend-service>\" # Firewall/security group verification sudo iptables -L -n -v | grep <backend-ip> # Review health check logs sudo journalctl -u spooky.service | grep \"health check\" Resolution: Restore backend service, fix network connectivity, or adjust health check parameters.","title":"Common Issues"},{"location":"deployment/production/#backup-and-disaster-recovery","text":"","title":"Backup and Disaster Recovery"},{"location":"deployment/production/#configuration-backup","text":"# /usr/local/bin/backup-spooky-config.sh #!/bin/bash set -euo pipefail BACKUP_ROOT=\"/var/backups/spooky\" TIMESTAMP=$(date +%Y%m%d-%H%M%S) BACKUP_DIR=\"${BACKUP_ROOT}/${TIMESTAMP}\" mkdir -p \"${BACKUP_DIR}\" # Backup configuration cp -a /etc/spooky/config.yaml \"${BACKUP_DIR}/\" # Backup certificates (excluding private keys for security) cp /etc/spooky/certs/fullchain.pem \"${BACKUP_DIR}/\" # Store metadata cat > \"${BACKUP_DIR}/metadata.txt\" << EOF Backup Date: $(date -Is) Hostname: $(hostname -f) Spooky Version: $(spooky --version 2>&1 || echo \"unknown\") EOF # Compress backup tar czf \"${BACKUP_ROOT}/spooky-config-${TIMESTAMP}.tar.gz\" -C \"${BACKUP_ROOT}\" \"${TIMESTAMP}\" rm -rf \"${BACKUP_DIR}\" # Rotate old backups (keep 30 days) find \"${BACKUP_ROOT}\" -name \"spooky-config-*.tar.gz\" -mtime +30 -delete echo \"Backup completed: ${BACKUP_ROOT}/spooky-config-${TIMESTAMP}.tar.gz\" Schedule via cron: # Run daily at 2 AM 0 2 * * * /usr/local/bin/backup-spooky-config.sh >> /var/log/spooky/backup.log 2>&1","title":"Configuration Backup"},{"location":"deployment/production/#recovery-procedures","text":"Configuration Restoration # Extract backup tar xzf /var/backups/spooky/spooky-config-YYYYMMDD-HHMMSS.tar.gz -C /tmp # Restore configuration sudo cp /tmp/YYYYMMDD-HHMMSS/config.yaml /etc/spooky/config.yaml sudo chown root:spooky /etc/spooky/config.yaml sudo chmod 640 /etc/spooky/config.yaml # Restart service (hot reload not currently supported) sudo systemctl restart spooky.service Complete System Rebuild 1. Provision new host with OS installation 2. Apply system configuration (kernel tuning, resource limits) 3. Install Spooky binary 4. Restore configuration from backup 5. Install TLS certificates 6. Start service and verify health 7. Update load balancer to include new instance Recovery Time Objective (RTO): Target <15 minutes with automation Recovery Point Objective (RPO): Configuration changes backed up daily","title":"Recovery Procedures"},{"location":"deployment/production/#maintenance-windows","text":"","title":"Maintenance Windows"},{"location":"deployment/production/#planned-maintenance-checklist","text":"Pre-Maintenance - [ ] Notify stakeholders of maintenance window - [ ] Verify backup procedures completed successfully - [ ] Review rollback procedures - [ ] Prepare configuration changes or binary updates - [ ] Verify staging environment changes successful During Maintenance - [ ] Remove instance from load balancer (if applicable) - [ ] Drain existing connections (if graceful shutdown supported) - [ ] Apply updates (configuration, binary, certificates) - [ ] Restart service - [ ] Verify service health and connectivity - [ ] Monitor error rates and latency for 5 minutes - [ ] Return instance to load balancer Post-Maintenance - [ ] Confirm all instances operational - [ ] Review metrics for anomalies - [ ] Update change log - [ ] Close maintenance notification","title":"Planned Maintenance Checklist"},{"location":"deployment/production/#additional-resources","text":"Spooky Configuration Reference: /docs/configuration/reference.md Load Balancing Strategies: /docs/user-guide/load-balancing.md Troubleshooting Guide: /docs/troubleshooting/common-issues.md Architecture Overview: /docs/architecture/overview.md For issues not covered in this guide, consult the project repository issue tracker or engage with the development team.","title":"Additional Resources"},{"location":"getting-started/installation/","text":"System Requirements Hardware: - CPU: 1 core minimum (2+ cores recommended for production) - Memory: 256MB RAM minimum (1GB+ recommended) - Disk: 100MB for binary and configuration files - Network: UDP port access for QUIC traffic Software: - Rust 1.85 or later (2024 edition) - Operating System: Linux, macOS, or Windows - Build tools: CMake, pkg-config, C compiler toolchain Permissions: - Spooky must run as root (required for QUIC/UDP socket binding) Installation Methods Pre-built Binaries Download the latest release from GitHub Releases . Linux (x86_64): wget https://github.com/nishujangra/spooky/releases/download/v0.1.0/spooky-linux-x86_64.tar.gz tar -xzf spooky-linux-x86_64.tar.gz sudo install -m 755 spooky /usr/local/bin/spooky macOS (x86_64/ARM64): wget https://github.com/nishujangra/spooky/releases/download/v0.1.0/spooky-macos-universal.tar.gz tar -xzf spooky-macos-universal.tar.gz sudo install -m 755 spooky /usr/local/bin/spooky Cargo Install Install directly from crates.io: cargo install spooky This compiles from source and installs to ~/.cargo/bin/ . Ensure this directory is in your PATH. Build from Source Clone and build: git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release The binary is generated at target/release/spooky . Run tests (optional): cargo test cargo test -p spooky-edge --test lb_integration System-wide installation: sudo install -m 755 target/release/spooky /usr/local/bin/spooky Platform-Specific Setup Ubuntu/Debian # Install build dependencies sudo apt update sudo apt install -y cmake build-essential pkg-config # Install Rust if not present curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh source ~/.cargo/env # Build and install git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release sudo install -m 755 target/release/spooky /usr/local/bin/spooky CentOS/RHEL 8+ # Install build dependencies sudo dnf groupinstall -y \"Development Tools\" sudo dnf install -y cmake pkgconfig # Install Rust if not present curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh source ~/.cargo/env # Build and install git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release sudo install -m 755 target/release/spooky /usr/local/bin/spooky macOS # Install dependencies via Homebrew brew install cmake pkg-config rust # Build and install git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release sudo install -m 755 target/release/spooky /usr/local/bin/spooky Windows Prerequisites: 1. Install Rust from rustup.rs 2. Install Visual Studio Build Tools with C++ support from Microsoft Build: git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release Binary location: target\\release\\spooky.exe Docker Deployment Dockerfile: FROM rust:1.70-slim as builder WORKDIR /app COPY . . RUN cargo build --release FROM debian:bookworm-slim RUN apt-get update && \\ apt-get install -y ca-certificates && \\ rm -rf /var/lib/apt/lists/* COPY --from=builder /app/target/release/spooky /usr/local/bin/spooky EXPOSE 9889/udp CMD [\"spooky\"] Build and run: docker build -t spooky:latest . docker run -d \\ --name spooky \\ -p 9889:9889/udp \\ -v /path/to/config.yaml:/etc/spooky/config.yaml:ro \\ -v /path/to/certs:/etc/spooky/certs:ro \\ spooky:latest --config /etc/spooky/config.yaml Using Docker Compose: version: '3.8' services: spooky: build: . ports: - \"9889:9889/udp\" volumes: - ./config.yaml:/etc/spooky/config.yaml:ro - ./certs:/etc/spooky/certs:ro command: [\"--config\", \"/etc/spooky/config.yaml\"] restart: unless-stopped Installation Verification # Verify binary is accessible spooky --version # Display help and available options spooky --help # Validate configuration syntax (startup validation happens before serving) spooky --config /path/to/config.yaml Expected output from spooky --version : spooky 0.1.0 Post-Installation Configuration 1. Create Configuration Directory sudo mkdir -p /etc/spooky sudo mkdir -p /etc/spooky/certs sudo chown -R $(whoami) /etc/spooky 2. Generate TLS Certificates Self-signed certificates (development): openssl req -x509 -newkey rsa:4096 -nodes \\ -keyout /etc/spooky/certs/key.pem \\ -out /etc/spooky/certs/cert.pem \\ -days 365 \\ -subj \"/CN=proxy.example.com\" For production certificates, see TLS Configuration . 3. Create Base Configuration Create /etc/spooky/config.yaml : version: 1 listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"/etc/spooky/certs/cert.pem\" key: \"/etc/spooky/certs/key.pem\" upstream: default: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"backend-1\" address: \"127.0.0.1:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 log: level: info 4. System Service Setup (Linux) Create /etc/systemd/system/spooky.service : [Unit] Description=Spooky HTTP/3 to HTTP/2 Proxy After=network.target [Service] Type=simple User=spooky Group=spooky ExecStart=/usr/local/bin/spooky --config /etc/spooky/config.yaml Restart=on-failure RestartSec=5s # Security hardening NoNewPrivileges=true PrivateTmp=true ProtectSystem=strict ProtectHome=true ReadWritePaths=/var/log/spooky [Install] WantedBy=multi-user.target Create service user and enable: sudo useradd -r -s /bin/false spooky sudo systemctl daemon-reload sudo systemctl enable spooky.service sudo systemctl start spooky.service 5. Log Management By default, Spooky logs to stderr (captured by journald under systemd). To write logs to a file instead, set log.file.enabled: true in your config: log: level: info file: enabled: true path: /var/log/spooky/spooky.log Configure log rotation for file-based logging: Create /etc/logrotate.d/spooky : /var/log/spooky/*.log { daily rotate 14 compress delaycompress missingok notifempty create 0640 spooky spooky sharedscripts postrotate systemctl restart spooky.service >/dev/null 2>&1 || true endscript } Troubleshooting Installation Build fails with linker errors: - Ensure build tools are installed: cmake , pkg-config , C compiler - Update Rust toolchain: rustup update Permission denied when binding to port: - Use port > 1024, or grant capability: sudo setcap CAP_NET_BIND_SERVICE=+eip /usr/local/bin/spooky - Run as privileged user (not recommended for production) Certificate errors on startup: - Verify certificate and key paths in configuration - Check file permissions: certificates must be readable by the service user - Validate certificate format: openssl x509 -in cert.pem -text -noout Binary not found after cargo install: - Add ~/.cargo/bin to PATH: export PATH=\"$HOME/.cargo/bin:$PATH\" - Add to shell profile for persistence Next Steps Configuration Reference - Complete configuration options TLS Setup Guide - Production certificate management Production Deployment - Production deployment best practices Troubleshooting - Common issues and solutions","title":"Installation"},{"location":"getting-started/installation/#system-requirements","text":"Hardware: - CPU: 1 core minimum (2+ cores recommended for production) - Memory: 256MB RAM minimum (1GB+ recommended) - Disk: 100MB for binary and configuration files - Network: UDP port access for QUIC traffic Software: - Rust 1.85 or later (2024 edition) - Operating System: Linux, macOS, or Windows - Build tools: CMake, pkg-config, C compiler toolchain Permissions: - Spooky must run as root (required for QUIC/UDP socket binding)","title":"System Requirements"},{"location":"getting-started/installation/#installation-methods","text":"","title":"Installation Methods"},{"location":"getting-started/installation/#pre-built-binaries","text":"Download the latest release from GitHub Releases . Linux (x86_64): wget https://github.com/nishujangra/spooky/releases/download/v0.1.0/spooky-linux-x86_64.tar.gz tar -xzf spooky-linux-x86_64.tar.gz sudo install -m 755 spooky /usr/local/bin/spooky macOS (x86_64/ARM64): wget https://github.com/nishujangra/spooky/releases/download/v0.1.0/spooky-macos-universal.tar.gz tar -xzf spooky-macos-universal.tar.gz sudo install -m 755 spooky /usr/local/bin/spooky","title":"Pre-built Binaries"},{"location":"getting-started/installation/#cargo-install","text":"Install directly from crates.io: cargo install spooky This compiles from source and installs to ~/.cargo/bin/ . Ensure this directory is in your PATH.","title":"Cargo Install"},{"location":"getting-started/installation/#build-from-source","text":"Clone and build: git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release The binary is generated at target/release/spooky . Run tests (optional): cargo test cargo test -p spooky-edge --test lb_integration System-wide installation: sudo install -m 755 target/release/spooky /usr/local/bin/spooky","title":"Build from Source"},{"location":"getting-started/installation/#platform-specific-setup","text":"","title":"Platform-Specific Setup"},{"location":"getting-started/installation/#ubuntudebian","text":"# Install build dependencies sudo apt update sudo apt install -y cmake build-essential pkg-config # Install Rust if not present curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh source ~/.cargo/env # Build and install git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release sudo install -m 755 target/release/spooky /usr/local/bin/spooky","title":"Ubuntu/Debian"},{"location":"getting-started/installation/#centosrhel-8","text":"# Install build dependencies sudo dnf groupinstall -y \"Development Tools\" sudo dnf install -y cmake pkgconfig # Install Rust if not present curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh source ~/.cargo/env # Build and install git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release sudo install -m 755 target/release/spooky /usr/local/bin/spooky","title":"CentOS/RHEL 8+"},{"location":"getting-started/installation/#macos","text":"# Install dependencies via Homebrew brew install cmake pkg-config rust # Build and install git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release sudo install -m 755 target/release/spooky /usr/local/bin/spooky","title":"macOS"},{"location":"getting-started/installation/#windows","text":"Prerequisites: 1. Install Rust from rustup.rs 2. Install Visual Studio Build Tools with C++ support from Microsoft Build: git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release Binary location: target\\release\\spooky.exe","title":"Windows"},{"location":"getting-started/installation/#docker-deployment","text":"Dockerfile: FROM rust:1.70-slim as builder WORKDIR /app COPY . . RUN cargo build --release FROM debian:bookworm-slim RUN apt-get update && \\ apt-get install -y ca-certificates && \\ rm -rf /var/lib/apt/lists/* COPY --from=builder /app/target/release/spooky /usr/local/bin/spooky EXPOSE 9889/udp CMD [\"spooky\"] Build and run: docker build -t spooky:latest . docker run -d \\ --name spooky \\ -p 9889:9889/udp \\ -v /path/to/config.yaml:/etc/spooky/config.yaml:ro \\ -v /path/to/certs:/etc/spooky/certs:ro \\ spooky:latest --config /etc/spooky/config.yaml Using Docker Compose: version: '3.8' services: spooky: build: . ports: - \"9889:9889/udp\" volumes: - ./config.yaml:/etc/spooky/config.yaml:ro - ./certs:/etc/spooky/certs:ro command: [\"--config\", \"/etc/spooky/config.yaml\"] restart: unless-stopped","title":"Docker Deployment"},{"location":"getting-started/installation/#installation-verification","text":"# Verify binary is accessible spooky --version # Display help and available options spooky --help # Validate configuration syntax (startup validation happens before serving) spooky --config /path/to/config.yaml Expected output from spooky --version : spooky 0.1.0","title":"Installation Verification"},{"location":"getting-started/installation/#post-installation-configuration","text":"","title":"Post-Installation Configuration"},{"location":"getting-started/installation/#1-create-configuration-directory","text":"sudo mkdir -p /etc/spooky sudo mkdir -p /etc/spooky/certs sudo chown -R $(whoami) /etc/spooky","title":"1. Create Configuration Directory"},{"location":"getting-started/installation/#2-generate-tls-certificates","text":"Self-signed certificates (development): openssl req -x509 -newkey rsa:4096 -nodes \\ -keyout /etc/spooky/certs/key.pem \\ -out /etc/spooky/certs/cert.pem \\ -days 365 \\ -subj \"/CN=proxy.example.com\" For production certificates, see TLS Configuration .","title":"2. Generate TLS Certificates"},{"location":"getting-started/installation/#3-create-base-configuration","text":"Create /etc/spooky/config.yaml : version: 1 listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"/etc/spooky/certs/cert.pem\" key: \"/etc/spooky/certs/key.pem\" upstream: default: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"backend-1\" address: \"127.0.0.1:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 log: level: info","title":"3. Create Base Configuration"},{"location":"getting-started/installation/#4-system-service-setup-linux","text":"Create /etc/systemd/system/spooky.service : [Unit] Description=Spooky HTTP/3 to HTTP/2 Proxy After=network.target [Service] Type=simple User=spooky Group=spooky ExecStart=/usr/local/bin/spooky --config /etc/spooky/config.yaml Restart=on-failure RestartSec=5s # Security hardening NoNewPrivileges=true PrivateTmp=true ProtectSystem=strict ProtectHome=true ReadWritePaths=/var/log/spooky [Install] WantedBy=multi-user.target Create service user and enable: sudo useradd -r -s /bin/false spooky sudo systemctl daemon-reload sudo systemctl enable spooky.service sudo systemctl start spooky.service","title":"4. System Service Setup (Linux)"},{"location":"getting-started/installation/#5-log-management","text":"By default, Spooky logs to stderr (captured by journald under systemd). To write logs to a file instead, set log.file.enabled: true in your config: log: level: info file: enabled: true path: /var/log/spooky/spooky.log Configure log rotation for file-based logging: Create /etc/logrotate.d/spooky : /var/log/spooky/*.log { daily rotate 14 compress delaycompress missingok notifempty create 0640 spooky spooky sharedscripts postrotate systemctl restart spooky.service >/dev/null 2>&1 || true endscript }","title":"5. Log Management"},{"location":"getting-started/installation/#troubleshooting-installation","text":"Build fails with linker errors: - Ensure build tools are installed: cmake , pkg-config , C compiler - Update Rust toolchain: rustup update Permission denied when binding to port: - Use port > 1024, or grant capability: sudo setcap CAP_NET_BIND_SERVICE=+eip /usr/local/bin/spooky - Run as privileged user (not recommended for production) Certificate errors on startup: - Verify certificate and key paths in configuration - Check file permissions: certificates must be readable by the service user - Validate certificate format: openssl x509 -in cert.pem -text -noout Binary not found after cargo install: - Add ~/.cargo/bin to PATH: export PATH=\"$HOME/.cargo/bin:$PATH\" - Add to shell profile for persistence","title":"Troubleshooting Installation"},{"location":"getting-started/installation/#next-steps","text":"Configuration Reference - Complete configuration options TLS Setup Guide - Production certificate management Production Deployment - Production deployment best practices Troubleshooting - Common issues and solutions","title":"Next Steps"},{"location":"getting-started/overview/","text":"Spooky is an HTTP/3 to HTTP/2 reverse proxy and load balancer. It terminates QUIC connections at the edge and forwards requests to HTTP/2 backends, enabling HTTP/3 client support without modifying existing infrastructure. What Spooky Does Spooky bridges the gap between modern HTTP/3 clients and production HTTP/2 backends by: Terminating QUIC connections with TLS 1.3 Converting HTTP/3 streams to HTTP/2 requests Distributing load across backend pools with active health checks Routing requests based on path prefix and hostname patterns Architecture HTTP/3 Client \u2192 QUIC/TLS \u2192 Spooky Edge \u2192 HTTP/2 \u2192 Backend Servers Core Components: Edge : QUIC termination and HTTP/3 session management Bridge : Protocol conversion between HTTP/3 and HTTP/2 Transport : HTTP/2 connection pooling and lifecycle management Load Balancer : Backend selection algorithms and health tracking Router : Path and host-based request routing Key Features Protocol Support - HTTP/3 and QUIC (RFC 9114, RFC 9000) - TLS 1.3 with certificate chain validation - HTTP/2 backend connectivity Load Balancing - Random distribution - Round-robin rotation (default) - Consistent hashing with configurable virtual nodes - Global load balancing strategy (same for all upstreams) Routing - Path prefix matching with longest-match selection - Host-based routing - Multiple upstream pools with independent configurations Health Management - Active HTTP health checks with configurable intervals - Automatic backend removal on failure threshold - Cooldown periods for recovery System Requirements Runtime Requirements: - Rust 1.85 or later (edition 2024) - Linux, macOS, or Windows - UDP port access for QUIC traffic - 256MB RAM minimum (1GB recommended for production) Build Dependencies: # Ubuntu/Debian sudo apt install cmake build-essential pkg-config # macOS brew install cmake pkg-config Quick Start # Clone and build git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release # Generate certificates make certs-selfsigned # Start proxy ./target/release/spooky --config config/config.yaml Configuration Example Spooky uses YAML configuration with validation at startup: version: 1 listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"certs/cert.pem\" key: \"certs/key.pem\" upstream: api_backend: load_balancing: type: \"round-robin\" route: path_prefix: \"/api\" backends: - id: \"api-1\" address: \"127.0.0.1:8001\" weight: 100 health_check: path: \"/health\" interval: 5000 default_backend: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"default-1\" address: \"127.0.0.1:8080\" weight: 100 log: level: info Testing Connectivity Verify the proxy is functioning with an HTTP/3 client: curl --http3-only -k \\ --resolve proxy.example.com:9889:127.0.0.1 \\ https://proxy.example.com:9889/api/health Project Status Spooky is experimental. Core features are implemented and functional, but the project is not production-ready. Expect rough edges, missing features, and breaking changes. Currently working: QUIC termination and HTTP/3 support HTTP/2 backend forwarding with connection pooling Multiple load balancing algorithms Active health checking with automatic recovery Path and host-based routing with upstream pools Next Steps Installation Guide - Complete installation instructions Configuration Reference - Full configuration documentation TLS Setup - Certificate generation and configuration Load Balancing Guide - Backend selection strategies Production Deployment - Production deployment guidelines","title":"Overview"},{"location":"getting-started/overview/#what-spooky-does","text":"Spooky bridges the gap between modern HTTP/3 clients and production HTTP/2 backends by: Terminating QUIC connections with TLS 1.3 Converting HTTP/3 streams to HTTP/2 requests Distributing load across backend pools with active health checks Routing requests based on path prefix and hostname patterns","title":"What Spooky Does"},{"location":"getting-started/overview/#architecture","text":"HTTP/3 Client \u2192 QUIC/TLS \u2192 Spooky Edge \u2192 HTTP/2 \u2192 Backend Servers Core Components: Edge : QUIC termination and HTTP/3 session management Bridge : Protocol conversion between HTTP/3 and HTTP/2 Transport : HTTP/2 connection pooling and lifecycle management Load Balancer : Backend selection algorithms and health tracking Router : Path and host-based request routing","title":"Architecture"},{"location":"getting-started/overview/#key-features","text":"Protocol Support - HTTP/3 and QUIC (RFC 9114, RFC 9000) - TLS 1.3 with certificate chain validation - HTTP/2 backend connectivity Load Balancing - Random distribution - Round-robin rotation (default) - Consistent hashing with configurable virtual nodes - Global load balancing strategy (same for all upstreams) Routing - Path prefix matching with longest-match selection - Host-based routing - Multiple upstream pools with independent configurations Health Management - Active HTTP health checks with configurable intervals - Automatic backend removal on failure threshold - Cooldown periods for recovery","title":"Key Features"},{"location":"getting-started/overview/#system-requirements","text":"Runtime Requirements: - Rust 1.85 or later (edition 2024) - Linux, macOS, or Windows - UDP port access for QUIC traffic - 256MB RAM minimum (1GB recommended for production) Build Dependencies: # Ubuntu/Debian sudo apt install cmake build-essential pkg-config # macOS brew install cmake pkg-config","title":"System Requirements"},{"location":"getting-started/overview/#quick-start","text":"# Clone and build git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release # Generate certificates make certs-selfsigned # Start proxy ./target/release/spooky --config config/config.yaml","title":"Quick Start"},{"location":"getting-started/overview/#configuration-example","text":"Spooky uses YAML configuration with validation at startup: version: 1 listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"certs/cert.pem\" key: \"certs/key.pem\" upstream: api_backend: load_balancing: type: \"round-robin\" route: path_prefix: \"/api\" backends: - id: \"api-1\" address: \"127.0.0.1:8001\" weight: 100 health_check: path: \"/health\" interval: 5000 default_backend: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"default-1\" address: \"127.0.0.1:8080\" weight: 100 log: level: info","title":"Configuration Example"},{"location":"getting-started/overview/#testing-connectivity","text":"Verify the proxy is functioning with an HTTP/3 client: curl --http3-only -k \\ --resolve proxy.example.com:9889:127.0.0.1 \\ https://proxy.example.com:9889/api/health","title":"Testing Connectivity"},{"location":"getting-started/overview/#project-status","text":"Spooky is experimental. Core features are implemented and functional, but the project is not production-ready. Expect rough edges, missing features, and breaking changes. Currently working: QUIC termination and HTTP/3 support HTTP/2 backend forwarding with connection pooling Multiple load balancing algorithms Active health checking with automatic recovery Path and host-based routing with upstream pools","title":"Project Status"},{"location":"getting-started/overview/#next-steps","text":"Installation Guide - Complete installation instructions Configuration Reference - Full configuration documentation TLS Setup - Certificate generation and configuration Load Balancing Guide - Backend selection strategies Production Deployment - Production deployment guidelines","title":"Next Steps"},{"location":"protocols/http3/","text":"HTTP/3 is the third major version of the Hypertext Transfer Protocol, designed to improve performance and reliability by running over QUIC instead of TCP. It represents a fundamental shift in how HTTP traffic is transported while maintaining compatibility with existing HTTP semantics. Evolution of HTTP Transport HTTP/1.1 Limitations HTTP/1.1 uses text-based, whitespace-delimited message framing that is human-readable but computationally expensive to parse. The protocol lacks native multiplexing support, requiring multiple TCP connections to achieve parallelism. This approach degrades congestion control effectiveness and increases connection overhead. Key characteristics: - Plain-text message format with verbose headers - One request per connection without pipelining support - Multiple connections required for concurrent requests - No header compression mechanism HTTP/2 Improvements and Remaining Issues HTTP/2 introduced binary framing and stream multiplexing over a single TCP connection, significantly improving efficiency. However, it inherited TCP's head-of-line blocking problem: packet loss on any stream stalls all multiplexed streams until retransmission occurs. Improvements over HTTP/1.1: - Binary framing layer for efficient parsing - Stream multiplexing with priority signaling - Header compression using HPACK - Server push capability Limitations: - TCP head-of-line blocking affects all streams - Connection establishment requires separate TCP and TLS handshakes - Limited resilience to network path changes QUIC as the Transport Foundation QUIC addresses HTTP/2's limitations by providing stream multiplexing at the transport layer with independent stream reliability. Built on UDP, QUIC integrates TLS 1.3 directly into the protocol, enabling faster connection establishment and 0-RTT resumption. QUIC features leveraged by HTTP/3: - Per-stream flow control and independent loss recovery - Connection-level congestion control - Integrated TLS 1.3 with single handshake - Connection migration support via connection identifiers - Reduced latency with 0-RTT data transmission HTTP/3 maps HTTP semantics onto QUIC streams, inheriting these transport-layer improvements while maintaining HTTP/2's design principles. HTTP/3 Protocol Architecture Transport Layer HTTP/3 operates exclusively over QUIC (RFC 9000), utilizing QUIC's stream multiplexing and flow control mechanisms. Each HTTP request/response pair maps to a single bidirectional QUIC stream, providing stream-level isolation and independent delivery guarantees. Frame Structure HTTP/3 communication uses typed frames transmitted within QUIC streams. Common frame types include: HEADERS : Carries HTTP header fields (compressed using QPACK) DATA : Conveys request or response payload SETTINGS : Communicates connection-level parameters PUSH_PROMISE : Initiates server push GOAWAY : Initiates graceful connection shutdown Frame types are variable-length encoded, with the frame type and length fields preceding the payload. Stream Types HTTP/3 defines three stream types: Request streams (client-initiated bidirectional): Carry HTTP request/response exchanges Control stream (unidirectional): Manages connection-wide settings and parameters Push streams (server-initiated unidirectional): Deliver server-pushed responses Request/Response Model Each HTTP request/response cycle consumes one client-initiated bidirectional QUIC stream. The client sends HEADERS and optional DATA frames, then half-closes the stream. The server responds with HEADERS and DATA frames before closing its sending side. Stream independence ensures that packet loss on one stream does not block others, eliminating TCP-layer head-of-line blocking. Server Push HTTP/3 supports server push through the PUSH_PROMISE frame, which reserves a push stream identifier. The server then sends the pushed response on the corresponding push stream. Clients can limit or disable push using MAX_PUSH_ID and CANCEL_PUSH frames. Header Compression with QPACK HTTP/3 replaces HTTP/2's HPACK compression with QPACK (RFC 9204), designed to handle QUIC's unordered stream delivery. QPACK maintains compression efficiency while allowing headers to be decoded independently when streams arrive out of order. QPACK features: - Dynamic table updates on a dedicated unidirectional stream - Reference tracking to prevent decoding dependencies - Configurable risk/compression tradeoff - Backward compatibility with HPACK concepts Header Format Examples HTTP/1.1 Header Format GET /index.html HTTP/1.1 Host: example.com User-Agent: Mozilla/5.0 Accept: text/html,application/xhtml+xml Accept-Encoding: gzip, deflate Connection: keep-alive HTTP/1.1 uses plain-text headers with CRLF delimiters. Each header field is a name-value pair separated by a colon. HTTP/2 Header Format HEADERS Frame (Stream 1): :method = GET :path = /index.html :scheme = https :authority = example.com user-agent = Mozilla/5.0 accept = text/html,application/xhtml+xml accept-encoding = gzip, deflate HTTP/2 introduces pseudo-header fields (prefixed with : ) for request metadata. Headers are encoded using HPACK and transmitted in a HEADERS frame. HTTP/3 Header Format HEADERS Frame (Stream 0): :method = GET :path = /index.html :scheme = https :authority = example.com user-agent = Mozilla/5.0 accept = text/html,application/xhtml+xml accept-encoding = gzip, deflate HTTP/3 maintains HTTP/2's pseudo-header syntax but uses QPACK encoding instead of HPACK. The semantic meaning and structure are identical to HTTP/2. HTTP/2 vs HTTP/3 Comparison Feature HTTP/2 HTTP/3 Transport Protocol TCP QUIC (over UDP) Connection Establishment Separate TCP and TLS handshakes (2-3 RTT) Combined QUIC+TLS handshake (1-RTT) Header Compression HPACK QPACK Stream Multiplexing Application layer (within TCP connection) Transport layer (native QUIC streams) Head-of-Line Blocking TCP-layer blocking affects all streams Per-stream delivery (no cross-stream HoL) Connection Migration Not supported (TCP 4-tuple bound) Supported via connection IDs 0-RTT Support Limited (TLS 1.3 early data with constraints) Native 0-RTT with replay protection Header Field Syntax Pseudo-headers ( :method , :path , :scheme ) Same as HTTP/2 Server Push PUSH_PROMISE frame PUSH_PROMISE frame (same semantics) Flow Control Stream and connection level Stream and connection level HTTP/3 Implementation in Spooky Spooky implements HTTP/3 termination at the edge, accepting HTTP/3 client connections and translating them to HTTP/2 for backend forwarding. This approach enables HTTP/3 client support without requiring backend infrastructure changes. Protocol Conversion The Spooky bridge component handles bidirectional translation between HTTP/3 and HTTP/2: Request path : HTTP/3 HEADERS and DATA frames are parsed, decompressed via QPACK, and re-encoded as HTTP/2 frames with HPACK compression for backend transmission. Response path : HTTP/2 responses from backends are decoded, headers are converted to QPACK format, and frames are transmitted over the client's HTTP/3 stream. Stream Mapping Each HTTP/3 client stream maps to a corresponding HTTP/2 stream on a backend connection. Stream lifecycle events (creation, data transfer, closure) are synchronized between protocols. Connection Management Spooky maintains separate connection pools for client-facing HTTP/3 sessions and backend HTTP/2 connections. QUIC connection state is managed by the edge component, while HTTP/2 connection pooling is handled by the transport layer. Feature Support Header compression : Full QPACK support for clients, HPACK for backends Flow control : QUIC stream and connection flow control enforced Server push : Not currently implemented (client-to-proxy only) Priority signaling : HTTP/3 priority frames are mapped to HTTP/2 priority when supported by backends Performance Characteristics HTTP/3 offers several performance advantages over HTTP/2: Reduced latency : Combined QUIC+TLS handshake reduces connection establishment time. 0-RTT resumption enables immediate data transmission for repeat connections. Improved loss resilience : Independent stream delivery prevents packet loss on one stream from blocking others. TCP's retransmission delays do not propagate across streams. Better mobile performance : Connection migration allows QUIC connections to survive network changes (Wi-Fi to cellular transitions) without interruption. Optimal congestion control : QUIC's congestion control operates at the connection level without TCP's limitations, enabling more accurate RTT estimation and loss detection. References RFC 9114: HTTP/3 RFC 9000: QUIC Transport Protocol RFC 9204: QPACK Header Compression RFC 9218: Extensible Prioritization Scheme for HTTP","title":"HTTP/3"},{"location":"protocols/http3/#evolution-of-http-transport","text":"","title":"Evolution of HTTP Transport"},{"location":"protocols/http3/#http11-limitations","text":"HTTP/1.1 uses text-based, whitespace-delimited message framing that is human-readable but computationally expensive to parse. The protocol lacks native multiplexing support, requiring multiple TCP connections to achieve parallelism. This approach degrades congestion control effectiveness and increases connection overhead. Key characteristics: - Plain-text message format with verbose headers - One request per connection without pipelining support - Multiple connections required for concurrent requests - No header compression mechanism","title":"HTTP/1.1 Limitations"},{"location":"protocols/http3/#http2-improvements-and-remaining-issues","text":"HTTP/2 introduced binary framing and stream multiplexing over a single TCP connection, significantly improving efficiency. However, it inherited TCP's head-of-line blocking problem: packet loss on any stream stalls all multiplexed streams until retransmission occurs. Improvements over HTTP/1.1: - Binary framing layer for efficient parsing - Stream multiplexing with priority signaling - Header compression using HPACK - Server push capability Limitations: - TCP head-of-line blocking affects all streams - Connection establishment requires separate TCP and TLS handshakes - Limited resilience to network path changes","title":"HTTP/2 Improvements and Remaining Issues"},{"location":"protocols/http3/#quic-as-the-transport-foundation","text":"QUIC addresses HTTP/2's limitations by providing stream multiplexing at the transport layer with independent stream reliability. Built on UDP, QUIC integrates TLS 1.3 directly into the protocol, enabling faster connection establishment and 0-RTT resumption. QUIC features leveraged by HTTP/3: - Per-stream flow control and independent loss recovery - Connection-level congestion control - Integrated TLS 1.3 with single handshake - Connection migration support via connection identifiers - Reduced latency with 0-RTT data transmission HTTP/3 maps HTTP semantics onto QUIC streams, inheriting these transport-layer improvements while maintaining HTTP/2's design principles.","title":"QUIC as the Transport Foundation"},{"location":"protocols/http3/#http3-protocol-architecture","text":"","title":"HTTP/3 Protocol Architecture"},{"location":"protocols/http3/#transport-layer","text":"HTTP/3 operates exclusively over QUIC (RFC 9000), utilizing QUIC's stream multiplexing and flow control mechanisms. Each HTTP request/response pair maps to a single bidirectional QUIC stream, providing stream-level isolation and independent delivery guarantees.","title":"Transport Layer"},{"location":"protocols/http3/#frame-structure","text":"HTTP/3 communication uses typed frames transmitted within QUIC streams. Common frame types include: HEADERS : Carries HTTP header fields (compressed using QPACK) DATA : Conveys request or response payload SETTINGS : Communicates connection-level parameters PUSH_PROMISE : Initiates server push GOAWAY : Initiates graceful connection shutdown Frame types are variable-length encoded, with the frame type and length fields preceding the payload.","title":"Frame Structure"},{"location":"protocols/http3/#stream-types","text":"HTTP/3 defines three stream types: Request streams (client-initiated bidirectional): Carry HTTP request/response exchanges Control stream (unidirectional): Manages connection-wide settings and parameters Push streams (server-initiated unidirectional): Deliver server-pushed responses","title":"Stream Types"},{"location":"protocols/http3/#requestresponse-model","text":"Each HTTP request/response cycle consumes one client-initiated bidirectional QUIC stream. The client sends HEADERS and optional DATA frames, then half-closes the stream. The server responds with HEADERS and DATA frames before closing its sending side. Stream independence ensures that packet loss on one stream does not block others, eliminating TCP-layer head-of-line blocking.","title":"Request/Response Model"},{"location":"protocols/http3/#server-push","text":"HTTP/3 supports server push through the PUSH_PROMISE frame, which reserves a push stream identifier. The server then sends the pushed response on the corresponding push stream. Clients can limit or disable push using MAX_PUSH_ID and CANCEL_PUSH frames.","title":"Server Push"},{"location":"protocols/http3/#header-compression-with-qpack","text":"HTTP/3 replaces HTTP/2's HPACK compression with QPACK (RFC 9204), designed to handle QUIC's unordered stream delivery. QPACK maintains compression efficiency while allowing headers to be decoded independently when streams arrive out of order. QPACK features: - Dynamic table updates on a dedicated unidirectional stream - Reference tracking to prevent decoding dependencies - Configurable risk/compression tradeoff - Backward compatibility with HPACK concepts","title":"Header Compression with QPACK"},{"location":"protocols/http3/#header-format-examples","text":"","title":"Header Format Examples"},{"location":"protocols/http3/#http11-header-format","text":"GET /index.html HTTP/1.1 Host: example.com User-Agent: Mozilla/5.0 Accept: text/html,application/xhtml+xml Accept-Encoding: gzip, deflate Connection: keep-alive HTTP/1.1 uses plain-text headers with CRLF delimiters. Each header field is a name-value pair separated by a colon.","title":"HTTP/1.1 Header Format"},{"location":"protocols/http3/#http2-header-format","text":"HEADERS Frame (Stream 1): :method = GET :path = /index.html :scheme = https :authority = example.com user-agent = Mozilla/5.0 accept = text/html,application/xhtml+xml accept-encoding = gzip, deflate HTTP/2 introduces pseudo-header fields (prefixed with : ) for request metadata. Headers are encoded using HPACK and transmitted in a HEADERS frame.","title":"HTTP/2 Header Format"},{"location":"protocols/http3/#http3-header-format","text":"HEADERS Frame (Stream 0): :method = GET :path = /index.html :scheme = https :authority = example.com user-agent = Mozilla/5.0 accept = text/html,application/xhtml+xml accept-encoding = gzip, deflate HTTP/3 maintains HTTP/2's pseudo-header syntax but uses QPACK encoding instead of HPACK. The semantic meaning and structure are identical to HTTP/2.","title":"HTTP/3 Header Format"},{"location":"protocols/http3/#http2-vs-http3-comparison","text":"Feature HTTP/2 HTTP/3 Transport Protocol TCP QUIC (over UDP) Connection Establishment Separate TCP and TLS handshakes (2-3 RTT) Combined QUIC+TLS handshake (1-RTT) Header Compression HPACK QPACK Stream Multiplexing Application layer (within TCP connection) Transport layer (native QUIC streams) Head-of-Line Blocking TCP-layer blocking affects all streams Per-stream delivery (no cross-stream HoL) Connection Migration Not supported (TCP 4-tuple bound) Supported via connection IDs 0-RTT Support Limited (TLS 1.3 early data with constraints) Native 0-RTT with replay protection Header Field Syntax Pseudo-headers ( :method , :path , :scheme ) Same as HTTP/2 Server Push PUSH_PROMISE frame PUSH_PROMISE frame (same semantics) Flow Control Stream and connection level Stream and connection level","title":"HTTP/2 vs HTTP/3 Comparison"},{"location":"protocols/http3/#http3-implementation-in-spooky","text":"Spooky implements HTTP/3 termination at the edge, accepting HTTP/3 client connections and translating them to HTTP/2 for backend forwarding. This approach enables HTTP/3 client support without requiring backend infrastructure changes.","title":"HTTP/3 Implementation in Spooky"},{"location":"protocols/http3/#protocol-conversion","text":"The Spooky bridge component handles bidirectional translation between HTTP/3 and HTTP/2: Request path : HTTP/3 HEADERS and DATA frames are parsed, decompressed via QPACK, and re-encoded as HTTP/2 frames with HPACK compression for backend transmission. Response path : HTTP/2 responses from backends are decoded, headers are converted to QPACK format, and frames are transmitted over the client's HTTP/3 stream.","title":"Protocol Conversion"},{"location":"protocols/http3/#stream-mapping","text":"Each HTTP/3 client stream maps to a corresponding HTTP/2 stream on a backend connection. Stream lifecycle events (creation, data transfer, closure) are synchronized between protocols.","title":"Stream Mapping"},{"location":"protocols/http3/#connection-management","text":"Spooky maintains separate connection pools for client-facing HTTP/3 sessions and backend HTTP/2 connections. QUIC connection state is managed by the edge component, while HTTP/2 connection pooling is handled by the transport layer.","title":"Connection Management"},{"location":"protocols/http3/#feature-support","text":"Header compression : Full QPACK support for clients, HPACK for backends Flow control : QUIC stream and connection flow control enforced Server push : Not currently implemented (client-to-proxy only) Priority signaling : HTTP/3 priority frames are mapped to HTTP/2 priority when supported by backends","title":"Feature Support"},{"location":"protocols/http3/#performance-characteristics","text":"HTTP/3 offers several performance advantages over HTTP/2: Reduced latency : Combined QUIC+TLS handshake reduces connection establishment time. 0-RTT resumption enables immediate data transmission for repeat connections. Improved loss resilience : Independent stream delivery prevents packet loss on one stream from blocking others. TCP's retransmission delays do not propagate across streams. Better mobile performance : Connection migration allows QUIC connections to survive network changes (Wi-Fi to cellular transitions) without interruption. Optimal congestion control : QUIC's congestion control operates at the connection level without TCP's limitations, enabling more accurate RTT estimation and loss detection.","title":"Performance Characteristics"},{"location":"protocols/http3/#references","text":"RFC 9114: HTTP/3 RFC 9000: QUIC Transport Protocol RFC 9204: QPACK Header Compression RFC 9218: Extensible Prioritization Scheme for HTTP","title":"References"},{"location":"protocols/quic/","text":"QUIC (Quick UDP Internet Connections) is a multiplexed, secure transport protocol designed to address the performance limitations of TCP while maintaining reliability guarantees. Built on UDP, QUIC integrates transport and cryptographic handshakes, provides native stream multiplexing, and supports connection migration across network changes. Note : QUIC is a proper name, not an acronym, despite its origin as \"Quick UDP Internet Connections.\" Protocol Fundamentals Design Goals QUIC was designed to overcome specific limitations in TCP-based protocols: Eliminate head-of-line blocking : TCP's byte-stream abstraction causes packet loss to block all multiplexed application streams. QUIC provides independent stream delivery. Reduce connection establishment latency : TCP and TLS negotiate separately, requiring multiple round trips. QUIC combines transport and cryptographic handshakes into a single exchange. Enable connection migration : TCP connections are bound to a 4-tuple (source IP, source port, destination IP, destination port). Network changes break connections. QUIC uses connection identifiers to survive NAT rebinding and network transitions. Modernize congestion control : QUIC's design allows rapid deployment of improved congestion control algorithms without requiring operating system updates. Provide encrypted transport by default : All QUIC payload data is encrypted using TLS 1.3, with only minimal connection metadata exposed. Transport Architecture QUIC operates as a connection-oriented protocol over UDP datagrams. Each UDP packet carries a QUIC packet containing one or more frames. Frames represent protocol operations: stream data transfer, acknowledgments, flow control updates, and connection management. Key architectural components: Connections : Stateful communication contexts identified by connection IDs Packets : UDP datagram payloads containing encrypted frames Frames : Typed protocol messages (stream data, control signals, etc.) Streams : Ordered byte-stream channels multiplexed within a connection Connection Lifecycle Connection Establishment QUIC connections are established through a handshake that combines transport-layer and cryptographic negotiation. The process involves: 1-RTT Handshake (First Connection): Client Server | | |-- Initial[CRYPTO, PADDING] -------------->| | (ClientHello, Transport Params) | | | |<-- Initial[CRYPTO, ACK] -------------------| |<-- Handshake[CRYPTO] ----------------------| | (ServerHello, EncryptedExtensions, | | Certificate, CertificateVerify, | | Finished, Transport Params) | | | |-- Handshake[CRYPTO, ACK] ---------------->| | (Finished) | | | |<== Application Data ======================| |==> Application Data ======================| After the handshake completes, both endpoints can send application data. The entire process requires one round trip. 0-RTT Handshake (Resumption): For resumed connections, clients can send application data in the first flight alongside the ClientHello, reducing latency to zero round trips at the cost of replay protection: Client Server | | |-- Initial[CRYPTO] + 0-RTT[Stream Data] -->| | (ClientHello, Application Data) | | | |<-- Initial[CRYPTO] + Handshake[CRYPTO] ----| | (ServerHello, Finished) | | | |==> 1-RTT[Stream Data] =====================| |<== 1-RTT[Stream Data] =====================| 0-RTT data must be idempotent and replay-safe, as attackers can capture and replay the initial packet. Transport Parameters During connection establishment, endpoints exchange transport parameters that define connection behavior: initial_max_data : Connection-level flow control limit initial_max_stream_data_bidi_local/remote : Per-stream flow control limits initial_max_streams_bidi/uni : Maximum concurrent streams allowed max_idle_timeout : Connection idle timeout duration max_udp_payload_size : Maximum UDP payload the endpoint can receive active_connection_id_limit : Number of connection IDs the peer can provide These parameters are immutable for the connection lifetime and must not change during resumption. Connection Migration QUIC connections are identified by connection IDs rather than network 4-tuples, enabling migration across network paths. Use cases include: NAT rebinding : UDP port changes due to NAT timeout Network transitions : Mobile device switching from Wi-Fi to cellular Load balancing : Server-initiated migration for capacity management Migration process: Client sends a packet from a new network path with a new source address Server validates the new path using a PATH_CHALLENGE frame Client responds with PATH_RESPONSE to prove path ownership Server begins sending packets to the new address Connection migration is client-initiated by default. Servers can only migrate by providing new connection IDs for clients to use. Connection Termination QUIC supports three termination mechanisms: 1. Graceful Shutdown (CONNECTION_CLOSE): Either endpoint can send a CONNECTION_CLOSE frame to cleanly terminate the connection. The frame includes an error code and reason phrase. The peer acknowledges termination and both sides enter the closing state. 2. Idle Timeout: If no packets are exchanged for the negotiated max_idle_timeout duration, the connection is silently closed without sending frames. This prevents resource exhaustion from abandoned connections. 3. Stateless Reset: If an endpoint loses connection state (due to restart or migration), it cannot process incoming packets. The endpoint sends a stateless reset token to force the peer to abandon the connection immediately without further handshake. Stream Multiplexing Stream Types QUIC provides four stream types based on directionality and initiator: Type Initiated By Direction Use Case Bidirectional (client) Client Both HTTP request/response Bidirectional (server) Server Both Server-initiated interactions Unidirectional (client) Client Client\u2192Server Control streams, telemetry Unidirectional (server) Server Server\u2192Client Server push, control streams Stream IDs encode the type and initiator in the two least-significant bits: 0x00, 0x04, 0x08... : Client-initiated bidirectional 0x01, 0x05, 0x09... : Server-initiated bidirectional 0x02, 0x06, 0x0A... : Client-initiated unidirectional 0x03, 0x07, 0x0B... : Server-initiated unidirectional Stream Lifecycle Streams are created implicitly by sending a frame with the corresponding stream ID. No explicit open signal is required. State transitions: Idle : Stream ID has not been used Open : Data is being transmitted Half-closed (local) : Local endpoint finished sending (sent FIN bit) Half-closed (remote) : Remote endpoint finished sending (received FIN bit) Closed : Both sides finished sending Streams can be terminated early using RESET_STREAM (sending direction) and STOP_SENDING (receiving direction) frames. Flow Control QUIC implements credit-based flow control at two levels: Stream-level flow control: Each stream has an independent flow control window. The receiver advertises the maximum byte offset it is willing to receive via MAX_STREAM_DATA frames. The sender must not exceed this limit. Connection-level flow control: The total bytes across all streams are subject to connection-level limits advertised via MAX_DATA frames. This prevents a single stream from consuming all connection resources. Flow control updates are sent asynchronously as data is consumed, allowing the sender to continue transmission without blocking. Reliability and Loss Recovery Reliable Delivery QUIC guarantees ordered, reliable delivery within each stream. Frames are assigned packet numbers (monotonically increasing per packet). The receiver acknowledges received packets using ACK frames, which include: Largest acknowledged packet number ACK delay (time between packet receipt and ACK generation) ACK ranges (compressed representation of received packets) Loss Detection QUIC uses acknowledgment-based loss detection. Packets are considered lost if: Reordering threshold : Three or more packets sent later have been acknowledged Time threshold : Sufficient time has elapsed since transmission without acknowledgment (typically 9/8 * smoothed RTT) Declared lost packets trigger retransmission of their frames. Retransmission Lost frames are retransmitted in new packets with new packet numbers. QUIC does not retransmit packets verbatim; only the frames within them are resent. This decoupling allows acknowledgments to unambiguously identify which transmission was received. Congestion Control QUIC mandates congestion control but does not prescribe a specific algorithm. Implementations commonly use: NewReno : TCP NewReno adapted for QUIC (RFC 9002 default) CUBIC : More aggressive window growth for high-bandwidth paths BBR : Bottleneck bandwidth and round-trip propagation time estimation Congestion control operates per connection, not per stream. All streams share the congestion window and pacing budget. Packet Structure QUIC packets consist of a header and payload. Two header types exist: Long Header (Handshake Packets) Used during connection establishment. Contains: Version field (4 bytes) Destination Connection ID Length + Destination Connection ID Source Connection ID Length + Source Connection ID Type-specific fields (token, length) Packet Number Encrypted payload Long header packets have types: Initial, 0-RTT, Handshake, and Retry. Short Header (1-RTT Packets) Used after handshake completion. Contains: Spin bit (latency measurement) Destination Connection ID Packet Number Encrypted payload Short headers minimize overhead for long-lived connections. Frame Types Common frame types: PADDING : Increases packet size for PMTU probing PING : Elicits acknowledgment without sending data ACK : Acknowledges received packets STREAM : Carries stream data with offset and length MAX_DATA / MAX_STREAM_DATA : Advertises flow control updates RESET_STREAM / STOP_SENDING : Aborts stream transmission CONNECTION_CLOSE : Terminates the connection PATH_CHALLENGE / PATH_RESPONSE : Validates network path during migration Security Properties Encryption QUIC uses TLS 1.3 for key derivation and encryption. Packet payloads are encrypted using AEAD algorithms (typically AES-128-GCM or ChaCha20-Poly1305). Packet headers are protected to prevent ossification by middleboxes. Encryption levels: Initial : Derived from well-known salt and client-chosen Destination Connection ID 0-RTT : Derived from resumed session keys Handshake : Derived from ephemeral handshake keys 1-RTT : Derived from final application traffic keys Header Protection Packet numbers are encrypted to prevent middleboxes from tracking flows and inferring loss patterns. A sample from the encrypted payload is used to mask the packet number and flags. Replay Protection 1-RTT data is protected by TLS's anti-replay mechanisms. 0-RTT data is inherently replayable and must be used only for idempotent operations. Servers can optionally reject 0-RTT to enforce strict replay protection. QUIC Implementation in Spooky Spooky uses the quiche QUIC library for connection management and HTTP/3 framing. The implementation handles: Connection Management Endpoint creation : Spooky binds to the configured UDP port and initializes the QUIC endpoint with TLS configuration Connection acceptance : Incoming QUIC connections are validated against TLS certificate requirements Idle timeout : Connections are closed after the configured idle period to reclaim resources Graceful shutdown : Spooky sends CONNECTION_CLOSE frames during shutdown to inform clients TLS Integration TLS 1.3 is mandatory for QUIC. Spooky loads certificate chains and private keys at startup: let tls_config = rustls::ServerConfig::builder() .with_safe_defaults() .with_no_client_auth() .with_single_cert(certs, private_key)?; ALPN (Application-Layer Protocol Negotiation) is configured to advertise \"h3\" for HTTP/3: tls_config.alpn_protocols = vec![b\"h3\".to_vec()]; Stream Handling HTTP/3 streams are mapped to QUIC streams: Request streams : Client-initiated bidirectional streams carry HTTP requests and responses Control stream : Unidirectional stream for HTTP/3 settings and control frames QPACK streams : Separate unidirectional streams for QPACK encoder/decoder communication Spooky's edge component accepts incoming streams and passes them to the bridge for HTTP/3 processing. Flow Control Spooky configures QUIC transport parameters to balance memory usage and throughput: Connection-level flow control prevents excessive buffering across all streams Stream-level flow control limits individual stream memory consumption Defaults are tuned for typical web traffic patterns but can be adjusted for specific workloads Congestion Control Spooky uses the default NewReno congestion control provided by quiche . This choice balances fairness with TCP flows and predictable behavior across network conditions. Performance Considerations UDP Socket Optimization QUIC performance depends on efficient UDP socket handling: Receive buffer size : Increase SO_RCVBUF to reduce packet loss during bursts Send buffer size : Increase SO_SNDBUF to support large congestion windows GRO/GSO : Generic Receive/Send Offload reduces per-packet processing overhead on Linux Packet Pacing Sending packets in bursts can trigger packet loss due to buffer overflow. QUIC implementations pace packet transmission based on the congestion window and RTT to smooth traffic. CPU Usage QUIC's cryptographic operations (AEAD encryption/decryption, header protection) consume CPU. Performance tuning: Use hardware-accelerated cryptographic instructions (AES-NI) Batch packet processing to amortize per-packet overhead Offload TLS operations to dedicated threads if necessary Memory Footprint Each QUIC connection maintains state for: Send and receive buffers for each stream Loss recovery metadata (sent packet information) Congestion control state (RTT estimates, congestion window) Spooky limits concurrent connections and streams to bound memory usage. Comparison with TCP Feature TCP QUIC Transport Layer Kernel-space implementation User-space implementation Connection Establishment 3-way handshake + TLS (2-3 RTT) Combined handshake (1-RTT or 0-RTT) Stream Multiplexing Not supported (requires app layer) Native transport-level multiplexing Head-of-Line Blocking All streams blocked by packet loss Independent per-stream delivery Connection Migration Not supported Supported via connection IDs Congestion Control OS-dependent, slow to update User-space, rapid algorithm evolution Encryption Optional (TLS on top) Mandatory (integrated TLS 1.3) NAT Traversal Requires keep-alive mechanisms Built-in connection migration support Deployment Requires OS updates Application-level deployment References RFC 9000: QUIC: A UDP-Based Multiplexed and Secure Transport RFC 9001: Using TLS to Secure QUIC RFC 9002: QUIC Loss Detection and Congestion Control RFC 8999: Version-Independent Properties of QUIC RFC 9221: An Unreliable Datagram Extension to QUIC RFC 9287: Greasing the QUIC Bit","title":"QUIC"},{"location":"protocols/quic/#protocol-fundamentals","text":"","title":"Protocol Fundamentals"},{"location":"protocols/quic/#design-goals","text":"QUIC was designed to overcome specific limitations in TCP-based protocols: Eliminate head-of-line blocking : TCP's byte-stream abstraction causes packet loss to block all multiplexed application streams. QUIC provides independent stream delivery. Reduce connection establishment latency : TCP and TLS negotiate separately, requiring multiple round trips. QUIC combines transport and cryptographic handshakes into a single exchange. Enable connection migration : TCP connections are bound to a 4-tuple (source IP, source port, destination IP, destination port). Network changes break connections. QUIC uses connection identifiers to survive NAT rebinding and network transitions. Modernize congestion control : QUIC's design allows rapid deployment of improved congestion control algorithms without requiring operating system updates. Provide encrypted transport by default : All QUIC payload data is encrypted using TLS 1.3, with only minimal connection metadata exposed.","title":"Design Goals"},{"location":"protocols/quic/#transport-architecture","text":"QUIC operates as a connection-oriented protocol over UDP datagrams. Each UDP packet carries a QUIC packet containing one or more frames. Frames represent protocol operations: stream data transfer, acknowledgments, flow control updates, and connection management. Key architectural components: Connections : Stateful communication contexts identified by connection IDs Packets : UDP datagram payloads containing encrypted frames Frames : Typed protocol messages (stream data, control signals, etc.) Streams : Ordered byte-stream channels multiplexed within a connection","title":"Transport Architecture"},{"location":"protocols/quic/#connection-lifecycle","text":"","title":"Connection Lifecycle"},{"location":"protocols/quic/#connection-establishment","text":"QUIC connections are established through a handshake that combines transport-layer and cryptographic negotiation. The process involves: 1-RTT Handshake (First Connection): Client Server | | |-- Initial[CRYPTO, PADDING] -------------->| | (ClientHello, Transport Params) | | | |<-- Initial[CRYPTO, ACK] -------------------| |<-- Handshake[CRYPTO] ----------------------| | (ServerHello, EncryptedExtensions, | | Certificate, CertificateVerify, | | Finished, Transport Params) | | | |-- Handshake[CRYPTO, ACK] ---------------->| | (Finished) | | | |<== Application Data ======================| |==> Application Data ======================| After the handshake completes, both endpoints can send application data. The entire process requires one round trip. 0-RTT Handshake (Resumption): For resumed connections, clients can send application data in the first flight alongside the ClientHello, reducing latency to zero round trips at the cost of replay protection: Client Server | | |-- Initial[CRYPTO] + 0-RTT[Stream Data] -->| | (ClientHello, Application Data) | | | |<-- Initial[CRYPTO] + Handshake[CRYPTO] ----| | (ServerHello, Finished) | | | |==> 1-RTT[Stream Data] =====================| |<== 1-RTT[Stream Data] =====================| 0-RTT data must be idempotent and replay-safe, as attackers can capture and replay the initial packet.","title":"Connection Establishment"},{"location":"protocols/quic/#transport-parameters","text":"During connection establishment, endpoints exchange transport parameters that define connection behavior: initial_max_data : Connection-level flow control limit initial_max_stream_data_bidi_local/remote : Per-stream flow control limits initial_max_streams_bidi/uni : Maximum concurrent streams allowed max_idle_timeout : Connection idle timeout duration max_udp_payload_size : Maximum UDP payload the endpoint can receive active_connection_id_limit : Number of connection IDs the peer can provide These parameters are immutable for the connection lifetime and must not change during resumption.","title":"Transport Parameters"},{"location":"protocols/quic/#connection-migration","text":"QUIC connections are identified by connection IDs rather than network 4-tuples, enabling migration across network paths. Use cases include: NAT rebinding : UDP port changes due to NAT timeout Network transitions : Mobile device switching from Wi-Fi to cellular Load balancing : Server-initiated migration for capacity management Migration process: Client sends a packet from a new network path with a new source address Server validates the new path using a PATH_CHALLENGE frame Client responds with PATH_RESPONSE to prove path ownership Server begins sending packets to the new address Connection migration is client-initiated by default. Servers can only migrate by providing new connection IDs for clients to use.","title":"Connection Migration"},{"location":"protocols/quic/#connection-termination","text":"QUIC supports three termination mechanisms: 1. Graceful Shutdown (CONNECTION_CLOSE): Either endpoint can send a CONNECTION_CLOSE frame to cleanly terminate the connection. The frame includes an error code and reason phrase. The peer acknowledges termination and both sides enter the closing state. 2. Idle Timeout: If no packets are exchanged for the negotiated max_idle_timeout duration, the connection is silently closed without sending frames. This prevents resource exhaustion from abandoned connections. 3. Stateless Reset: If an endpoint loses connection state (due to restart or migration), it cannot process incoming packets. The endpoint sends a stateless reset token to force the peer to abandon the connection immediately without further handshake.","title":"Connection Termination"},{"location":"protocols/quic/#stream-multiplexing","text":"","title":"Stream Multiplexing"},{"location":"protocols/quic/#stream-types","text":"QUIC provides four stream types based on directionality and initiator: Type Initiated By Direction Use Case Bidirectional (client) Client Both HTTP request/response Bidirectional (server) Server Both Server-initiated interactions Unidirectional (client) Client Client\u2192Server Control streams, telemetry Unidirectional (server) Server Server\u2192Client Server push, control streams Stream IDs encode the type and initiator in the two least-significant bits: 0x00, 0x04, 0x08... : Client-initiated bidirectional 0x01, 0x05, 0x09... : Server-initiated bidirectional 0x02, 0x06, 0x0A... : Client-initiated unidirectional 0x03, 0x07, 0x0B... : Server-initiated unidirectional","title":"Stream Types"},{"location":"protocols/quic/#stream-lifecycle","text":"Streams are created implicitly by sending a frame with the corresponding stream ID. No explicit open signal is required. State transitions: Idle : Stream ID has not been used Open : Data is being transmitted Half-closed (local) : Local endpoint finished sending (sent FIN bit) Half-closed (remote) : Remote endpoint finished sending (received FIN bit) Closed : Both sides finished sending Streams can be terminated early using RESET_STREAM (sending direction) and STOP_SENDING (receiving direction) frames.","title":"Stream Lifecycle"},{"location":"protocols/quic/#flow-control","text":"QUIC implements credit-based flow control at two levels: Stream-level flow control: Each stream has an independent flow control window. The receiver advertises the maximum byte offset it is willing to receive via MAX_STREAM_DATA frames. The sender must not exceed this limit. Connection-level flow control: The total bytes across all streams are subject to connection-level limits advertised via MAX_DATA frames. This prevents a single stream from consuming all connection resources. Flow control updates are sent asynchronously as data is consumed, allowing the sender to continue transmission without blocking.","title":"Flow Control"},{"location":"protocols/quic/#reliability-and-loss-recovery","text":"","title":"Reliability and Loss Recovery"},{"location":"protocols/quic/#reliable-delivery","text":"QUIC guarantees ordered, reliable delivery within each stream. Frames are assigned packet numbers (monotonically increasing per packet). The receiver acknowledges received packets using ACK frames, which include: Largest acknowledged packet number ACK delay (time between packet receipt and ACK generation) ACK ranges (compressed representation of received packets)","title":"Reliable Delivery"},{"location":"protocols/quic/#loss-detection","text":"QUIC uses acknowledgment-based loss detection. Packets are considered lost if: Reordering threshold : Three or more packets sent later have been acknowledged Time threshold : Sufficient time has elapsed since transmission without acknowledgment (typically 9/8 * smoothed RTT) Declared lost packets trigger retransmission of their frames.","title":"Loss Detection"},{"location":"protocols/quic/#retransmission","text":"Lost frames are retransmitted in new packets with new packet numbers. QUIC does not retransmit packets verbatim; only the frames within them are resent. This decoupling allows acknowledgments to unambiguously identify which transmission was received.","title":"Retransmission"},{"location":"protocols/quic/#congestion-control","text":"QUIC mandates congestion control but does not prescribe a specific algorithm. Implementations commonly use: NewReno : TCP NewReno adapted for QUIC (RFC 9002 default) CUBIC : More aggressive window growth for high-bandwidth paths BBR : Bottleneck bandwidth and round-trip propagation time estimation Congestion control operates per connection, not per stream. All streams share the congestion window and pacing budget.","title":"Congestion Control"},{"location":"protocols/quic/#packet-structure","text":"QUIC packets consist of a header and payload. Two header types exist:","title":"Packet Structure"},{"location":"protocols/quic/#long-header-handshake-packets","text":"Used during connection establishment. Contains: Version field (4 bytes) Destination Connection ID Length + Destination Connection ID Source Connection ID Length + Source Connection ID Type-specific fields (token, length) Packet Number Encrypted payload Long header packets have types: Initial, 0-RTT, Handshake, and Retry.","title":"Long Header (Handshake Packets)"},{"location":"protocols/quic/#short-header-1-rtt-packets","text":"Used after handshake completion. Contains: Spin bit (latency measurement) Destination Connection ID Packet Number Encrypted payload Short headers minimize overhead for long-lived connections.","title":"Short Header (1-RTT Packets)"},{"location":"protocols/quic/#frame-types","text":"Common frame types: PADDING : Increases packet size for PMTU probing PING : Elicits acknowledgment without sending data ACK : Acknowledges received packets STREAM : Carries stream data with offset and length MAX_DATA / MAX_STREAM_DATA : Advertises flow control updates RESET_STREAM / STOP_SENDING : Aborts stream transmission CONNECTION_CLOSE : Terminates the connection PATH_CHALLENGE / PATH_RESPONSE : Validates network path during migration","title":"Frame Types"},{"location":"protocols/quic/#security-properties","text":"","title":"Security Properties"},{"location":"protocols/quic/#encryption","text":"QUIC uses TLS 1.3 for key derivation and encryption. Packet payloads are encrypted using AEAD algorithms (typically AES-128-GCM or ChaCha20-Poly1305). Packet headers are protected to prevent ossification by middleboxes. Encryption levels: Initial : Derived from well-known salt and client-chosen Destination Connection ID 0-RTT : Derived from resumed session keys Handshake : Derived from ephemeral handshake keys 1-RTT : Derived from final application traffic keys","title":"Encryption"},{"location":"protocols/quic/#header-protection","text":"Packet numbers are encrypted to prevent middleboxes from tracking flows and inferring loss patterns. A sample from the encrypted payload is used to mask the packet number and flags.","title":"Header Protection"},{"location":"protocols/quic/#replay-protection","text":"1-RTT data is protected by TLS's anti-replay mechanisms. 0-RTT data is inherently replayable and must be used only for idempotent operations. Servers can optionally reject 0-RTT to enforce strict replay protection.","title":"Replay Protection"},{"location":"protocols/quic/#quic-implementation-in-spooky","text":"Spooky uses the quiche QUIC library for connection management and HTTP/3 framing. The implementation handles:","title":"QUIC Implementation in Spooky"},{"location":"protocols/quic/#connection-management","text":"Endpoint creation : Spooky binds to the configured UDP port and initializes the QUIC endpoint with TLS configuration Connection acceptance : Incoming QUIC connections are validated against TLS certificate requirements Idle timeout : Connections are closed after the configured idle period to reclaim resources Graceful shutdown : Spooky sends CONNECTION_CLOSE frames during shutdown to inform clients","title":"Connection Management"},{"location":"protocols/quic/#tls-integration","text":"TLS 1.3 is mandatory for QUIC. Spooky loads certificate chains and private keys at startup: let tls_config = rustls::ServerConfig::builder() .with_safe_defaults() .with_no_client_auth() .with_single_cert(certs, private_key)?; ALPN (Application-Layer Protocol Negotiation) is configured to advertise \"h3\" for HTTP/3: tls_config.alpn_protocols = vec![b\"h3\".to_vec()];","title":"TLS Integration"},{"location":"protocols/quic/#stream-handling","text":"HTTP/3 streams are mapped to QUIC streams: Request streams : Client-initiated bidirectional streams carry HTTP requests and responses Control stream : Unidirectional stream for HTTP/3 settings and control frames QPACK streams : Separate unidirectional streams for QPACK encoder/decoder communication Spooky's edge component accepts incoming streams and passes them to the bridge for HTTP/3 processing.","title":"Stream Handling"},{"location":"protocols/quic/#flow-control_1","text":"Spooky configures QUIC transport parameters to balance memory usage and throughput: Connection-level flow control prevents excessive buffering across all streams Stream-level flow control limits individual stream memory consumption Defaults are tuned for typical web traffic patterns but can be adjusted for specific workloads","title":"Flow Control"},{"location":"protocols/quic/#congestion-control_1","text":"Spooky uses the default NewReno congestion control provided by quiche . This choice balances fairness with TCP flows and predictable behavior across network conditions.","title":"Congestion Control"},{"location":"protocols/quic/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"protocols/quic/#udp-socket-optimization","text":"QUIC performance depends on efficient UDP socket handling: Receive buffer size : Increase SO_RCVBUF to reduce packet loss during bursts Send buffer size : Increase SO_SNDBUF to support large congestion windows GRO/GSO : Generic Receive/Send Offload reduces per-packet processing overhead on Linux","title":"UDP Socket Optimization"},{"location":"protocols/quic/#packet-pacing","text":"Sending packets in bursts can trigger packet loss due to buffer overflow. QUIC implementations pace packet transmission based on the congestion window and RTT to smooth traffic.","title":"Packet Pacing"},{"location":"protocols/quic/#cpu-usage","text":"QUIC's cryptographic operations (AEAD encryption/decryption, header protection) consume CPU. Performance tuning: Use hardware-accelerated cryptographic instructions (AES-NI) Batch packet processing to amortize per-packet overhead Offload TLS operations to dedicated threads if necessary","title":"CPU Usage"},{"location":"protocols/quic/#memory-footprint","text":"Each QUIC connection maintains state for: Send and receive buffers for each stream Loss recovery metadata (sent packet information) Congestion control state (RTT estimates, congestion window) Spooky limits concurrent connections and streams to bound memory usage.","title":"Memory Footprint"},{"location":"protocols/quic/#comparison-with-tcp","text":"Feature TCP QUIC Transport Layer Kernel-space implementation User-space implementation Connection Establishment 3-way handshake + TLS (2-3 RTT) Combined handshake (1-RTT or 0-RTT) Stream Multiplexing Not supported (requires app layer) Native transport-level multiplexing Head-of-Line Blocking All streams blocked by packet loss Independent per-stream delivery Connection Migration Not supported Supported via connection IDs Congestion Control OS-dependent, slow to update User-space, rapid algorithm evolution Encryption Optional (TLS on top) Mandatory (integrated TLS 1.3) NAT Traversal Requires keep-alive mechanisms Built-in connection migration support Deployment Requires OS updates Application-level deployment","title":"Comparison with TCP"},{"location":"protocols/quic/#references","text":"RFC 9000: QUIC: A UDP-Based Multiplexed and Secure Transport RFC 9001: Using TLS to Secure QUIC RFC 9002: QUIC Loss Detection and Congestion Control RFC 8999: Version-Independent Properties of QUIC RFC 9221: An Unreliable Datagram Extension to QUIC RFC 9287: Greasing the QUIC Bit","title":"References"},{"location":"troubleshooting/common-issues/","text":"Technical reference for diagnosing and resolving operational issues in Spooky HTTP/3 to HTTP/2 gateway deployments. Configuration Errors Invalid Configuration Schema Error Messages : Invalid version: expected '1', found '2' Invalid protocol: expected 'http3', found 'http2' Invalid log level: debug-verbose Invalid load balancing type: 'weighted' for upstream 'api' Root Causes : - Configuration schema version mismatch - Unsupported protocol specification - Invalid log level (valid: whisper , haunt , spooky , scream , poltergeist , silence , trace , debug , info , warn , error , off ) - Unsupported load balancing algorithm (valid: random , round-robin , round_robin , rr , consistent-hash , consistent_hash , ch ) Diagnostic Commands : # Validate YAML syntax python3 -c \"import yaml; yaml.safe_load(open('config.yaml'))\" 2>&1 # Check configuration structure grep -E \"^version:|^listen:|^upstream:\" config.yaml # Verify log level grep \"log:\" -A 2 config.yaml # Check load balancing configuration grep \"load_balancing:\" -A 2 config.yaml Resolution : - Set version: 1 in configuration file - Use protocol: http3 for listen configuration - Correct log levels according to valid options - Update load balancing type to supported algorithms Listen Configuration Errors Error Messages : Listen address is empty Invalid listen port: 0 (must be between 1 and 65535) Invalid listen port: 70000 (must be between 1 and 65535) Failed to bind UDP socket Root Causes : - Missing or empty listen address - Port number outside valid range (1-65535) - Port already in use by another process - Insufficient privileges for privileged ports (<1024) Diagnostic Commands : # Check port availability (UDP) sudo ss -ulnp | grep :443 # Identify process using port sudo lsof -i UDP:443 # Check socket permissions sudo setcap -v 'cap_net_bind_service=+ep' /usr/local/bin/spooky # Verify listen configuration grep -A 5 \"^listen:\" config.yaml Resolution : # Grant capability for privileged port binding sudo setcap 'cap_net_bind_service=+ep' /usr/local/bin/spooky # Or bind to non-privileged port sed -i 's/port: 443/port: 8443/' config.yaml # Kill conflicting process sudo fuser -k 443/udp Upstream Pool Configuration Errors Error Messages : No upstreams configured Upstream name is empty Upstream 'api' has no backends configured Upstream 'api' must have either 'host' or 'path_prefix' route matcher Route path_prefix cannot be empty for upstream 'api' Route path_prefix must start with '/' for upstream 'api': api/v1 Root Causes : - Empty upstream map in configuration - Missing route matching criteria (no host or path_prefix ) - Invalid path prefix format (must start with / ) - Empty backend list for upstream pool Diagnostic Commands : # List configured upstreams grep \"^upstream:\" -A 50 config.yaml | grep -E \"^ [a-z]\" # Check route configuration yq '.upstream[].route' config.yaml # Validate path prefixes grep \"path_prefix:\" config.yaml Resolution : # Correct upstream configuration upstream: api: route: host: \"api.example.com\" # Host-based routing path_prefix: \"/api\" # Must start with / load_balancing: type: round-robin backends: - id: backend1 address: \"10.0.1.10:8080\" weight: 100 Backend Configuration Errors Error Messages : Backend ID is empty in upstream 'api' Backend address is empty for backend 'backend1' in upstream 'api' Backend address '10.0.1.10' in upstream 'api' must be in host:port format Backend 'backend1' in upstream 'api' has invalid weight (0) Health check interval is invalid (0) for backend 'backend1' in upstream 'api' Health check timeout is invalid (0) for backend 'backend1' in upstream 'api' Health check failure threshold is invalid (0) for backend 'backend1' in upstream 'api' Health check success threshold is invalid (0) for backend 'backend1' in upstream 'api' Health check cooldown is invalid (0) for backend 'backend1' in upstream 'api' Root Causes : - Missing or malformed backend address (must be host:port ) - Zero values for weight or health check parameters - Invalid health check configuration Diagnostic Commands : # Validate backend addresses grep \"address:\" config.yaml | grep -v \":\" # Check health check configuration yq '.upstream[].backends[].health_check' config.yaml # Verify backend weights yq '.upstream[].backends[].weight' config.yaml Resolution : # Correct backend configuration backends: - id: backend1 address: \"10.0.1.10:8080\" # Must include port weight: 100 # Must be > 0 health_check: path: \"/health\" interval: 5000 # Must be > 0 (milliseconds) timeout_ms: 2000 # Must be > 0 failure_threshold: 3 # Must be > 0 success_threshold: 2 # Must be > 0 cooldown_ms: 10000 # Must be > 0 TLS Certificate Problems Certificate File Access Errors Error Messages : TLS certificate file does not exist: /etc/spooky/certs/server.crt TLS private key file does not exist: /etc/spooky/certs/server.key Cannot read TLS certificate file '/etc/spooky/certs/server.crt': Permission denied Cannot read TLS private key file '/etc/spooky/certs/server.key': Permission denied Failed to load certificate '/etc/spooky/certs/server.crt': No such file or directory Failed to load key '/etc/spooky/certs/server.key': error:02001002:system library:fopen:No such file or directory Root Causes : - Certificate or key file path does not exist - Insufficient file permissions for Spooky process - Invalid PEM format - File ownership prevents access Diagnostic Commands : # Verify file existence and permissions ls -la /etc/spooky/certs/server.{crt,key} # Check file ownership stat /etc/spooky/certs/server.crt # Test read access sudo -u spooky cat /etc/spooky/certs/server.crt > /dev/null # Validate PEM format openssl x509 -in /etc/spooky/certs/server.crt -text -noout openssl rsa -in /etc/spooky/certs/server.key -check -noout Resolution : # Fix file permissions sudo chown spooky:spooky /etc/spooky/certs/server.{crt,key} sudo chmod 644 /etc/spooky/certs/server.crt sudo chmod 600 /etc/spooky/certs/server.key # Verify certificate chain openssl verify -CAfile ca.crt /etc/spooky/certs/server.crt # Test certificate-key pair match diff <(openssl x509 -in server.crt -noout -modulus | openssl md5) \\ <(openssl rsa -in server.key -noout -modulus | openssl md5) TLS Handshake Failures Error Messages : TLS configuration error during request processing: handshake failure Failed to load certificate: invalid certificate format QUIC recv failed: TlsFail Root Causes : - Certificate-key mismatch - Expired certificate - Incomplete certificate chain - Unsupported TLS version - Client does not support required ALPN protocols ( h3 , h3-29 ) Diagnostic Commands : # Check certificate expiration openssl x509 -in server.crt -noout -dates # Verify certificate chain openssl s_client -connect localhost:443 -showcerts < /dev/null # Check ALPN negotiation (requires curl with HTTP/3 support) curl --http3 -v https://localhost:443 2>&1 | grep -i alpn # Monitor TLS handshake packets sudo tcpdump -i any -n udp port 443 -X | grep -A 20 \"Initial\" Resolution : # Regenerate certificate with proper SAN openssl req -new -x509 -days 365 -key server.key -out server.crt \\ -subj \"/CN=example.com\" \\ -addext \"subjectAltName=DNS:example.com,DNS:*.example.com\" # Ensure certificate chain is complete cat server.crt intermediate.crt > fullchain.crt # Update configuration sed -i 's|cert: .*|cert: /etc/spooky/certs/fullchain.crt|' config.yaml QUIC Connection Issues Connection ID Mismatch Error Messages : Wrong QUIC HEADER Non-Initial packet for unknown connection, ignoring Dropping packet for unknown connection from 192.168.1.10:52341 (DCID: a3f2...) Root Causes : - Client using stale connection ID after server restart - Connection ID collision or corruption - NAT rebinding without proper migration support - Packet reordering or duplication Diagnostic Commands : # Monitor connection IDs in logs journalctl -u spooky -f | grep -E \"DCID|SCID\" # Check active QUIC connections ss -u -a | grep :443 # Capture QUIC packets for analysis sudo tcpdump -i any -w quic.pcap udp port 443 tshark -r quic.pcap -Y quic # Count connection errors journalctl -u spooky --since \"1 hour ago\" | grep -c \"Wrong QUIC HEADER\" Resolution : - Issue is typically transient; clients will establish new connections - Ensure set_disable_active_migration(true) is set in QUIC config - Check for network middleboxes modifying UDP payloads - Increase max_idle_timeout if connections drop prematurely Version Negotiation Failures Error Messages : Version negotiation failed: buffer too short Failed to send version negotiation: Network unreachable Root Causes : - Client requesting unsupported QUIC version - MTU constraints preventing version negotiation packet transmission - Network path blocking UDP packets - Firewall stateful inspection interfering with QUIC Diagnostic Commands : # Check supported QUIC version journalctl -u spooky | grep \"PROTOCOL_VERSION\" # Test MTU path tracepath -n -b 443 target-host # Verify UDP egress nc -u -v -w 1 target-host 443 < /dev/null # Monitor version negotiation packets sudo tcpdump -i any udp port 443 -v | grep -i version Resolution : # Configure smaller UDP payload size # Edit config or quiche parameters: # set_max_recv_udp_payload_size(1200) # set_max_send_udp_payload_size(1200) # Adjust firewall rules sudo iptables -I INPUT -p udp --dport 443 -j ACCEPT sudo iptables -I OUTPUT -p udp --sport 443 -j ACCEPT QUIC Timeout and Idle Connections Error Messages : QUIC recv failed: Done Connection closed, not storing Root Causes : - Idle timeout exceeded (default 5000ms in Spooky) - Network path timeout - Client terminated connection without proper close - NAT binding expired Diagnostic Commands : # Check connection lifetimes journalctl -u spooky | grep -E \"Creating new connection|Connection closed\" | tail -20 # Monitor timeout events journalctl -u spooky -f | grep \"on_timeout\" # Analyze connection duration distribution journalctl -u spooky --since \"1 hour ago\" | \\ grep \"Creating new connection\" | wc -l # Check NAT timeout settings (if behind NAT) cat /proc/sys/net/netfilter/nf_conntrack_udp_timeout Resolution : // Adjust idle timeout in quiche configuration quic_config.set_max_idle_timeout(10000); // Increase to 10 seconds // Tune UDP stream limits quic_config.set_initial_max_streams_bidi(200); quic_config.set_initial_max_streams_uni(200); Backend Connectivity Failures Unknown Backend Errors Error Messages : No route found for path: /api/users (host: Some(\"api.example.com\")) Upstream pool not found for: api unknown backend: 10.0.1.10:8080 Root Causes : - Request path/host does not match any configured upstream route - Upstream pool not properly initialized - Backend not registered in H2 connection pool - Route matching logic precedence issue Diagnostic Commands : # List configured routes yq '.upstream[] | {route}' config.yaml # Test route matching journalctl -u spooky -f | grep \"No route found\" # Verify H2 pool initialization journalctl -u spooky --since \"10 minutes ago\" | grep -i \"pool\" # Check backend registration ss -t | grep :8080 | wc -l Resolution : # Ensure proper route specificity (longest prefix matching) upstream: api_v2: route: host: \"api.example.com\" path_prefix: \"/api/v2\" # More specific backends: [...] api_v1: route: host: \"api.example.com\" path_prefix: \"/api\" # Less specific backends: [...] default: route: path_prefix: \"/\" # Catch-all backends: [...] HTTP/2 Connection Pool Errors Error Messages : Transport error: send: connection error detected: frame with invalid size Transport error: send: connection closed Transport error: body: stream error received: stream no longer needed Backend timeout Root Causes : - Backend closed HTTP/2 connection unexpectedly - H2 frame size violation - Backend service crashed or restarted - Connection pool exhaustion (>64 inflight requests per backend) - Network timeout (2s default in Spooky) Diagnostic Commands : # Monitor H2 connection errors journalctl -u spooky -f | grep \"Transport error\" # Check backend H2 support curl -I --http2 http://10.0.1.10:8080/ # Test backend health endpoint curl -v http://10.0.1.10:8080/health # Monitor connection pool saturation journalctl -u spooky | grep \"semaphore closed\" # Check backend service status systemctl status backend-service Resolution : # Increase backend timeout if needed # Edit quic_listener.rs: BACKEND_TIMEOUT = Duration::from_secs(5); # Adjust max inflight requests per backend # Edit quic_listener.rs: MAX_INFLIGHT_PER_BACKEND = 128; # Restart backend service sudo systemctl restart backend-service # Monitor backend connection states watch -n 1 'ss -t -a | grep :8080' Backend Health Check Failures Error Messages : Backend 10.0.1.10:8080 became unhealthy Health checks disabled: no Tokio runtime available Root Causes : - Backend failing health check endpoint - Health check timeout too aggressive - Backend intermittently unavailable - Network path to backend unreliable - Health check threshold too sensitive Diagnostic Commands : # Monitor health transitions journalctl -u spooky -f | grep -E \"became healthy|became unhealthy\" # Manual health check curl -w \"@-\" -o /dev/null -s http://10.0.1.10:8080/health <<< \\ 'time_total: %{time_total}s\\nhttp_code: %{http_code}\\n' # Check health check configuration yq '.upstream[].backends[].health_check' config.yaml # Monitor backend response times httping -c 10 http://10.0.1.10:8080/health Resolution : # Adjust health check parameters for stability backends: - id: backend1 address: \"10.0.1.10:8080\" health_check: path: \"/health\" interval: 10000 # Increase interval timeout_ms: 5000 # Increase timeout failure_threshold: 5 # Require more failures success_threshold: 2 # Require consecutive successes cooldown_ms: 30000 # Longer cooldown Load Balancing Issues No Healthy Backends Available Error Messages : no healthy servers no servers configured for upstream Root Causes : - All backends failed health checks - Empty backend list for upstream - Backends in cooldown period after failures - Circuit breaker triggered Diagnostic Commands : # Check backend health status journalctl -u spooky | grep -E \"became healthy|became unhealthy\" | tail -20 # Monitor 503 Service Unavailable responses journalctl -u spooky | grep \"status 503\" # Count healthy vs total backends per upstream yq '.upstream[].backends | length' config.yaml # Check recent health transitions journalctl -u spooky --since \"5 minutes ago\" | grep \"Backend\" Resolution : # Verify backend services are running for backend in 10.0.1.10:8080 10.0.1.11:8080; do echo -n \"$backend: \" curl -s -o /dev/null -w \"%{http_code}\" http://$backend/health || echo \"FAIL\" echo done # Temporarily disable health checks for debugging # Set failure_threshold very high in config.yaml # Restart Spooky to reset health state sudo systemctl restart spooky Uneven Load Distribution Symptoms : - One backend receives disproportionate traffic - Round-robin not cycling through backends - Consistent hash not distributing evenly Root Causes : - Backend weight misconfiguration - Inconsistent hash key selection (always same key) - Some backends marked unhealthy - Hash ring replica count too low for consistent-hash Diagnostic Commands : # Analyze backend selection distribution journalctl -u spooky | grep \"Selected backend\" | \\ awk '{print $(NF-2)}' | sort | uniq -c # Check backend weights yq '.upstream[].backends[] | \"\\(.id): \\(.weight)\"' config.yaml # Monitor load balancing algorithm journalctl -u spooky | grep \"via round-robin\\|via consistent-hash\\|via random\" # Check hash key consistency journalctl -u spooky | grep \"request_hash_key\" Resolution : # Ensure proper weight distribution backends: - id: backend1 address: \"10.0.1.10:8080\" weight: 100 - id: backend2 address: \"10.0.1.11:8080\" weight: 100 # Equal weight for even distribution # For consistent-hash, increase replica count # Edit lb/src/lib.rs: DEFAULT_REPLICAS = 128; Performance Problems High Latency Symptoms : - latency_ms in logs consistently >1000ms - Slow response times reported by clients - Backend timeout errors (503 status) Root Causes : - Backend processing delay - Network congestion - Connection pool saturation - CPU saturation on Spooky host - Inefficient load balancing Diagnostic Commands : # Analyze latency distribution journalctl -u spooky --since \"1 hour ago\" | \\ grep \"latency_ms\" | \\ awk '{print $(NF)}' | \\ sort -n | \\ awk '{sum+=$1; arr[NR]=$1} END { print \"min:\", arr[1]; print \"p50:\", arr[int(NR*0.5)]; print \"p95:\", arr[int(NR*0.95)]; print \"p99:\", arr[int(NR*0.99)]; print \"max:\", arr[NR]; print \"avg:\", sum/NR; }' # Monitor CPU usage top -b -n 1 | grep spooky # Check connection pool contention journalctl -u spooky | grep \"semaphore\" | tail -20 # Measure backend response time directly time curl http://10.0.1.10:8080/api/test Resolution : # Increase backend timeout if backends are slow but reliable # Edit BACKEND_TIMEOUT in quic_listener.rs # Scale backend capacity # Add more backends to upstream pool # Increase connection pool size # Edit MAX_INFLIGHT_PER_BACKEND in quic_listener.rs # Optimize backend application # Profile and optimize backend code Memory Growth Symptoms : - RSS memory continuously increasing - Out of memory errors - System swapping Root Causes : - Connection leak (connections not properly closed) - Request/response body buffering - Metrics accumulation - QUIC connection state not cleaned up Diagnostic Commands : # Monitor memory usage over time while true; do ps -p $(pgrep spooky) -o pid,vsz,rss,cmd | tail -1 sleep 10 done # Check connection count ss -u | grep -c :443 # Analyze memory map sudo pmap -x $(pgrep spooky) # Check for file descriptor leaks ls -l /proc/$(pgrep spooky)/fd | wc -l Resolution : # Restart Spooky periodically (temporary mitigation) sudo systemctl restart spooky # Monitor connection cleanup journalctl -u spooky -f | grep \"Connection closed\" # Reduce max idle timeout # Edit quic_config.set_max_idle_timeout(3000); # Limit connection count # Implement connection limit in accept logic UDP Packet Loss Symptoms : - Retransmissions in QUIC logs - Client timeout errors - Degraded throughput Root Causes : - Network congestion - UDP buffer overflow (receive buffer too small) - Firewall dropping packets - MTU fragmentation Diagnostic Commands : # Check UDP buffer sizes sysctl net.core.rmem_max net.core.rmem_default sysctl net.core.wmem_max net.core.wmem_default # Monitor UDP statistics netstat -su | grep -E \"packet receive errors|receive buffer errors\" # Capture packet loss sudo tcpdump -i any -c 1000 udp port 443 -w capture.pcap tshark -r capture.pcap -q -z io,stat,1 # Check interface statistics ip -s link show eth0 Resolution : # Increase UDP buffer sizes sudo sysctl -w net.core.rmem_max=26214400 sudo sysctl -w net.core.wmem_max=26214400 sudo sysctl -w net.core.rmem_default=26214400 sudo sysctl -w net.core.wmem_default=26214400 # Make permanent echo \"net.core.rmem_max=26214400\" | sudo tee -a /etc/sysctl.conf echo \"net.core.wmem_max=26214400\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p # Reduce UDP payload size # Edit quic_config.set_max_recv_udp_payload_size(1350); Debugging Techniques Enable Debug Logging # config.yaml log: level: haunt # debug level # Restart to apply sudo systemctl restart spooky # Monitor debug logs journalctl -u spooky -f --output=cat Analyze Request Flow # Trace specific request path journalctl -u spooky | grep -E \"HTTP/3 request|Selected backend|Upstream.*status\" | \\ grep \"/api/users\" # Monitor complete request lifecycle journalctl -u spooky -f | \\ grep -E \"Creating new connection|HTTP/3 request|Selected backend|status.*latency_ms|Connection closed\" Packet Capture Analysis # Capture QUIC traffic sudo tcpdump -i any -w spooky.pcap udp port 443 # Analyze with tshark tshark -r spooky.pcap -Y quic -T fields \\ -e frame.time -e ip.src -e ip.dst -e quic.header_form # Decrypt QUIC (requires SSLKEYLOGFILE) SSLKEYLOGFILE=/tmp/keys.log curl --http3 https://localhost:443/ tshark -r spooky.pcap -o tls.keylog_file:/tmp/keys.log -Y http3 Performance Profiling # CPU profiling with perf sudo perf record -F 99 -p $(pgrep spooky) -g -- sleep 30 sudo perf report --stdio | head -50 # Flamegraph generation sudo perf record -F 99 -p $(pgrep spooky) -g -- sleep 30 sudo perf script | stackcollapse-perf.pl | flamegraph.pl > flamegraph.svg # Memory profiling (if built with jemalloc) export MALLOC_CONF=prof:true,prof_prefix:/tmp/jeprof sudo systemctl restart spooky # Send traffic, then analyze with jeprof Common Error Reference Error Message HTTP Status Cause Resolution invalid request 400 Malformed HTTP/3 headers Check client request format no servers configured for upstream 503 Empty backend list Add backends to upstream config no healthy servers 503 All backends unhealthy Check backend health, adjust thresholds invalid server 503 Backend index out of bounds Configuration reload race condition upstream error 502 Backend connection failed Verify backend connectivity upstream timeout 503 Backend exceeded 2s timeout Increase BACKEND_TIMEOUT or optimize backend internal server error 500 TLS configuration error Check certificate/key files Wrong QUIC HEADER (dropped) Malformed QUIC packet Check for network corruption No route found for path (internal) No matching upstream route Add route configuration Upstream pool not found (internal) Pool initialization failure Check logs for startup errors Support and Escalation When reporting issues, include: # 1. Version information spooky --version # 2. Configuration (sanitized) yq eval 'del(.listen.tls.key, .listen.tls.cert)' config.yaml # 3. System information uname -a cat /etc/os-release # 4. Error logs (last 100 lines) journalctl -u spooky --no-pager -n 100 --since \"1 hour ago\" # 5. Resource utilization ps aux | grep spooky ss -u | grep -c :443 free -h # 6. Network diagnostics ss -ulnp | grep spooky sudo iptables -L -n -v | grep 443 For production incidents, capture diagnostic bundle: #!/bin/bash mkdir -p spooky-diagnostics cd spooky-diagnostics spooky --version > version.txt uname -a > system.txt journalctl -u spooky --no-pager -n 500 > logs.txt yq eval 'del(.listen.tls.key, .listen.tls.cert)' ../config.yaml > config.yaml ps aux | grep spooky > processes.txt ss -tulnp > sockets.txt free -h > memory.txt sudo tcpdump -i any -c 100 -w capture.pcap udp port 443 cd .. tar czf spooky-diagnostics-$(date +%Y%m%d-%H%M%S).tar.gz spooky-diagnostics/","title":"Troubleshooting"},{"location":"troubleshooting/common-issues/#configuration-errors","text":"","title":"Configuration Errors"},{"location":"troubleshooting/common-issues/#invalid-configuration-schema","text":"Error Messages : Invalid version: expected '1', found '2' Invalid protocol: expected 'http3', found 'http2' Invalid log level: debug-verbose Invalid load balancing type: 'weighted' for upstream 'api' Root Causes : - Configuration schema version mismatch - Unsupported protocol specification - Invalid log level (valid: whisper , haunt , spooky , scream , poltergeist , silence , trace , debug , info , warn , error , off ) - Unsupported load balancing algorithm (valid: random , round-robin , round_robin , rr , consistent-hash , consistent_hash , ch ) Diagnostic Commands : # Validate YAML syntax python3 -c \"import yaml; yaml.safe_load(open('config.yaml'))\" 2>&1 # Check configuration structure grep -E \"^version:|^listen:|^upstream:\" config.yaml # Verify log level grep \"log:\" -A 2 config.yaml # Check load balancing configuration grep \"load_balancing:\" -A 2 config.yaml Resolution : - Set version: 1 in configuration file - Use protocol: http3 for listen configuration - Correct log levels according to valid options - Update load balancing type to supported algorithms","title":"Invalid Configuration Schema"},{"location":"troubleshooting/common-issues/#listen-configuration-errors","text":"Error Messages : Listen address is empty Invalid listen port: 0 (must be between 1 and 65535) Invalid listen port: 70000 (must be between 1 and 65535) Failed to bind UDP socket Root Causes : - Missing or empty listen address - Port number outside valid range (1-65535) - Port already in use by another process - Insufficient privileges for privileged ports (<1024) Diagnostic Commands : # Check port availability (UDP) sudo ss -ulnp | grep :443 # Identify process using port sudo lsof -i UDP:443 # Check socket permissions sudo setcap -v 'cap_net_bind_service=+ep' /usr/local/bin/spooky # Verify listen configuration grep -A 5 \"^listen:\" config.yaml Resolution : # Grant capability for privileged port binding sudo setcap 'cap_net_bind_service=+ep' /usr/local/bin/spooky # Or bind to non-privileged port sed -i 's/port: 443/port: 8443/' config.yaml # Kill conflicting process sudo fuser -k 443/udp","title":"Listen Configuration Errors"},{"location":"troubleshooting/common-issues/#upstream-pool-configuration-errors","text":"Error Messages : No upstreams configured Upstream name is empty Upstream 'api' has no backends configured Upstream 'api' must have either 'host' or 'path_prefix' route matcher Route path_prefix cannot be empty for upstream 'api' Route path_prefix must start with '/' for upstream 'api': api/v1 Root Causes : - Empty upstream map in configuration - Missing route matching criteria (no host or path_prefix ) - Invalid path prefix format (must start with / ) - Empty backend list for upstream pool Diagnostic Commands : # List configured upstreams grep \"^upstream:\" -A 50 config.yaml | grep -E \"^ [a-z]\" # Check route configuration yq '.upstream[].route' config.yaml # Validate path prefixes grep \"path_prefix:\" config.yaml Resolution : # Correct upstream configuration upstream: api: route: host: \"api.example.com\" # Host-based routing path_prefix: \"/api\" # Must start with / load_balancing: type: round-robin backends: - id: backend1 address: \"10.0.1.10:8080\" weight: 100","title":"Upstream Pool Configuration Errors"},{"location":"troubleshooting/common-issues/#backend-configuration-errors","text":"Error Messages : Backend ID is empty in upstream 'api' Backend address is empty for backend 'backend1' in upstream 'api' Backend address '10.0.1.10' in upstream 'api' must be in host:port format Backend 'backend1' in upstream 'api' has invalid weight (0) Health check interval is invalid (0) for backend 'backend1' in upstream 'api' Health check timeout is invalid (0) for backend 'backend1' in upstream 'api' Health check failure threshold is invalid (0) for backend 'backend1' in upstream 'api' Health check success threshold is invalid (0) for backend 'backend1' in upstream 'api' Health check cooldown is invalid (0) for backend 'backend1' in upstream 'api' Root Causes : - Missing or malformed backend address (must be host:port ) - Zero values for weight or health check parameters - Invalid health check configuration Diagnostic Commands : # Validate backend addresses grep \"address:\" config.yaml | grep -v \":\" # Check health check configuration yq '.upstream[].backends[].health_check' config.yaml # Verify backend weights yq '.upstream[].backends[].weight' config.yaml Resolution : # Correct backend configuration backends: - id: backend1 address: \"10.0.1.10:8080\" # Must include port weight: 100 # Must be > 0 health_check: path: \"/health\" interval: 5000 # Must be > 0 (milliseconds) timeout_ms: 2000 # Must be > 0 failure_threshold: 3 # Must be > 0 success_threshold: 2 # Must be > 0 cooldown_ms: 10000 # Must be > 0","title":"Backend Configuration Errors"},{"location":"troubleshooting/common-issues/#tls-certificate-problems","text":"","title":"TLS Certificate Problems"},{"location":"troubleshooting/common-issues/#certificate-file-access-errors","text":"Error Messages : TLS certificate file does not exist: /etc/spooky/certs/server.crt TLS private key file does not exist: /etc/spooky/certs/server.key Cannot read TLS certificate file '/etc/spooky/certs/server.crt': Permission denied Cannot read TLS private key file '/etc/spooky/certs/server.key': Permission denied Failed to load certificate '/etc/spooky/certs/server.crt': No such file or directory Failed to load key '/etc/spooky/certs/server.key': error:02001002:system library:fopen:No such file or directory Root Causes : - Certificate or key file path does not exist - Insufficient file permissions for Spooky process - Invalid PEM format - File ownership prevents access Diagnostic Commands : # Verify file existence and permissions ls -la /etc/spooky/certs/server.{crt,key} # Check file ownership stat /etc/spooky/certs/server.crt # Test read access sudo -u spooky cat /etc/spooky/certs/server.crt > /dev/null # Validate PEM format openssl x509 -in /etc/spooky/certs/server.crt -text -noout openssl rsa -in /etc/spooky/certs/server.key -check -noout Resolution : # Fix file permissions sudo chown spooky:spooky /etc/spooky/certs/server.{crt,key} sudo chmod 644 /etc/spooky/certs/server.crt sudo chmod 600 /etc/spooky/certs/server.key # Verify certificate chain openssl verify -CAfile ca.crt /etc/spooky/certs/server.crt # Test certificate-key pair match diff <(openssl x509 -in server.crt -noout -modulus | openssl md5) \\ <(openssl rsa -in server.key -noout -modulus | openssl md5)","title":"Certificate File Access Errors"},{"location":"troubleshooting/common-issues/#tls-handshake-failures","text":"Error Messages : TLS configuration error during request processing: handshake failure Failed to load certificate: invalid certificate format QUIC recv failed: TlsFail Root Causes : - Certificate-key mismatch - Expired certificate - Incomplete certificate chain - Unsupported TLS version - Client does not support required ALPN protocols ( h3 , h3-29 ) Diagnostic Commands : # Check certificate expiration openssl x509 -in server.crt -noout -dates # Verify certificate chain openssl s_client -connect localhost:443 -showcerts < /dev/null # Check ALPN negotiation (requires curl with HTTP/3 support) curl --http3 -v https://localhost:443 2>&1 | grep -i alpn # Monitor TLS handshake packets sudo tcpdump -i any -n udp port 443 -X | grep -A 20 \"Initial\" Resolution : # Regenerate certificate with proper SAN openssl req -new -x509 -days 365 -key server.key -out server.crt \\ -subj \"/CN=example.com\" \\ -addext \"subjectAltName=DNS:example.com,DNS:*.example.com\" # Ensure certificate chain is complete cat server.crt intermediate.crt > fullchain.crt # Update configuration sed -i 's|cert: .*|cert: /etc/spooky/certs/fullchain.crt|' config.yaml","title":"TLS Handshake Failures"},{"location":"troubleshooting/common-issues/#quic-connection-issues","text":"","title":"QUIC Connection Issues"},{"location":"troubleshooting/common-issues/#connection-id-mismatch","text":"Error Messages : Wrong QUIC HEADER Non-Initial packet for unknown connection, ignoring Dropping packet for unknown connection from 192.168.1.10:52341 (DCID: a3f2...) Root Causes : - Client using stale connection ID after server restart - Connection ID collision or corruption - NAT rebinding without proper migration support - Packet reordering or duplication Diagnostic Commands : # Monitor connection IDs in logs journalctl -u spooky -f | grep -E \"DCID|SCID\" # Check active QUIC connections ss -u -a | grep :443 # Capture QUIC packets for analysis sudo tcpdump -i any -w quic.pcap udp port 443 tshark -r quic.pcap -Y quic # Count connection errors journalctl -u spooky --since \"1 hour ago\" | grep -c \"Wrong QUIC HEADER\" Resolution : - Issue is typically transient; clients will establish new connections - Ensure set_disable_active_migration(true) is set in QUIC config - Check for network middleboxes modifying UDP payloads - Increase max_idle_timeout if connections drop prematurely","title":"Connection ID Mismatch"},{"location":"troubleshooting/common-issues/#version-negotiation-failures","text":"Error Messages : Version negotiation failed: buffer too short Failed to send version negotiation: Network unreachable Root Causes : - Client requesting unsupported QUIC version - MTU constraints preventing version negotiation packet transmission - Network path blocking UDP packets - Firewall stateful inspection interfering with QUIC Diagnostic Commands : # Check supported QUIC version journalctl -u spooky | grep \"PROTOCOL_VERSION\" # Test MTU path tracepath -n -b 443 target-host # Verify UDP egress nc -u -v -w 1 target-host 443 < /dev/null # Monitor version negotiation packets sudo tcpdump -i any udp port 443 -v | grep -i version Resolution : # Configure smaller UDP payload size # Edit config or quiche parameters: # set_max_recv_udp_payload_size(1200) # set_max_send_udp_payload_size(1200) # Adjust firewall rules sudo iptables -I INPUT -p udp --dport 443 -j ACCEPT sudo iptables -I OUTPUT -p udp --sport 443 -j ACCEPT","title":"Version Negotiation Failures"},{"location":"troubleshooting/common-issues/#quic-timeout-and-idle-connections","text":"Error Messages : QUIC recv failed: Done Connection closed, not storing Root Causes : - Idle timeout exceeded (default 5000ms in Spooky) - Network path timeout - Client terminated connection without proper close - NAT binding expired Diagnostic Commands : # Check connection lifetimes journalctl -u spooky | grep -E \"Creating new connection|Connection closed\" | tail -20 # Monitor timeout events journalctl -u spooky -f | grep \"on_timeout\" # Analyze connection duration distribution journalctl -u spooky --since \"1 hour ago\" | \\ grep \"Creating new connection\" | wc -l # Check NAT timeout settings (if behind NAT) cat /proc/sys/net/netfilter/nf_conntrack_udp_timeout Resolution : // Adjust idle timeout in quiche configuration quic_config.set_max_idle_timeout(10000); // Increase to 10 seconds // Tune UDP stream limits quic_config.set_initial_max_streams_bidi(200); quic_config.set_initial_max_streams_uni(200);","title":"QUIC Timeout and Idle Connections"},{"location":"troubleshooting/common-issues/#backend-connectivity-failures","text":"","title":"Backend Connectivity Failures"},{"location":"troubleshooting/common-issues/#unknown-backend-errors","text":"Error Messages : No route found for path: /api/users (host: Some(\"api.example.com\")) Upstream pool not found for: api unknown backend: 10.0.1.10:8080 Root Causes : - Request path/host does not match any configured upstream route - Upstream pool not properly initialized - Backend not registered in H2 connection pool - Route matching logic precedence issue Diagnostic Commands : # List configured routes yq '.upstream[] | {route}' config.yaml # Test route matching journalctl -u spooky -f | grep \"No route found\" # Verify H2 pool initialization journalctl -u spooky --since \"10 minutes ago\" | grep -i \"pool\" # Check backend registration ss -t | grep :8080 | wc -l Resolution : # Ensure proper route specificity (longest prefix matching) upstream: api_v2: route: host: \"api.example.com\" path_prefix: \"/api/v2\" # More specific backends: [...] api_v1: route: host: \"api.example.com\" path_prefix: \"/api\" # Less specific backends: [...] default: route: path_prefix: \"/\" # Catch-all backends: [...]","title":"Unknown Backend Errors"},{"location":"troubleshooting/common-issues/#http2-connection-pool-errors","text":"Error Messages : Transport error: send: connection error detected: frame with invalid size Transport error: send: connection closed Transport error: body: stream error received: stream no longer needed Backend timeout Root Causes : - Backend closed HTTP/2 connection unexpectedly - H2 frame size violation - Backend service crashed or restarted - Connection pool exhaustion (>64 inflight requests per backend) - Network timeout (2s default in Spooky) Diagnostic Commands : # Monitor H2 connection errors journalctl -u spooky -f | grep \"Transport error\" # Check backend H2 support curl -I --http2 http://10.0.1.10:8080/ # Test backend health endpoint curl -v http://10.0.1.10:8080/health # Monitor connection pool saturation journalctl -u spooky | grep \"semaphore closed\" # Check backend service status systemctl status backend-service Resolution : # Increase backend timeout if needed # Edit quic_listener.rs: BACKEND_TIMEOUT = Duration::from_secs(5); # Adjust max inflight requests per backend # Edit quic_listener.rs: MAX_INFLIGHT_PER_BACKEND = 128; # Restart backend service sudo systemctl restart backend-service # Monitor backend connection states watch -n 1 'ss -t -a | grep :8080'","title":"HTTP/2 Connection Pool Errors"},{"location":"troubleshooting/common-issues/#backend-health-check-failures","text":"Error Messages : Backend 10.0.1.10:8080 became unhealthy Health checks disabled: no Tokio runtime available Root Causes : - Backend failing health check endpoint - Health check timeout too aggressive - Backend intermittently unavailable - Network path to backend unreliable - Health check threshold too sensitive Diagnostic Commands : # Monitor health transitions journalctl -u spooky -f | grep -E \"became healthy|became unhealthy\" # Manual health check curl -w \"@-\" -o /dev/null -s http://10.0.1.10:8080/health <<< \\ 'time_total: %{time_total}s\\nhttp_code: %{http_code}\\n' # Check health check configuration yq '.upstream[].backends[].health_check' config.yaml # Monitor backend response times httping -c 10 http://10.0.1.10:8080/health Resolution : # Adjust health check parameters for stability backends: - id: backend1 address: \"10.0.1.10:8080\" health_check: path: \"/health\" interval: 10000 # Increase interval timeout_ms: 5000 # Increase timeout failure_threshold: 5 # Require more failures success_threshold: 2 # Require consecutive successes cooldown_ms: 30000 # Longer cooldown","title":"Backend Health Check Failures"},{"location":"troubleshooting/common-issues/#load-balancing-issues","text":"","title":"Load Balancing Issues"},{"location":"troubleshooting/common-issues/#no-healthy-backends-available","text":"Error Messages : no healthy servers no servers configured for upstream Root Causes : - All backends failed health checks - Empty backend list for upstream - Backends in cooldown period after failures - Circuit breaker triggered Diagnostic Commands : # Check backend health status journalctl -u spooky | grep -E \"became healthy|became unhealthy\" | tail -20 # Monitor 503 Service Unavailable responses journalctl -u spooky | grep \"status 503\" # Count healthy vs total backends per upstream yq '.upstream[].backends | length' config.yaml # Check recent health transitions journalctl -u spooky --since \"5 minutes ago\" | grep \"Backend\" Resolution : # Verify backend services are running for backend in 10.0.1.10:8080 10.0.1.11:8080; do echo -n \"$backend: \" curl -s -o /dev/null -w \"%{http_code}\" http://$backend/health || echo \"FAIL\" echo done # Temporarily disable health checks for debugging # Set failure_threshold very high in config.yaml # Restart Spooky to reset health state sudo systemctl restart spooky","title":"No Healthy Backends Available"},{"location":"troubleshooting/common-issues/#uneven-load-distribution","text":"Symptoms : - One backend receives disproportionate traffic - Round-robin not cycling through backends - Consistent hash not distributing evenly Root Causes : - Backend weight misconfiguration - Inconsistent hash key selection (always same key) - Some backends marked unhealthy - Hash ring replica count too low for consistent-hash Diagnostic Commands : # Analyze backend selection distribution journalctl -u spooky | grep \"Selected backend\" | \\ awk '{print $(NF-2)}' | sort | uniq -c # Check backend weights yq '.upstream[].backends[] | \"\\(.id): \\(.weight)\"' config.yaml # Monitor load balancing algorithm journalctl -u spooky | grep \"via round-robin\\|via consistent-hash\\|via random\" # Check hash key consistency journalctl -u spooky | grep \"request_hash_key\" Resolution : # Ensure proper weight distribution backends: - id: backend1 address: \"10.0.1.10:8080\" weight: 100 - id: backend2 address: \"10.0.1.11:8080\" weight: 100 # Equal weight for even distribution # For consistent-hash, increase replica count # Edit lb/src/lib.rs: DEFAULT_REPLICAS = 128;","title":"Uneven Load Distribution"},{"location":"troubleshooting/common-issues/#performance-problems","text":"","title":"Performance Problems"},{"location":"troubleshooting/common-issues/#high-latency","text":"Symptoms : - latency_ms in logs consistently >1000ms - Slow response times reported by clients - Backend timeout errors (503 status) Root Causes : - Backend processing delay - Network congestion - Connection pool saturation - CPU saturation on Spooky host - Inefficient load balancing Diagnostic Commands : # Analyze latency distribution journalctl -u spooky --since \"1 hour ago\" | \\ grep \"latency_ms\" | \\ awk '{print $(NF)}' | \\ sort -n | \\ awk '{sum+=$1; arr[NR]=$1} END { print \"min:\", arr[1]; print \"p50:\", arr[int(NR*0.5)]; print \"p95:\", arr[int(NR*0.95)]; print \"p99:\", arr[int(NR*0.99)]; print \"max:\", arr[NR]; print \"avg:\", sum/NR; }' # Monitor CPU usage top -b -n 1 | grep spooky # Check connection pool contention journalctl -u spooky | grep \"semaphore\" | tail -20 # Measure backend response time directly time curl http://10.0.1.10:8080/api/test Resolution : # Increase backend timeout if backends are slow but reliable # Edit BACKEND_TIMEOUT in quic_listener.rs # Scale backend capacity # Add more backends to upstream pool # Increase connection pool size # Edit MAX_INFLIGHT_PER_BACKEND in quic_listener.rs # Optimize backend application # Profile and optimize backend code","title":"High Latency"},{"location":"troubleshooting/common-issues/#memory-growth","text":"Symptoms : - RSS memory continuously increasing - Out of memory errors - System swapping Root Causes : - Connection leak (connections not properly closed) - Request/response body buffering - Metrics accumulation - QUIC connection state not cleaned up Diagnostic Commands : # Monitor memory usage over time while true; do ps -p $(pgrep spooky) -o pid,vsz,rss,cmd | tail -1 sleep 10 done # Check connection count ss -u | grep -c :443 # Analyze memory map sudo pmap -x $(pgrep spooky) # Check for file descriptor leaks ls -l /proc/$(pgrep spooky)/fd | wc -l Resolution : # Restart Spooky periodically (temporary mitigation) sudo systemctl restart spooky # Monitor connection cleanup journalctl -u spooky -f | grep \"Connection closed\" # Reduce max idle timeout # Edit quic_config.set_max_idle_timeout(3000); # Limit connection count # Implement connection limit in accept logic","title":"Memory Growth"},{"location":"troubleshooting/common-issues/#udp-packet-loss","text":"Symptoms : - Retransmissions in QUIC logs - Client timeout errors - Degraded throughput Root Causes : - Network congestion - UDP buffer overflow (receive buffer too small) - Firewall dropping packets - MTU fragmentation Diagnostic Commands : # Check UDP buffer sizes sysctl net.core.rmem_max net.core.rmem_default sysctl net.core.wmem_max net.core.wmem_default # Monitor UDP statistics netstat -su | grep -E \"packet receive errors|receive buffer errors\" # Capture packet loss sudo tcpdump -i any -c 1000 udp port 443 -w capture.pcap tshark -r capture.pcap -q -z io,stat,1 # Check interface statistics ip -s link show eth0 Resolution : # Increase UDP buffer sizes sudo sysctl -w net.core.rmem_max=26214400 sudo sysctl -w net.core.wmem_max=26214400 sudo sysctl -w net.core.rmem_default=26214400 sudo sysctl -w net.core.wmem_default=26214400 # Make permanent echo \"net.core.rmem_max=26214400\" | sudo tee -a /etc/sysctl.conf echo \"net.core.wmem_max=26214400\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p # Reduce UDP payload size # Edit quic_config.set_max_recv_udp_payload_size(1350);","title":"UDP Packet Loss"},{"location":"troubleshooting/common-issues/#debugging-techniques","text":"","title":"Debugging Techniques"},{"location":"troubleshooting/common-issues/#enable-debug-logging","text":"# config.yaml log: level: haunt # debug level # Restart to apply sudo systemctl restart spooky # Monitor debug logs journalctl -u spooky -f --output=cat","title":"Enable Debug Logging"},{"location":"troubleshooting/common-issues/#analyze-request-flow","text":"# Trace specific request path journalctl -u spooky | grep -E \"HTTP/3 request|Selected backend|Upstream.*status\" | \\ grep \"/api/users\" # Monitor complete request lifecycle journalctl -u spooky -f | \\ grep -E \"Creating new connection|HTTP/3 request|Selected backend|status.*latency_ms|Connection closed\"","title":"Analyze Request Flow"},{"location":"troubleshooting/common-issues/#packet-capture-analysis","text":"# Capture QUIC traffic sudo tcpdump -i any -w spooky.pcap udp port 443 # Analyze with tshark tshark -r spooky.pcap -Y quic -T fields \\ -e frame.time -e ip.src -e ip.dst -e quic.header_form # Decrypt QUIC (requires SSLKEYLOGFILE) SSLKEYLOGFILE=/tmp/keys.log curl --http3 https://localhost:443/ tshark -r spooky.pcap -o tls.keylog_file:/tmp/keys.log -Y http3","title":"Packet Capture Analysis"},{"location":"troubleshooting/common-issues/#performance-profiling","text":"# CPU profiling with perf sudo perf record -F 99 -p $(pgrep spooky) -g -- sleep 30 sudo perf report --stdio | head -50 # Flamegraph generation sudo perf record -F 99 -p $(pgrep spooky) -g -- sleep 30 sudo perf script | stackcollapse-perf.pl | flamegraph.pl > flamegraph.svg # Memory profiling (if built with jemalloc) export MALLOC_CONF=prof:true,prof_prefix:/tmp/jeprof sudo systemctl restart spooky # Send traffic, then analyze with jeprof","title":"Performance Profiling"},{"location":"troubleshooting/common-issues/#common-error-reference","text":"Error Message HTTP Status Cause Resolution invalid request 400 Malformed HTTP/3 headers Check client request format no servers configured for upstream 503 Empty backend list Add backends to upstream config no healthy servers 503 All backends unhealthy Check backend health, adjust thresholds invalid server 503 Backend index out of bounds Configuration reload race condition upstream error 502 Backend connection failed Verify backend connectivity upstream timeout 503 Backend exceeded 2s timeout Increase BACKEND_TIMEOUT or optimize backend internal server error 500 TLS configuration error Check certificate/key files Wrong QUIC HEADER (dropped) Malformed QUIC packet Check for network corruption No route found for path (internal) No matching upstream route Add route configuration Upstream pool not found (internal) Pool initialization failure Check logs for startup errors","title":"Common Error Reference"},{"location":"troubleshooting/common-issues/#support-and-escalation","text":"When reporting issues, include: # 1. Version information spooky --version # 2. Configuration (sanitized) yq eval 'del(.listen.tls.key, .listen.tls.cert)' config.yaml # 3. System information uname -a cat /etc/os-release # 4. Error logs (last 100 lines) journalctl -u spooky --no-pager -n 100 --since \"1 hour ago\" # 5. Resource utilization ps aux | grep spooky ss -u | grep -c :443 free -h # 6. Network diagnostics ss -ulnp | grep spooky sudo iptables -L -n -v | grep 443 For production incidents, capture diagnostic bundle: #!/bin/bash mkdir -p spooky-diagnostics cd spooky-diagnostics spooky --version > version.txt uname -a > system.txt journalctl -u spooky --no-pager -n 500 > logs.txt yq eval 'del(.listen.tls.key, .listen.tls.cert)' ../config.yaml > config.yaml ps aux | grep spooky > processes.txt ss -tulnp > sockets.txt free -h > memory.txt sudo tcpdump -i any -c 100 -w capture.pcap udp port 443 cd .. tar czf spooky-diagnostics-$(date +%Y%m%d-%H%M%S).tar.gz spooky-diagnostics/","title":"Support and Escalation"},{"location":"tutorials/quickstart/","text":"This guide demonstrates how to deploy a working Spooky HTTP/3 proxy in under 10 minutes. You will set up a basic proxy configuration, generate self-signed certificates, and verify HTTP/3 connectivity to a backend service. Prerequisites Rust 1.85 or later installed (edition 2024) Basic familiarity with command-line tools An HTTP/2 backend service running locally (or use the example backend provided) UDP port 9889 available for QUIC traffic Step 1: Build Spooky Clone the repository and build the release binary: git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release The compiled binary will be located at target/release/spooky . Build time is typically 2-5 minutes depending on your system. Step 2: Generate Self-Signed Certificates QUIC requires TLS 1.3, so you need certificate and key files. For testing purposes, generate a self-signed certificate: mkdir -p certs openssl req -x509 -newkey rsa:4096 -nodes \\ -keyout certs/key.pem \\ -out certs/cert.pem \\ -days 365 \\ -subj \"/CN=localhost\" This creates: - certs/cert.pem : The TLS certificate - certs/key.pem : The private key Note: For production deployments, use certificates from a trusted Certificate Authority (CA). See TLS Configuration for production certificate setup. Step 3: Start a Test Backend Server You need an HTTP/2 backend for Spooky to forward traffic to. If you don't have one running, use the provided HTTP/2 test backend: # Using Spooky's built-in HTTP/2 test backend cargo run --bin h2_backend -- --port 8080 This starts an HTTP/2-only server on 127.0.0.1:8080 . Spooky requires HTTP/2 backends - HTTP/1.1 backends are not supported. Step 4: Create Configuration File Create a minimal configuration file named config.yaml : version: 1 listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"certs/cert.pem\" key: \"certs/key.pem\" upstream: default: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"local-backend\" address: \"127.0.0.1:8080\" weight: 100 health_check: path: \"/\" interval: 5000 timeout_ms: 2000 success_threshold: 2 failure_threshold: 3 log: level: info This configuration: - Listens for HTTP/3 connections on UDP port 9889 - Uses the generated self-signed certificates - Forwards all requests to 127.0.0.1:8080 using random load balancing - Performs health checks every 5 seconds on the backend Step 5: Start Spooky Launch the proxy with the configuration file: ./target/release/spooky --config config.yaml Expected output: [INFO] Loading configuration from config.yaml [INFO] Starting Spooky HTTP/3 proxy [INFO] Listening on 0.0.0.0:9889 (HTTP/3) [INFO] Backend local-backend (127.0.0.1:8080) marked healthy [INFO] Proxy started successfully The proxy is now accepting HTTP/3 connections on port 9889 and forwarding them to the backend on port 8080. Step 6: Test Connectivity Verify that HTTP/3 requests are being proxied correctly. You will need an HTTP/3-capable client such as curl with HTTP/3 support. Using curl with HTTP/3 If you have curl compiled with HTTP/3 support: curl --http3-only -k https://localhost:9889/ The -k flag bypasses certificate validation for self-signed certificates. You should see the response from your backend server. Using curl with Alt-Svc Discovery For a more realistic test that mimics browser behavior: curl -k \\ --resolve localhost:9889:127.0.0.1 \\ https://localhost:9889/ Verify HTTP/3 connectivity by forcing HTTP/3-only requests: curl -k --http3-only https://localhost:9889/ If successful, you should receive a response from your backend. HTTP/3 connectivity is confirmed when the request succeeds (Spooky doesn't advertise Alt-Svc headers). Using a Custom HTTP/3 Client If you don't have HTTP/3 support in curl, you can use other clients: Using h3i (HTTP/3 interactive client): cargo install h3i h3i https://localhost:9889/ --insecure Using qh3 (QUIC HTTP/3 client): git clone https://github.com/cloudflare/quiche.git cd quiche/tools/apps cargo build --release ./target/release/quiche-client https://localhost:9889/ --no-verify Step 7: Verify Backend Forwarding Check that requests are being forwarded to the backend. In the terminal running Spooky, you should see log entries indicating request handling: [INFO] QUIC connection established from 127.0.0.1:55420 [INFO] HTTP/3 stream 0: GET / [INFO] Forwarding to backend local-backend (127.0.0.1:8080) [INFO] Response 200 OK forwarded to client In the backend server terminal, verify that HTTP requests are being received. Step 8: Test Path-Based Routing (Optional) To demonstrate routing capabilities, modify the configuration to add multiple upstream pools: upstream: api_backend: load_balancing: type: \"round-robin\" route: path_prefix: \"/api\" backends: - id: \"api-server\" address: \"127.0.0.1:8001\" weight: 100 default_backend: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"default-server\" address: \"127.0.0.1:8080\" weight: 100 Restart Spooky with the updated configuration. Requests to /api/* will route to port 8001, while all other requests route to port 8080. Test the routing: # Routes to default backend (port 8080) curl --http3-only -k https://localhost:9889/ # Routes to API backend (port 8001) curl --http3-only -k https://localhost:9889/api/users Common Issues and Solutions Port Already in Use If port 9889 is already bound: Error: Address already in use (os error 98) Solution: Either stop the conflicting process or change the port in config.yaml . Backend Connection Refused If Spooky cannot connect to the backend: [ERROR] Failed to connect to backend local-backend: Connection refused Solution: Ensure the backend service is running on the configured address and port. Certificate Errors If the certificate is not found: [ERROR] Failed to load TLS certificate: No such file or directory Solution: Verify that the certificate paths in config.yaml are correct and the files exist. Health Check Failures If backends are marked unhealthy: [WARN] Backend local-backend health check failed: timeout Solution: Ensure the health check path exists on the backend and responds within the timeout period (default 2 seconds). Next Steps You now have a working HTTP/3 to HTTP/2 proxy. To further configure and optimize Spooky: Configuration Reference - Complete configuration options including advanced load balancing and routing TLS Setup - Configure production TLS certificates with Let's Encrypt or other CAs Load Balancing Guide - Understand different load balancing algorithms and when to use them Production Deployment - Best practices for production deployment including systemd integration and monitoring Troubleshooting - Solutions to common operational issues For HTTP/3 and QUIC protocol details: HTTP/3 Overview - HTTP/3 protocol implementation and differences from HTTP/2 QUIC Overview - QUIC transport protocol details and how Spooky uses it","title":"Quick Start"},{"location":"tutorials/quickstart/#prerequisites","text":"Rust 1.85 or later installed (edition 2024) Basic familiarity with command-line tools An HTTP/2 backend service running locally (or use the example backend provided) UDP port 9889 available for QUIC traffic","title":"Prerequisites"},{"location":"tutorials/quickstart/#step-1-build-spooky","text":"Clone the repository and build the release binary: git clone https://github.com/nishujangra/spooky.git cd spooky cargo build --release The compiled binary will be located at target/release/spooky . Build time is typically 2-5 minutes depending on your system.","title":"Step 1: Build Spooky"},{"location":"tutorials/quickstart/#step-2-generate-self-signed-certificates","text":"QUIC requires TLS 1.3, so you need certificate and key files. For testing purposes, generate a self-signed certificate: mkdir -p certs openssl req -x509 -newkey rsa:4096 -nodes \\ -keyout certs/key.pem \\ -out certs/cert.pem \\ -days 365 \\ -subj \"/CN=localhost\" This creates: - certs/cert.pem : The TLS certificate - certs/key.pem : The private key Note: For production deployments, use certificates from a trusted Certificate Authority (CA). See TLS Configuration for production certificate setup.","title":"Step 2: Generate Self-Signed Certificates"},{"location":"tutorials/quickstart/#step-3-start-a-test-backend-server","text":"You need an HTTP/2 backend for Spooky to forward traffic to. If you don't have one running, use the provided HTTP/2 test backend: # Using Spooky's built-in HTTP/2 test backend cargo run --bin h2_backend -- --port 8080 This starts an HTTP/2-only server on 127.0.0.1:8080 . Spooky requires HTTP/2 backends - HTTP/1.1 backends are not supported.","title":"Step 3: Start a Test Backend Server"},{"location":"tutorials/quickstart/#step-4-create-configuration-file","text":"Create a minimal configuration file named config.yaml : version: 1 listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"certs/cert.pem\" key: \"certs/key.pem\" upstream: default: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"local-backend\" address: \"127.0.0.1:8080\" weight: 100 health_check: path: \"/\" interval: 5000 timeout_ms: 2000 success_threshold: 2 failure_threshold: 3 log: level: info This configuration: - Listens for HTTP/3 connections on UDP port 9889 - Uses the generated self-signed certificates - Forwards all requests to 127.0.0.1:8080 using random load balancing - Performs health checks every 5 seconds on the backend","title":"Step 4: Create Configuration File"},{"location":"tutorials/quickstart/#step-5-start-spooky","text":"Launch the proxy with the configuration file: ./target/release/spooky --config config.yaml Expected output: [INFO] Loading configuration from config.yaml [INFO] Starting Spooky HTTP/3 proxy [INFO] Listening on 0.0.0.0:9889 (HTTP/3) [INFO] Backend local-backend (127.0.0.1:8080) marked healthy [INFO] Proxy started successfully The proxy is now accepting HTTP/3 connections on port 9889 and forwarding them to the backend on port 8080.","title":"Step 5: Start Spooky"},{"location":"tutorials/quickstart/#step-6-test-connectivity","text":"Verify that HTTP/3 requests are being proxied correctly. You will need an HTTP/3-capable client such as curl with HTTP/3 support.","title":"Step 6: Test Connectivity"},{"location":"tutorials/quickstart/#using-curl-with-http3","text":"If you have curl compiled with HTTP/3 support: curl --http3-only -k https://localhost:9889/ The -k flag bypasses certificate validation for self-signed certificates. You should see the response from your backend server.","title":"Using curl with HTTP/3"},{"location":"tutorials/quickstart/#using-curl-with-alt-svc-discovery","text":"For a more realistic test that mimics browser behavior: curl -k \\ --resolve localhost:9889:127.0.0.1 \\ https://localhost:9889/ Verify HTTP/3 connectivity by forcing HTTP/3-only requests: curl -k --http3-only https://localhost:9889/ If successful, you should receive a response from your backend. HTTP/3 connectivity is confirmed when the request succeeds (Spooky doesn't advertise Alt-Svc headers).","title":"Using curl with Alt-Svc Discovery"},{"location":"tutorials/quickstart/#using-a-custom-http3-client","text":"If you don't have HTTP/3 support in curl, you can use other clients: Using h3i (HTTP/3 interactive client): cargo install h3i h3i https://localhost:9889/ --insecure Using qh3 (QUIC HTTP/3 client): git clone https://github.com/cloudflare/quiche.git cd quiche/tools/apps cargo build --release ./target/release/quiche-client https://localhost:9889/ --no-verify","title":"Using a Custom HTTP/3 Client"},{"location":"tutorials/quickstart/#step-7-verify-backend-forwarding","text":"Check that requests are being forwarded to the backend. In the terminal running Spooky, you should see log entries indicating request handling: [INFO] QUIC connection established from 127.0.0.1:55420 [INFO] HTTP/3 stream 0: GET / [INFO] Forwarding to backend local-backend (127.0.0.1:8080) [INFO] Response 200 OK forwarded to client In the backend server terminal, verify that HTTP requests are being received.","title":"Step 7: Verify Backend Forwarding"},{"location":"tutorials/quickstart/#step-8-test-path-based-routing-optional","text":"To demonstrate routing capabilities, modify the configuration to add multiple upstream pools: upstream: api_backend: load_balancing: type: \"round-robin\" route: path_prefix: \"/api\" backends: - id: \"api-server\" address: \"127.0.0.1:8001\" weight: 100 default_backend: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"default-server\" address: \"127.0.0.1:8080\" weight: 100 Restart Spooky with the updated configuration. Requests to /api/* will route to port 8001, while all other requests route to port 8080. Test the routing: # Routes to default backend (port 8080) curl --http3-only -k https://localhost:9889/ # Routes to API backend (port 8001) curl --http3-only -k https://localhost:9889/api/users","title":"Step 8: Test Path-Based Routing (Optional)"},{"location":"tutorials/quickstart/#common-issues-and-solutions","text":"","title":"Common Issues and Solutions"},{"location":"tutorials/quickstart/#port-already-in-use","text":"If port 9889 is already bound: Error: Address already in use (os error 98) Solution: Either stop the conflicting process or change the port in config.yaml .","title":"Port Already in Use"},{"location":"tutorials/quickstart/#backend-connection-refused","text":"If Spooky cannot connect to the backend: [ERROR] Failed to connect to backend local-backend: Connection refused Solution: Ensure the backend service is running on the configured address and port.","title":"Backend Connection Refused"},{"location":"tutorials/quickstart/#certificate-errors","text":"If the certificate is not found: [ERROR] Failed to load TLS certificate: No such file or directory Solution: Verify that the certificate paths in config.yaml are correct and the files exist.","title":"Certificate Errors"},{"location":"tutorials/quickstart/#health-check-failures","text":"If backends are marked unhealthy: [WARN] Backend local-backend health check failed: timeout Solution: Ensure the health check path exists on the backend and responds within the timeout period (default 2 seconds).","title":"Health Check Failures"},{"location":"tutorials/quickstart/#next-steps","text":"You now have a working HTTP/3 to HTTP/2 proxy. To further configure and optimize Spooky: Configuration Reference - Complete configuration options including advanced load balancing and routing TLS Setup - Configure production TLS certificates with Let's Encrypt or other CAs Load Balancing Guide - Understand different load balancing algorithms and when to use them Production Deployment - Best practices for production deployment including systemd integration and monitoring Troubleshooting - Solutions to common operational issues For HTTP/3 and QUIC protocol details: HTTP/3 Overview - HTTP/3 protocol implementation and differences from HTTP/2 QUIC Overview - QUIC transport protocol details and how Spooky uses it","title":"Next Steps"},{"location":"user-guide/basics/","text":"This guide covers fundamental concepts and basic usage of Spooky, an HTTP/3 to HTTP/2 edge proxy. Architecture Overview Spooky operates as a protocol translation layer between HTTP/3 clients and HTTP/2 backend services: QUIC Connection Termination : Accepts incoming HTTP/3 requests over QUIC Protocol Translation : Converts HTTP/3 streams to HTTP/2 requests Load Balancing : Routes requests to backend servers based on configured algorithms Response Conversion : Translates backend HTTP/2 responses back to HTTP/3 Client Delivery : Returns responses to the client over QUIC Client (HTTP/3/QUIC) \u2192 Spooky Edge \u2192 Backend (HTTP/2) \u2193 Route Matching Load Balancing Health Checking Configuration Structure Minimal Configuration version: 1 listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"server.crt\" key: \"server.key\" upstream: default_pool: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"backend1\" address: \"127.0.0.1:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 log: level: info Configuration Sections listen : Defines the edge server configuration - protocol : Must be \"http3\" - port : UDP port for QUIC connections (default: 9889) - address : Bind address (default: \"0.0.0.0\") - tls.cert : Path to TLS certificate - tls.key : Path to TLS private key upstream : Named pools of backend servers with routing rules and load balancing configuration - Key: Arbitrary pool name for identification - load_balancing : Load balancing algorithm for this pool ( random , round-robin , consistent-hash ) - route : Routing criteria to match requests - backends : List of backend servers log : Logging configuration - level : Log verbosity (trace, debug, info, warn, error) - file.enabled : Write logs to a file instead of stderr (default: false) - file.path : Log file path (default: /var/log/spooky/spooky.log ) Upstream Pools and Routing Spooky supports multiple upstream pools with independent routing rules. Requests are matched using longest-prefix matching across all configured routes. Path-Based Routing upstream: api_pool: route: path_prefix: \"/api\" backends: - id: \"api1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 auth_pool: route: path_prefix: \"/auth\" backends: - id: \"auth1\" address: \"10.0.2.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 default_pool: route: path_prefix: \"/\" backends: - id: \"web1\" address: \"10.0.3.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 Routing Behavior : - Requests to /api/* \u2192 api_pool - Requests to /auth/* \u2192 auth_pool - All other requests \u2192 default_pool Host-Based Routing upstream: api_backend: route: host: \"api.example.com\" backends: - id: \"api1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 www_backend: route: host: \"www.example.com\" backends: - id: \"web1\" address: \"10.0.2.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 Combined Routing upstream: api_v2: route: host: \"api.example.com\" path_prefix: \"/v2\" backends: - id: \"api-v2-1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 Matches requests to api.example.com/v2/* . Backend Configuration Backend Parameters backends: - id: \"backend1\" # Unique identifier address: \"127.0.0.1:8080\" # Backend address (IP:port) weight: 100 # Relative weight for load balancing health_check: path: \"/health\" # Health check endpoint interval: 5000 # Check interval (milliseconds) timeout_ms: 2000 # Request timeout (milliseconds) failure_threshold: 3 # Consecutive failures before marking unhealthy success_threshold: 2 # Consecutive successes before marking healthy cooldown_ms: 5000 # Cooldown period after marking unhealthy Health Check Configuration interval : Time between health checks in milliseconds (default: 5000) timeout_ms : Maximum time to wait for health check response (default: 1000) failure_threshold : Number of consecutive failures required to mark backend unhealthy (default: 3) success_threshold : Number of consecutive successes required to mark backend healthy after being unhealthy (default: 2) cooldown_ms : Time to wait before attempting recovery after marking unhealthy (default: 5000) Health Check Implementation Backend services must implement health check endpoints that return 2xx status codes when healthy: // Node.js example app.get('/health', (req, res) => { // Verify critical dependencies if (database.isConnected() && cache.isReady()) { res.status(200).json({ status: 'healthy' }); } else { res.status(503).json({ status: 'unhealthy' }); } }); # Python example @app.route('/health') def health(): if check_database() and check_cache(): return {'status': 'healthy'}, 200 return {'status': 'unhealthy'}, 503 Command Line Interface Starting Spooky # Start with configuration file spooky --config config.yaml # Display version spooky --version Command Line Options Option Description Default --config Path to configuration file Required --version Display version information - --help Display help information - Note: Log level is configured in config.yaml ( log.level ) or via RUST_LOG environment variable. Configuration validation happens automatically during startup. Testing and Verification Testing with curl Requires curl built with HTTP/3 support (nghttp3 and ngtcp2): # Basic HTTP/3 request curl --http3-only -k https://localhost:9889/ # Test with custom host resolution curl --http3-only -k \\ --resolve example.com:9889:127.0.0.1 \\ https://example.com:9889/api/users # Test with headers curl --http3-only -k \\ -H \"Authorization: Bearer token\" \\ https://localhost:9889/protected # Verbose output for debugging curl --http3-only -k -v https://localhost:9889/ Load Balancing Verification # Generate concurrent requests for i in {1..20}; do curl --http3-only -k https://localhost:9889/ & done wait # Monitor backend request distribution (systemd) sudo journalctl -u spooky.service | grep \"routing to backend\" | \\ awk '{print $NF}' | sort | uniq -c # Monitor backend request distribution (if redirected to file) grep \"routing to backend\" /var/log/spooky/spooky.log | \\ awk '{print $NF}' | sort | uniq -c Health Check Monitoring # Monitor health check activity (systemd) sudo journalctl -u spooky.service -f | grep -i health # Monitor health check activity (direct process) spooky --config config.yaml 2>&1 | grep -i health # Check backend status (systemd) sudo journalctl -u spooky.service | grep \"backend.*healthy\" | tail -20 # Check backend status (if redirected to file) grep \"backend.*healthy\" /var/log/spooky/spooky.log | tail -20 Logging Log Levels Level Description Use Case trace Extremely verbose, includes protocol details Protocol debugging debug Detailed operational information Development, troubleshooting info General operational messages Production (default) warn Warning conditions Production error Error conditions Production Log Configuration # stderr (default) log: level: info # write to file log: level: info file: enabled: true path: /var/log/spooky/spooky.log Log Analysis systemd (log.file.enabled: false) # Follow logs in real-time sudo journalctl -u spooky.service -f # Filter by severity sudo journalctl -u spooky.service | grep ERROR # Search for specific requests sudo journalctl -u spooky.service | grep \"GET /api/users\" # Monitor backend selection sudo journalctl -u spooky.service | grep \"routing to backend\" # Track health check failures sudo journalctl -u spooky.service | grep \"health check failed\" File (log.file.enabled: true) # Follow logs in real-time tail -f /var/log/spooky/spooky.log # Filter by severity grep ERROR /var/log/spooky/spooky.log # Search for specific requests grep \"GET /api/users\" /var/log/spooky/spooky.log # Monitor backend selection grep \"routing to backend\" /var/log/spooky/spooky.log # Track health check failures grep \"health check failed\" /var/log/spooky/spooky.log Common Deployment Patterns Development Setup # Generate self-signed certificate openssl req -x509 -newkey rsa:2048 \\ -keyout server.key -out server.crt \\ -days 365 -nodes -subj \"/CN=localhost\" # Create development configuration cat > dev-config.yaml <<EOF version: 1 listen: protocol: http3 port: 9889 address: \"127.0.0.1\" tls: cert: \"server.crt\" key: \"server.key\" upstream: dev_pool: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"dev-backend\" address: \"127.0.0.1:3000\" weight: 100 health_check: path: \"/health\" interval: 5000 log: level: debug EOF # Start Spooky spooky --config dev-config.yaml Example Multi-Backend Setup Note: Spooky is experimental. The configuration below shows how a multi-backend setup would look, but is not a production deployment recommendation. # Obtain certificates (Let's Encrypt) certbot certonly --standalone -d example.com # Create production configuration cat > prod-config.yaml <<EOF version: 1 listen: protocol: http3 port: 443 address: \"0.0.0.0\" tls: cert: \"/etc/letsencrypt/live/example.com/fullchain.pem\" key: \"/etc/letsencrypt/live/example.com/privkey.pem\" upstream: prod_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/\" backends: - id: \"web-01\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 10000 timeout_ms: 3000 failure_threshold: 3 success_threshold: 2 cooldown_ms: 30000 - id: \"web-02\" address: \"10.0.1.11:8080\" weight: 100 health_check: path: \"/health\" interval: 10000 timeout_ms: 3000 failure_threshold: 3 success_threshold: 2 cooldown_ms: 30000 log: level: info EOF # Deploy as systemd service sudo systemctl start spooky sudo systemctl enable spooky Troubleshooting Connection Issues # Verify Spooky is listening on UDP sudo netstat -uln | grep 9889 # or sudo ss -uln | grep 9889 # Check firewall configuration sudo ufw status sudo iptables -L -n -v | grep 9889 # Test UDP connectivity nc -u -v -z localhost 9889 # Verify TLS certificate openssl s_client -connect localhost:9889 -showcerts Backend Connectivity # Test backend directly curl -v http://127.0.0.1:8080/health # Test through Spooky curl --http3-only -k -v https://localhost:9889/health # Check backend reachability from Spooky host telnet 10.0.1.10 8080 Configuration Validation # Validate configuration syntax (startup validation happens before serving) spooky --config config.yaml # Check for YAML syntax errors yamllint config.yaml # Verify routing configuration grep -A 10 \"route:\" config.yaml Performance Debugging # Monitor system resources top -p $(pgrep spooky) htop -p $(pgrep spooky) # Check UDP buffer statistics netstat -su | grep Udp # Monitor QUIC connections ss -u -a | grep 9889 # Check for packet loss netstat -s | grep -i lost Next Steps Review Load Balancing for detailed algorithm documentation Refer to Configuration Reference for complete parameter documentation See Deployment Guide for production deployment best practices","title":"Basics"},{"location":"user-guide/basics/#architecture-overview","text":"Spooky operates as a protocol translation layer between HTTP/3 clients and HTTP/2 backend services: QUIC Connection Termination : Accepts incoming HTTP/3 requests over QUIC Protocol Translation : Converts HTTP/3 streams to HTTP/2 requests Load Balancing : Routes requests to backend servers based on configured algorithms Response Conversion : Translates backend HTTP/2 responses back to HTTP/3 Client Delivery : Returns responses to the client over QUIC Client (HTTP/3/QUIC) \u2192 Spooky Edge \u2192 Backend (HTTP/2) \u2193 Route Matching Load Balancing Health Checking","title":"Architecture Overview"},{"location":"user-guide/basics/#configuration-structure","text":"","title":"Configuration Structure"},{"location":"user-guide/basics/#minimal-configuration","text":"version: 1 listen: protocol: http3 port: 9889 address: \"0.0.0.0\" tls: cert: \"server.crt\" key: \"server.key\" upstream: default_pool: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"backend1\" address: \"127.0.0.1:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 log: level: info","title":"Minimal Configuration"},{"location":"user-guide/basics/#configuration-sections","text":"listen : Defines the edge server configuration - protocol : Must be \"http3\" - port : UDP port for QUIC connections (default: 9889) - address : Bind address (default: \"0.0.0.0\") - tls.cert : Path to TLS certificate - tls.key : Path to TLS private key upstream : Named pools of backend servers with routing rules and load balancing configuration - Key: Arbitrary pool name for identification - load_balancing : Load balancing algorithm for this pool ( random , round-robin , consistent-hash ) - route : Routing criteria to match requests - backends : List of backend servers log : Logging configuration - level : Log verbosity (trace, debug, info, warn, error) - file.enabled : Write logs to a file instead of stderr (default: false) - file.path : Log file path (default: /var/log/spooky/spooky.log )","title":"Configuration Sections"},{"location":"user-guide/basics/#upstream-pools-and-routing","text":"Spooky supports multiple upstream pools with independent routing rules. Requests are matched using longest-prefix matching across all configured routes.","title":"Upstream Pools and Routing"},{"location":"user-guide/basics/#path-based-routing","text":"upstream: api_pool: route: path_prefix: \"/api\" backends: - id: \"api1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 auth_pool: route: path_prefix: \"/auth\" backends: - id: \"auth1\" address: \"10.0.2.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 default_pool: route: path_prefix: \"/\" backends: - id: \"web1\" address: \"10.0.3.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 Routing Behavior : - Requests to /api/* \u2192 api_pool - Requests to /auth/* \u2192 auth_pool - All other requests \u2192 default_pool","title":"Path-Based Routing"},{"location":"user-guide/basics/#host-based-routing","text":"upstream: api_backend: route: host: \"api.example.com\" backends: - id: \"api1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 www_backend: route: host: \"www.example.com\" backends: - id: \"web1\" address: \"10.0.2.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000","title":"Host-Based Routing"},{"location":"user-guide/basics/#combined-routing","text":"upstream: api_v2: route: host: \"api.example.com\" path_prefix: \"/v2\" backends: - id: \"api-v2-1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 Matches requests to api.example.com/v2/* .","title":"Combined Routing"},{"location":"user-guide/basics/#backend-configuration","text":"","title":"Backend Configuration"},{"location":"user-guide/basics/#backend-parameters","text":"backends: - id: \"backend1\" # Unique identifier address: \"127.0.0.1:8080\" # Backend address (IP:port) weight: 100 # Relative weight for load balancing health_check: path: \"/health\" # Health check endpoint interval: 5000 # Check interval (milliseconds) timeout_ms: 2000 # Request timeout (milliseconds) failure_threshold: 3 # Consecutive failures before marking unhealthy success_threshold: 2 # Consecutive successes before marking healthy cooldown_ms: 5000 # Cooldown period after marking unhealthy","title":"Backend Parameters"},{"location":"user-guide/basics/#health-check-configuration","text":"interval : Time between health checks in milliseconds (default: 5000) timeout_ms : Maximum time to wait for health check response (default: 1000) failure_threshold : Number of consecutive failures required to mark backend unhealthy (default: 3) success_threshold : Number of consecutive successes required to mark backend healthy after being unhealthy (default: 2) cooldown_ms : Time to wait before attempting recovery after marking unhealthy (default: 5000)","title":"Health Check Configuration"},{"location":"user-guide/basics/#health-check-implementation","text":"Backend services must implement health check endpoints that return 2xx status codes when healthy: // Node.js example app.get('/health', (req, res) => { // Verify critical dependencies if (database.isConnected() && cache.isReady()) { res.status(200).json({ status: 'healthy' }); } else { res.status(503).json({ status: 'unhealthy' }); } }); # Python example @app.route('/health') def health(): if check_database() and check_cache(): return {'status': 'healthy'}, 200 return {'status': 'unhealthy'}, 503","title":"Health Check Implementation"},{"location":"user-guide/basics/#command-line-interface","text":"","title":"Command Line Interface"},{"location":"user-guide/basics/#starting-spooky","text":"# Start with configuration file spooky --config config.yaml # Display version spooky --version","title":"Starting Spooky"},{"location":"user-guide/basics/#command-line-options","text":"Option Description Default --config Path to configuration file Required --version Display version information - --help Display help information - Note: Log level is configured in config.yaml ( log.level ) or via RUST_LOG environment variable. Configuration validation happens automatically during startup.","title":"Command Line Options"},{"location":"user-guide/basics/#testing-and-verification","text":"","title":"Testing and Verification"},{"location":"user-guide/basics/#testing-with-curl","text":"Requires curl built with HTTP/3 support (nghttp3 and ngtcp2): # Basic HTTP/3 request curl --http3-only -k https://localhost:9889/ # Test with custom host resolution curl --http3-only -k \\ --resolve example.com:9889:127.0.0.1 \\ https://example.com:9889/api/users # Test with headers curl --http3-only -k \\ -H \"Authorization: Bearer token\" \\ https://localhost:9889/protected # Verbose output for debugging curl --http3-only -k -v https://localhost:9889/","title":"Testing with curl"},{"location":"user-guide/basics/#load-balancing-verification","text":"# Generate concurrent requests for i in {1..20}; do curl --http3-only -k https://localhost:9889/ & done wait # Monitor backend request distribution (systemd) sudo journalctl -u spooky.service | grep \"routing to backend\" | \\ awk '{print $NF}' | sort | uniq -c # Monitor backend request distribution (if redirected to file) grep \"routing to backend\" /var/log/spooky/spooky.log | \\ awk '{print $NF}' | sort | uniq -c","title":"Load Balancing Verification"},{"location":"user-guide/basics/#health-check-monitoring","text":"# Monitor health check activity (systemd) sudo journalctl -u spooky.service -f | grep -i health # Monitor health check activity (direct process) spooky --config config.yaml 2>&1 | grep -i health # Check backend status (systemd) sudo journalctl -u spooky.service | grep \"backend.*healthy\" | tail -20 # Check backend status (if redirected to file) grep \"backend.*healthy\" /var/log/spooky/spooky.log | tail -20","title":"Health Check Monitoring"},{"location":"user-guide/basics/#logging","text":"","title":"Logging"},{"location":"user-guide/basics/#log-levels","text":"Level Description Use Case trace Extremely verbose, includes protocol details Protocol debugging debug Detailed operational information Development, troubleshooting info General operational messages Production (default) warn Warning conditions Production error Error conditions Production","title":"Log Levels"},{"location":"user-guide/basics/#log-configuration","text":"# stderr (default) log: level: info # write to file log: level: info file: enabled: true path: /var/log/spooky/spooky.log","title":"Log Configuration"},{"location":"user-guide/basics/#log-analysis","text":"systemd (log.file.enabled: false) # Follow logs in real-time sudo journalctl -u spooky.service -f # Filter by severity sudo journalctl -u spooky.service | grep ERROR # Search for specific requests sudo journalctl -u spooky.service | grep \"GET /api/users\" # Monitor backend selection sudo journalctl -u spooky.service | grep \"routing to backend\" # Track health check failures sudo journalctl -u spooky.service | grep \"health check failed\" File (log.file.enabled: true) # Follow logs in real-time tail -f /var/log/spooky/spooky.log # Filter by severity grep ERROR /var/log/spooky/spooky.log # Search for specific requests grep \"GET /api/users\" /var/log/spooky/spooky.log # Monitor backend selection grep \"routing to backend\" /var/log/spooky/spooky.log # Track health check failures grep \"health check failed\" /var/log/spooky/spooky.log","title":"Log Analysis"},{"location":"user-guide/basics/#common-deployment-patterns","text":"","title":"Common Deployment Patterns"},{"location":"user-guide/basics/#development-setup","text":"# Generate self-signed certificate openssl req -x509 -newkey rsa:2048 \\ -keyout server.key -out server.crt \\ -days 365 -nodes -subj \"/CN=localhost\" # Create development configuration cat > dev-config.yaml <<EOF version: 1 listen: protocol: http3 port: 9889 address: \"127.0.0.1\" tls: cert: \"server.crt\" key: \"server.key\" upstream: dev_pool: load_balancing: type: \"random\" route: path_prefix: \"/\" backends: - id: \"dev-backend\" address: \"127.0.0.1:3000\" weight: 100 health_check: path: \"/health\" interval: 5000 log: level: debug EOF # Start Spooky spooky --config dev-config.yaml","title":"Development Setup"},{"location":"user-guide/basics/#example-multi-backend-setup","text":"Note: Spooky is experimental. The configuration below shows how a multi-backend setup would look, but is not a production deployment recommendation. # Obtain certificates (Let's Encrypt) certbot certonly --standalone -d example.com # Create production configuration cat > prod-config.yaml <<EOF version: 1 listen: protocol: http3 port: 443 address: \"0.0.0.0\" tls: cert: \"/etc/letsencrypt/live/example.com/fullchain.pem\" key: \"/etc/letsencrypt/live/example.com/privkey.pem\" upstream: prod_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/\" backends: - id: \"web-01\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 10000 timeout_ms: 3000 failure_threshold: 3 success_threshold: 2 cooldown_ms: 30000 - id: \"web-02\" address: \"10.0.1.11:8080\" weight: 100 health_check: path: \"/health\" interval: 10000 timeout_ms: 3000 failure_threshold: 3 success_threshold: 2 cooldown_ms: 30000 log: level: info EOF # Deploy as systemd service sudo systemctl start spooky sudo systemctl enable spooky","title":"Example Multi-Backend Setup"},{"location":"user-guide/basics/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/basics/#connection-issues","text":"# Verify Spooky is listening on UDP sudo netstat -uln | grep 9889 # or sudo ss -uln | grep 9889 # Check firewall configuration sudo ufw status sudo iptables -L -n -v | grep 9889 # Test UDP connectivity nc -u -v -z localhost 9889 # Verify TLS certificate openssl s_client -connect localhost:9889 -showcerts","title":"Connection Issues"},{"location":"user-guide/basics/#backend-connectivity","text":"# Test backend directly curl -v http://127.0.0.1:8080/health # Test through Spooky curl --http3-only -k -v https://localhost:9889/health # Check backend reachability from Spooky host telnet 10.0.1.10 8080","title":"Backend Connectivity"},{"location":"user-guide/basics/#configuration-validation","text":"# Validate configuration syntax (startup validation happens before serving) spooky --config config.yaml # Check for YAML syntax errors yamllint config.yaml # Verify routing configuration grep -A 10 \"route:\" config.yaml","title":"Configuration Validation"},{"location":"user-guide/basics/#performance-debugging","text":"# Monitor system resources top -p $(pgrep spooky) htop -p $(pgrep spooky) # Check UDP buffer statistics netstat -su | grep Udp # Monitor QUIC connections ss -u -a | grep 9889 # Check for packet loss netstat -s | grep -i lost","title":"Performance Debugging"},{"location":"user-guide/basics/#next-steps","text":"Review Load Balancing for detailed algorithm documentation Refer to Configuration Reference for complete parameter documentation See Deployment Guide for production deployment best practices","title":"Next Steps"},{"location":"user-guide/load-balancing/","text":"Comprehensive guide to load balancing algorithms, health checking, and backend management in Spooky. Load Balancing Algorithms Spooky implements three load balancing algorithms, each optimized for different use cases. Each upstream pool configures its own algorithm independently via load_balancing.type . Round Robin Algorithm : Sequential distribution across healthy backends in a circular pattern. Configuration : upstream: api_pool: load_balancing: type: \"round-robin\" # Accepts: round-robin, round_robin, rr route: path_prefix: \"/api\" backends: - id: \"backend1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 - id: \"backend2\" address: \"10.0.1.11:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 Characteristics : - Sequential, predictable distribution pattern - State maintained across requests (counter increments per request) - Equal distribution when all backends have equal weight - Automatically skips unhealthy backends - Counter wraps on overflow (no reset on restart) Use Cases : - Stateless applications requiring even distribution - Backends with equal capacity and performance - Scenarios where predictable patterns are acceptable - General purpose load balancing Performance : Very low overhead (simple counter increment) Consistent Hashing Algorithm : Hash-based routing using a consistent hash ring with virtual nodes. Configuration : upstream: api_pool: load_balancing: type: \"consistent-hash\" # Accepts: consistent-hash, consistent_hash, ch # key: \"header:x-user-id\" # Planned feature: configurable hash key source route: path_prefix: \"/api\" backends: - id: \"backend1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 - id: \"backend2\" address: \"10.0.1.11:8080\" weight: 200 # 2x virtual nodes = 2x traffic share health_check: path: \"/health\" interval: 5000 Characteristics : - Deterministic routing based on hash key - Same key always routes to same backend (session affinity) - Uses 64 virtual replicas per backend per weight unit - Minimal request redistribution when backends change - FNV-1a hash function for distribution - Automatically skips unhealthy backends Hash Key Sources : Currently, the key parameter is configured but hash key extraction must be implemented in the proxy layer. The algorithm accepts any string key. # Current behavior: fixed key derivation from request load_balancing: type: \"consistent-hash\" # Planned configurable key sources (not currently implemented): # key: \"header:x-user-id\" # User ID from header # key: \"header:x-session-id\" # Session ID from header # key: \"cookie:session_id\" # Session cookie # key: \"query:user_id\" # Query parameter # key: \"path\" # Request path Use Cases : - Applications requiring session affinity - Cache locality optimization (same keys hit same backend caches) - Stateful applications without external session store - Minimizing cache misses during backend changes Performance : Low overhead (hash computation + BTreeMap lookup) Random Algorithm : Random selection from healthy backends. Configuration : upstream: api_pool: load_balancing: type: \"random\" route: path_prefix: \"/api\" backends: - id: \"backend1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 - id: \"backend2\" address: \"10.0.1.11:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 Characteristics : - Non-deterministic selection using thread-local RNG - No state maintained between requests - Statistically even distribution over time - No session affinity - Automatically skips unhealthy backends Use Cases : - Stateless applications - High-throughput scenarios where simplicity matters - Testing and development - Avoiding predictable patterns for security Performance : Very low overhead (random number generation) Algorithm Comparison Algorithm Complexity Session Affinity State Distribution Use Case Round Robin O(1) No Counter Even General purpose, predictable load Consistent Hash O(log n) Yes Hash ring Even (with weight) Session affinity, cache locality Random O(1) No None Statistically even Stateless, high throughput Backend Weighting Only consistent hashing respects backend weights. Round-robin and random algorithms currently ignore weights (weighted versions are planned for future release). Weight Configuration backends: - id: \"small-instance\" address: \"10.0.1.10:8080\" weight: 50 # Receives 1x traffic health_check: path: \"/health\" interval: 5000 - id: \"medium-instance\" address: \"10.0.1.11:8080\" weight: 100 # Receives 2x traffic (2x the first) health_check: path: \"/health\" interval: 5000 - id: \"large-instance\" address: \"10.0.1.12:8080\" weight: 200 # Receives 4x traffic (4x the first) health_check: path: \"/health\" interval: 5000 Weight Behavior : - Round Robin : Weight values are currently ignored (weighted round-robin planned for future release) - Consistent Hash : Number of virtual nodes = replicas \u00d7 weight (64 replicas per weight unit) - Random : Weight values are currently ignored (weighted random planned for future release) - Minimum : Weight values below 1 are clamped to 1 Health Checking Spooky performs active health checks on all backends. Unhealthy backends are automatically removed from rotation. Health Check Mechanism backends: - id: \"backend1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" # Health check endpoint path interval: 5000 # Check every 5 seconds timeout_ms: 2000 # 2 second timeout per check failure_threshold: 3 # 3 failures \u2192 mark unhealthy success_threshold: 2 # 2 successes \u2192 mark healthy cooldown_ms: 10000 # 10 second cooldown before recovery Health Check Parameters path : Endpoint to check (default: \"/health\") - Must return 2xx status code when healthy - Should be lightweight and fast interval : Time between checks in milliseconds (default: 5000) - Lower values = faster failure detection - Higher values = lower overhead timeout_ms : Maximum wait time for response (default: 1000) - Should be less than interval - Failed on timeout failure_threshold : Consecutive failures to mark unhealthy (default: 3) - Higher values = more tolerance for transient failures - Lower values = faster failure detection success_threshold : Consecutive successes to mark healthy (default: 2) - Higher values = more confidence before recovery - Lower values = faster recovery cooldown_ms : Minimum time to stay unhealthy (default: 5000) - Prevents flapping - Gives backend time to recover Health State Machine Initial State: Healthy | | failure_threshold consecutive failures v Unhealthy (cooldown period) | | cooldown expires v Unhealthy (testing) | | success_threshold consecutive successes v Healthy Healthy : Backend receives traffic, failures are counted Unhealthy (cooldown) : Backend removed from rotation, health checks continue but successes are ignored until cooldown expires Unhealthy (testing) : Backend still removed from rotation, consecutive successes are counted toward success_threshold Backend State Tracking The load balancer tracks per-backend state: struct BackendState { address: String, weight: u32, health_check: HealthCheck, consecutive_failures: u32, health_state: HealthState, // Healthy | Unhealthy { until, successes } } State transitions: - record_success() : Increments success counter, transitions to Healthy if threshold met - record_failure() : Increments failure counter, transitions to Unhealthy if threshold met Multiple Upstream Pools Spooky supports multiple upstream pools with independent routing and load balancing configuration. Each pool specifies its own algorithm. Configuration Example upstream: api_pool: load_balancing: type: \"consistent-hash\" # Session affinity for API requests route: path_prefix: \"/api\" backends: - id: \"api1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 - id: \"api2\" address: \"10.0.1.11:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 auth_pool: load_balancing: type: \"round-robin\" # Even spread across auth backends route: path_prefix: \"/auth\" backends: - id: \"auth1\" address: \"10.0.2.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 static_pool: load_balancing: type: \"random\" # Stateless static assets, any backend is fine route: path_prefix: \"/static\" backends: - id: \"cdn1\" address: \"10.0.3.10:8080\" weight: 100 health_check: path: \"/health\" interval: 10000 Route Matching : - Routes are evaluated by longest-prefix matching - Route with most specific (longest) path prefix wins - For equal-length prefixes, selection depends on HashMap iteration order - Important : Unmatched routes return an error - configure a catch-all upstream (e.g., path_prefix: \"/\" ) to handle all other requests Monitoring and Observability Logging Health check events are logged: [INFO] Backend backend1 health check passed [WARN] Backend backend2 health check failed: connection timeout [INFO] Backend backend2 marked unhealthy after 3 consecutive failures [INFO] Backend backend2 marked healthy after 2 consecutive successes [DEBUG] Routing request to backend backend1 (round-robin) [DEBUG] Routing request to backend backend2 (consistent-hash, key=user:123) Health Check Monitoring # Monitor health check activity (systemd) sudo journalctl -u spooky.service -f | grep -i health # Monitor health check activity (direct process) spooky --config config.yaml 2>&1 | grep -i health # Count backend state changes (systemd) sudo journalctl -u spooky.service | grep \"marked unhealthy\\|marked healthy\" | tail -20 # Count backend state changes (if redirected to file) grep \"marked unhealthy\\|marked healthy\" /var/log/spooky/spooky.log | tail -20 # Track specific backend (systemd) sudo journalctl -u spooky.service | grep \"backend1\" | grep health # Track specific backend (if redirected to file) grep \"backend1\" /var/log/spooky/spooky.log | grep health Load Distribution Analysis # Extract backend selection counts (systemd) sudo journalctl -u spooky.service | grep \"routing to backend\" | \\ awk '{print $NF}' | sort | uniq -c | sort -rn # Monitor routing decisions in real-time (systemd) sudo journalctl -u spooky.service -f | grep \"routing to backend\" # If redirected to file grep \"routing to backend\" /var/log/spooky/spooky.log | \\ awk '{print $NF}' | sort | uniq -c | sort -rn Performance Considerations Algorithm Performance Algorithm Time Complexity Memory per Backend Per-Request Cost Round Robin O(1) ~8 bytes Counter increment Consistent Hash O(log n) ~4 KB (64 replicas \u00d7 weight) Hash + BTreeMap lookup Random O(1) ~0 bytes RNG call n = number of healthy backends Backend Pool Operations // All operations filter to healthy backends first healthy_indices() // O(n) - scans all backends pick_backend(algorithm) // O(1) or O(log n) depending on algorithm mark_success(index) // O(1) - direct index access mark_failure(index) // O(1) - direct index access Scalability Backend Count : Algorithms scale to hundreds of backends Health Checks : Run asynchronously, do not block request path Memory : ~8-12 KB per backend (including health state and hash ring) CPU : Minimal overhead for all algorithms Troubleshooting Uneven Load Distribution Symptoms : Some backends receive disproportionate traffic Diagnosis : # Check backend weights grep -A 10 \"backends:\" config.yaml | grep -E \"id|weight\" # Monitor actual distribution (systemd) sudo journalctl -u spooky.service | grep \"routing to backend\" | \\ awk '{print $NF}' | sort | uniq -c # If redirected to file grep \"routing to backend\" /var/log/spooky/spooky.log | \\ awk '{print $NF}' | sort | uniq -c # Verify all backends are healthy (systemd) sudo journalctl -u spooky.service | grep \"healthy\" | tail -20 # If redirected to file grep \"healthy\" /var/log/spooky/spooky.log | tail -20 Solutions : - Verify backend weights are configured correctly - Check for unhealthy backends (temporarily removed from rotation) - For consistent-hash, verify hash keys are well-distributed - For round-robin, ensure sufficient request volume for even distribution Session Affinity Not Working Symptoms : Requests from same user/session hit different backends Diagnosis : # Verify consistent-hash configuration grep -A 5 \"load_balancing:\" config.yaml | grep -E \"type|key\" # Check if hash key is present in requests (systemd) sudo journalctl -u spooky.service -f | grep \"consistent-hash\" # Check if hash key is present in requests (if redirected to file) tail -f /var/log/spooky/spooky.log | grep \"consistent-hash\" # Test with known hash key curl --http3-only -H \"X-User-ID: test123\" https://localhost:9889/ Solutions : - Ensure load_balancing.type is \"consistent-hash\" - Note: Hash key is automatically derived from request (authority \u2192 path \u2192 method) - For session affinity, ensure requests include consistent authority or path components - Configurable key sources are planned for future implementation Frequent Health Check Failures Symptoms : Backends repeatedly marked unhealthy despite being functional Diagnosis : # Monitor health check failures (systemd) sudo journalctl -u spooky.service -f | grep \"health check failed\" # If redirected to file tail -f /var/log/spooky/spooky.log | grep \"health check failed\" # Test health endpoint directly curl -v http://10.0.1.10:8080/health # Check response time time curl http://10.0.1.10:8080/health # Verify network connectivity ping 10.0.1.10 traceroute 10.0.1.10 Solutions : - Increase health check timeout (timeout_ms) if endpoint is slow - Increase failure_threshold to tolerate transient failures - Optimize backend health endpoint performance - Check network latency and packet loss - Verify health check path is correct Backends Not Recovering Symptoms : Healthy backends remain marked unhealthy Diagnosis : # Check cooldown period grep -A 10 \"health_check:\" config.yaml | grep cooldown_ms # Monitor recovery attempts (systemd) sudo journalctl -u spooky.service -f | grep -E \"marked healthy|success\" # If redirected to file tail -f /var/log/spooky/spooky.log | grep -E \"marked healthy|success\" # Verify backend is actually healthy curl http://10.0.1.10:8080/health Solutions : - Reduce cooldown_ms for faster recovery - Reduce success_threshold if being too conservative - Verify backend health endpoint returns 2xx status - Check for health check timeout issues No Backends Available Symptoms : All backends marked unhealthy, requests fail Diagnosis : # List all backend states (systemd) sudo journalctl -u spooky.service | grep -i \"backend.*health\" | tail -20 # If redirected to file grep -i \"backend.*health\" /var/log/spooky/spooky.log | tail -20 # Check configuration grep -A 15 \"backends:\" config.yaml # Test each backend directly for backend in 10.0.1.10:8080 10.0.1.11:8080; do echo \"Testing $backend\" curl -v http://$backend/health done Solutions : - Fix backend health endpoints - Adjust health check parameters (increase timeout, threshold) - Verify backends are actually running and accessible - Check firewall rules between Spooky and backends Best Practices Health Check Configuration Set timeout_ms < interval to prevent check pileup Use failure_threshold \u2265 3 to avoid false positives Set cooldown_ms \u2265 10000 to prevent flapping Keep health endpoints lightweight (< 100ms response time) Algorithm Selection Use Round Robin for simple, even distribution with equal backends Use Consistent Hash when session affinity or cache locality matters Use Random for stateless, high-throughput scenarios Weight Configuration Base weights on backend capacity (CPU, memory, network) Start with equal weights, adjust based on monitoring Use relative weights (100, 200, 400) rather than absolute For consistent-hash, remember: weight \u00d7 64 = number of virtual nodes Upstream Pool Design Create separate pools for different services (API, auth, static) Order routes from most specific to least specific Use path_prefix for path-based routing Use host for virtual host routing Provide a catch-all default pool Advanced Configuration Examples Multi-Tier Application # Global load balancing strategy (applies to all upstream pools) # Per-upstream load_balancing is planned but not currently active load_balancing: type: \"round-robin\" upstream: api_tier: route: path_prefix: \"/api\" backends: - id: \"api1\" address: \"10.0.1.10:8080\" weight: 200 health_check: path: \"/health\" interval: 5000 timeout_ms: 2000 failure_threshold: 3 success_threshold: 2 cooldown_ms: 15000 auth_tier: route: path_prefix: \"/auth\" backends: - id: \"auth1\" address: \"10.0.2.10:8080\" weight: 100 health_check: path: \"/health\" interval: 3000 timeout_ms: 1000 failure_threshold: 2 success_threshold: 2 cooldown_ms: 10000 static_tier: route: path_prefix: \"/static\" backends: - id: \"cdn1\" address: \"10.0.3.10:8080\" weight: 100 health_check: path: \"/health\" interval: 30000 timeout_ms: 5000 failure_threshold: 5 success_threshold: 1 cooldown_ms: 60000 Heterogeneous Backend Capacities upstream: prod_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/\" backends: - id: \"small-1\" address: \"10.0.1.10:8080\" weight: 50 # 2 vCPU, 4 GB RAM health_check: path: \"/health\" interval: 5000 - id: \"medium-1\" address: \"10.0.1.11:8080\" weight: 100 # 4 vCPU, 8 GB RAM health_check: path: \"/health\" interval: 5000 - id: \"large-1\" address: \"10.0.1.12:8080\" weight: 200 # 8 vCPU, 16 GB RAM health_check: path: \"/health\" interval: 5000 Related Documentation See Basics for general configuration and deployment Refer to Configuration Reference for complete parameter documentation See Architecture Guide for internal implementation details","title":"Load Balancing"},{"location":"user-guide/load-balancing/#load-balancing-algorithms","text":"Spooky implements three load balancing algorithms, each optimized for different use cases. Each upstream pool configures its own algorithm independently via load_balancing.type .","title":"Load Balancing Algorithms"},{"location":"user-guide/load-balancing/#round-robin","text":"Algorithm : Sequential distribution across healthy backends in a circular pattern. Configuration : upstream: api_pool: load_balancing: type: \"round-robin\" # Accepts: round-robin, round_robin, rr route: path_prefix: \"/api\" backends: - id: \"backend1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 - id: \"backend2\" address: \"10.0.1.11:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 Characteristics : - Sequential, predictable distribution pattern - State maintained across requests (counter increments per request) - Equal distribution when all backends have equal weight - Automatically skips unhealthy backends - Counter wraps on overflow (no reset on restart) Use Cases : - Stateless applications requiring even distribution - Backends with equal capacity and performance - Scenarios where predictable patterns are acceptable - General purpose load balancing Performance : Very low overhead (simple counter increment)","title":"Round Robin"},{"location":"user-guide/load-balancing/#consistent-hashing","text":"Algorithm : Hash-based routing using a consistent hash ring with virtual nodes. Configuration : upstream: api_pool: load_balancing: type: \"consistent-hash\" # Accepts: consistent-hash, consistent_hash, ch # key: \"header:x-user-id\" # Planned feature: configurable hash key source route: path_prefix: \"/api\" backends: - id: \"backend1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 - id: \"backend2\" address: \"10.0.1.11:8080\" weight: 200 # 2x virtual nodes = 2x traffic share health_check: path: \"/health\" interval: 5000 Characteristics : - Deterministic routing based on hash key - Same key always routes to same backend (session affinity) - Uses 64 virtual replicas per backend per weight unit - Minimal request redistribution when backends change - FNV-1a hash function for distribution - Automatically skips unhealthy backends Hash Key Sources : Currently, the key parameter is configured but hash key extraction must be implemented in the proxy layer. The algorithm accepts any string key. # Current behavior: fixed key derivation from request load_balancing: type: \"consistent-hash\" # Planned configurable key sources (not currently implemented): # key: \"header:x-user-id\" # User ID from header # key: \"header:x-session-id\" # Session ID from header # key: \"cookie:session_id\" # Session cookie # key: \"query:user_id\" # Query parameter # key: \"path\" # Request path Use Cases : - Applications requiring session affinity - Cache locality optimization (same keys hit same backend caches) - Stateful applications without external session store - Minimizing cache misses during backend changes Performance : Low overhead (hash computation + BTreeMap lookup)","title":"Consistent Hashing"},{"location":"user-guide/load-balancing/#random","text":"Algorithm : Random selection from healthy backends. Configuration : upstream: api_pool: load_balancing: type: \"random\" route: path_prefix: \"/api\" backends: - id: \"backend1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 - id: \"backend2\" address: \"10.0.1.11:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 Characteristics : - Non-deterministic selection using thread-local RNG - No state maintained between requests - Statistically even distribution over time - No session affinity - Automatically skips unhealthy backends Use Cases : - Stateless applications - High-throughput scenarios where simplicity matters - Testing and development - Avoiding predictable patterns for security Performance : Very low overhead (random number generation)","title":"Random"},{"location":"user-guide/load-balancing/#algorithm-comparison","text":"Algorithm Complexity Session Affinity State Distribution Use Case Round Robin O(1) No Counter Even General purpose, predictable load Consistent Hash O(log n) Yes Hash ring Even (with weight) Session affinity, cache locality Random O(1) No None Statistically even Stateless, high throughput","title":"Algorithm Comparison"},{"location":"user-guide/load-balancing/#backend-weighting","text":"Only consistent hashing respects backend weights. Round-robin and random algorithms currently ignore weights (weighted versions are planned for future release).","title":"Backend Weighting"},{"location":"user-guide/load-balancing/#weight-configuration","text":"backends: - id: \"small-instance\" address: \"10.0.1.10:8080\" weight: 50 # Receives 1x traffic health_check: path: \"/health\" interval: 5000 - id: \"medium-instance\" address: \"10.0.1.11:8080\" weight: 100 # Receives 2x traffic (2x the first) health_check: path: \"/health\" interval: 5000 - id: \"large-instance\" address: \"10.0.1.12:8080\" weight: 200 # Receives 4x traffic (4x the first) health_check: path: \"/health\" interval: 5000 Weight Behavior : - Round Robin : Weight values are currently ignored (weighted round-robin planned for future release) - Consistent Hash : Number of virtual nodes = replicas \u00d7 weight (64 replicas per weight unit) - Random : Weight values are currently ignored (weighted random planned for future release) - Minimum : Weight values below 1 are clamped to 1","title":"Weight Configuration"},{"location":"user-guide/load-balancing/#health-checking","text":"Spooky performs active health checks on all backends. Unhealthy backends are automatically removed from rotation.","title":"Health Checking"},{"location":"user-guide/load-balancing/#health-check-mechanism","text":"backends: - id: \"backend1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" # Health check endpoint path interval: 5000 # Check every 5 seconds timeout_ms: 2000 # 2 second timeout per check failure_threshold: 3 # 3 failures \u2192 mark unhealthy success_threshold: 2 # 2 successes \u2192 mark healthy cooldown_ms: 10000 # 10 second cooldown before recovery","title":"Health Check Mechanism"},{"location":"user-guide/load-balancing/#health-check-parameters","text":"path : Endpoint to check (default: \"/health\") - Must return 2xx status code when healthy - Should be lightweight and fast interval : Time between checks in milliseconds (default: 5000) - Lower values = faster failure detection - Higher values = lower overhead timeout_ms : Maximum wait time for response (default: 1000) - Should be less than interval - Failed on timeout failure_threshold : Consecutive failures to mark unhealthy (default: 3) - Higher values = more tolerance for transient failures - Lower values = faster failure detection success_threshold : Consecutive successes to mark healthy (default: 2) - Higher values = more confidence before recovery - Lower values = faster recovery cooldown_ms : Minimum time to stay unhealthy (default: 5000) - Prevents flapping - Gives backend time to recover","title":"Health Check Parameters"},{"location":"user-guide/load-balancing/#health-state-machine","text":"Initial State: Healthy | | failure_threshold consecutive failures v Unhealthy (cooldown period) | | cooldown expires v Unhealthy (testing) | | success_threshold consecutive successes v Healthy Healthy : Backend receives traffic, failures are counted Unhealthy (cooldown) : Backend removed from rotation, health checks continue but successes are ignored until cooldown expires Unhealthy (testing) : Backend still removed from rotation, consecutive successes are counted toward success_threshold","title":"Health State Machine"},{"location":"user-guide/load-balancing/#backend-state-tracking","text":"The load balancer tracks per-backend state: struct BackendState { address: String, weight: u32, health_check: HealthCheck, consecutive_failures: u32, health_state: HealthState, // Healthy | Unhealthy { until, successes } } State transitions: - record_success() : Increments success counter, transitions to Healthy if threshold met - record_failure() : Increments failure counter, transitions to Unhealthy if threshold met","title":"Backend State Tracking"},{"location":"user-guide/load-balancing/#multiple-upstream-pools","text":"Spooky supports multiple upstream pools with independent routing and load balancing configuration. Each pool specifies its own algorithm.","title":"Multiple Upstream Pools"},{"location":"user-guide/load-balancing/#configuration-example","text":"upstream: api_pool: load_balancing: type: \"consistent-hash\" # Session affinity for API requests route: path_prefix: \"/api\" backends: - id: \"api1\" address: \"10.0.1.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 - id: \"api2\" address: \"10.0.1.11:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 auth_pool: load_balancing: type: \"round-robin\" # Even spread across auth backends route: path_prefix: \"/auth\" backends: - id: \"auth1\" address: \"10.0.2.10:8080\" weight: 100 health_check: path: \"/health\" interval: 5000 static_pool: load_balancing: type: \"random\" # Stateless static assets, any backend is fine route: path_prefix: \"/static\" backends: - id: \"cdn1\" address: \"10.0.3.10:8080\" weight: 100 health_check: path: \"/health\" interval: 10000 Route Matching : - Routes are evaluated by longest-prefix matching - Route with most specific (longest) path prefix wins - For equal-length prefixes, selection depends on HashMap iteration order - Important : Unmatched routes return an error - configure a catch-all upstream (e.g., path_prefix: \"/\" ) to handle all other requests","title":"Configuration Example"},{"location":"user-guide/load-balancing/#monitoring-and-observability","text":"","title":"Monitoring and Observability"},{"location":"user-guide/load-balancing/#logging","text":"Health check events are logged: [INFO] Backend backend1 health check passed [WARN] Backend backend2 health check failed: connection timeout [INFO] Backend backend2 marked unhealthy after 3 consecutive failures [INFO] Backend backend2 marked healthy after 2 consecutive successes [DEBUG] Routing request to backend backend1 (round-robin) [DEBUG] Routing request to backend backend2 (consistent-hash, key=user:123)","title":"Logging"},{"location":"user-guide/load-balancing/#health-check-monitoring","text":"# Monitor health check activity (systemd) sudo journalctl -u spooky.service -f | grep -i health # Monitor health check activity (direct process) spooky --config config.yaml 2>&1 | grep -i health # Count backend state changes (systemd) sudo journalctl -u spooky.service | grep \"marked unhealthy\\|marked healthy\" | tail -20 # Count backend state changes (if redirected to file) grep \"marked unhealthy\\|marked healthy\" /var/log/spooky/spooky.log | tail -20 # Track specific backend (systemd) sudo journalctl -u spooky.service | grep \"backend1\" | grep health # Track specific backend (if redirected to file) grep \"backend1\" /var/log/spooky/spooky.log | grep health","title":"Health Check Monitoring"},{"location":"user-guide/load-balancing/#load-distribution-analysis","text":"# Extract backend selection counts (systemd) sudo journalctl -u spooky.service | grep \"routing to backend\" | \\ awk '{print $NF}' | sort | uniq -c | sort -rn # Monitor routing decisions in real-time (systemd) sudo journalctl -u spooky.service -f | grep \"routing to backend\" # If redirected to file grep \"routing to backend\" /var/log/spooky/spooky.log | \\ awk '{print $NF}' | sort | uniq -c | sort -rn","title":"Load Distribution Analysis"},{"location":"user-guide/load-balancing/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"user-guide/load-balancing/#algorithm-performance","text":"Algorithm Time Complexity Memory per Backend Per-Request Cost Round Robin O(1) ~8 bytes Counter increment Consistent Hash O(log n) ~4 KB (64 replicas \u00d7 weight) Hash + BTreeMap lookup Random O(1) ~0 bytes RNG call n = number of healthy backends","title":"Algorithm Performance"},{"location":"user-guide/load-balancing/#backend-pool-operations","text":"// All operations filter to healthy backends first healthy_indices() // O(n) - scans all backends pick_backend(algorithm) // O(1) or O(log n) depending on algorithm mark_success(index) // O(1) - direct index access mark_failure(index) // O(1) - direct index access","title":"Backend Pool Operations"},{"location":"user-guide/load-balancing/#scalability","text":"Backend Count : Algorithms scale to hundreds of backends Health Checks : Run asynchronously, do not block request path Memory : ~8-12 KB per backend (including health state and hash ring) CPU : Minimal overhead for all algorithms","title":"Scalability"},{"location":"user-guide/load-balancing/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/load-balancing/#uneven-load-distribution","text":"Symptoms : Some backends receive disproportionate traffic Diagnosis : # Check backend weights grep -A 10 \"backends:\" config.yaml | grep -E \"id|weight\" # Monitor actual distribution (systemd) sudo journalctl -u spooky.service | grep \"routing to backend\" | \\ awk '{print $NF}' | sort | uniq -c # If redirected to file grep \"routing to backend\" /var/log/spooky/spooky.log | \\ awk '{print $NF}' | sort | uniq -c # Verify all backends are healthy (systemd) sudo journalctl -u spooky.service | grep \"healthy\" | tail -20 # If redirected to file grep \"healthy\" /var/log/spooky/spooky.log | tail -20 Solutions : - Verify backend weights are configured correctly - Check for unhealthy backends (temporarily removed from rotation) - For consistent-hash, verify hash keys are well-distributed - For round-robin, ensure sufficient request volume for even distribution","title":"Uneven Load Distribution"},{"location":"user-guide/load-balancing/#session-affinity-not-working","text":"Symptoms : Requests from same user/session hit different backends Diagnosis : # Verify consistent-hash configuration grep -A 5 \"load_balancing:\" config.yaml | grep -E \"type|key\" # Check if hash key is present in requests (systemd) sudo journalctl -u spooky.service -f | grep \"consistent-hash\" # Check if hash key is present in requests (if redirected to file) tail -f /var/log/spooky/spooky.log | grep \"consistent-hash\" # Test with known hash key curl --http3-only -H \"X-User-ID: test123\" https://localhost:9889/ Solutions : - Ensure load_balancing.type is \"consistent-hash\" - Note: Hash key is automatically derived from request (authority \u2192 path \u2192 method) - For session affinity, ensure requests include consistent authority or path components - Configurable key sources are planned for future implementation","title":"Session Affinity Not Working"},{"location":"user-guide/load-balancing/#frequent-health-check-failures","text":"Symptoms : Backends repeatedly marked unhealthy despite being functional Diagnosis : # Monitor health check failures (systemd) sudo journalctl -u spooky.service -f | grep \"health check failed\" # If redirected to file tail -f /var/log/spooky/spooky.log | grep \"health check failed\" # Test health endpoint directly curl -v http://10.0.1.10:8080/health # Check response time time curl http://10.0.1.10:8080/health # Verify network connectivity ping 10.0.1.10 traceroute 10.0.1.10 Solutions : - Increase health check timeout (timeout_ms) if endpoint is slow - Increase failure_threshold to tolerate transient failures - Optimize backend health endpoint performance - Check network latency and packet loss - Verify health check path is correct","title":"Frequent Health Check Failures"},{"location":"user-guide/load-balancing/#backends-not-recovering","text":"Symptoms : Healthy backends remain marked unhealthy Diagnosis : # Check cooldown period grep -A 10 \"health_check:\" config.yaml | grep cooldown_ms # Monitor recovery attempts (systemd) sudo journalctl -u spooky.service -f | grep -E \"marked healthy|success\" # If redirected to file tail -f /var/log/spooky/spooky.log | grep -E \"marked healthy|success\" # Verify backend is actually healthy curl http://10.0.1.10:8080/health Solutions : - Reduce cooldown_ms for faster recovery - Reduce success_threshold if being too conservative - Verify backend health endpoint returns 2xx status - Check for health check timeout issues","title":"Backends Not Recovering"},{"location":"user-guide/load-balancing/#no-backends-available","text":"Symptoms : All backends marked unhealthy, requests fail Diagnosis : # List all backend states (systemd) sudo journalctl -u spooky.service | grep -i \"backend.*health\" | tail -20 # If redirected to file grep -i \"backend.*health\" /var/log/spooky/spooky.log | tail -20 # Check configuration grep -A 15 \"backends:\" config.yaml # Test each backend directly for backend in 10.0.1.10:8080 10.0.1.11:8080; do echo \"Testing $backend\" curl -v http://$backend/health done Solutions : - Fix backend health endpoints - Adjust health check parameters (increase timeout, threshold) - Verify backends are actually running and accessible - Check firewall rules between Spooky and backends","title":"No Backends Available"},{"location":"user-guide/load-balancing/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/load-balancing/#health-check-configuration","text":"Set timeout_ms < interval to prevent check pileup Use failure_threshold \u2265 3 to avoid false positives Set cooldown_ms \u2265 10000 to prevent flapping Keep health endpoints lightweight (< 100ms response time)","title":"Health Check Configuration"},{"location":"user-guide/load-balancing/#algorithm-selection","text":"Use Round Robin for simple, even distribution with equal backends Use Consistent Hash when session affinity or cache locality matters Use Random for stateless, high-throughput scenarios","title":"Algorithm Selection"},{"location":"user-guide/load-balancing/#weight-configuration_1","text":"Base weights on backend capacity (CPU, memory, network) Start with equal weights, adjust based on monitoring Use relative weights (100, 200, 400) rather than absolute For consistent-hash, remember: weight \u00d7 64 = number of virtual nodes","title":"Weight Configuration"},{"location":"user-guide/load-balancing/#upstream-pool-design","text":"Create separate pools for different services (API, auth, static) Order routes from most specific to least specific Use path_prefix for path-based routing Use host for virtual host routing Provide a catch-all default pool","title":"Upstream Pool Design"},{"location":"user-guide/load-balancing/#advanced-configuration-examples","text":"","title":"Advanced Configuration Examples"},{"location":"user-guide/load-balancing/#multi-tier-application","text":"# Global load balancing strategy (applies to all upstream pools) # Per-upstream load_balancing is planned but not currently active load_balancing: type: \"round-robin\" upstream: api_tier: route: path_prefix: \"/api\" backends: - id: \"api1\" address: \"10.0.1.10:8080\" weight: 200 health_check: path: \"/health\" interval: 5000 timeout_ms: 2000 failure_threshold: 3 success_threshold: 2 cooldown_ms: 15000 auth_tier: route: path_prefix: \"/auth\" backends: - id: \"auth1\" address: \"10.0.2.10:8080\" weight: 100 health_check: path: \"/health\" interval: 3000 timeout_ms: 1000 failure_threshold: 2 success_threshold: 2 cooldown_ms: 10000 static_tier: route: path_prefix: \"/static\" backends: - id: \"cdn1\" address: \"10.0.3.10:8080\" weight: 100 health_check: path: \"/health\" interval: 30000 timeout_ms: 5000 failure_threshold: 5 success_threshold: 1 cooldown_ms: 60000","title":"Multi-Tier Application"},{"location":"user-guide/load-balancing/#heterogeneous-backend-capacities","text":"upstream: prod_pool: load_balancing: type: \"round-robin\" route: path_prefix: \"/\" backends: - id: \"small-1\" address: \"10.0.1.10:8080\" weight: 50 # 2 vCPU, 4 GB RAM health_check: path: \"/health\" interval: 5000 - id: \"medium-1\" address: \"10.0.1.11:8080\" weight: 100 # 4 vCPU, 8 GB RAM health_check: path: \"/health\" interval: 5000 - id: \"large-1\" address: \"10.0.1.12:8080\" weight: 200 # 8 vCPU, 16 GB RAM health_check: path: \"/health\" interval: 5000","title":"Heterogeneous Backend Capacities"},{"location":"user-guide/load-balancing/#related-documentation","text":"See Basics for general configuration and deployment Refer to Configuration Reference for complete parameter documentation See Architecture Guide for internal implementation details","title":"Related Documentation"}]}